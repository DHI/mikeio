[
  {
    "objectID": "user-guide/dfs1.html",
    "href": "user-guide/dfs1.html",
    "title": "Dfs1",
    "section": "",
    "text": "A dfs1 file contains node-based line series data. Dfs1 files do not contain enough metadata to determine their geographical position, but have a relative distance from the origo.\nimport mikeio\n\nds = mikeio.read(\"../data/tide1.dfs1\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:97, x:10)\ntime: 2019-01-01 00:00:00 - 2019-01-03 00:00:00 (97 records)\ngeometry: Grid1D (n=10, dx=0.06667)\nitems:\n  0:  Level &lt;Water Level&gt; (meter)",
    "crumbs": [
      "User Guide",
      "Dfs1"
    ]
  },
  {
    "objectID": "user-guide/dfs1.html#grid-1d",
    "href": "user-guide/dfs1.html#grid-1d",
    "title": "Dfs1",
    "section": "Grid 1D",
    "text": "Grid 1D\nThe spatial information is available in the geometry attribute (accessible from Dfs1, Dataset, and DataArray), which in the case of a dfs1 file is a Grid1D geometry.\n\nds.geometry\n\n&lt;mikeio.Grid1D&gt;\nx: [0, 0.06667, ..., 0.6] (nx=10, dx=0.06667)\n\n\nGrid1D’s primary properties and methods are:\n\nx\nnx\ndx\nfind_index()\nisel()\n\nSee API specification for details.",
    "crumbs": [
      "User Guide",
      "Dfs1"
    ]
  },
  {
    "objectID": "user-guide/dfs1.html#creating-a-dfs1-file",
    "href": "user-guide/dfs1.html#creating-a-dfs1-file",
    "title": "Dfs1",
    "section": "Creating a dfs1 file",
    "text": "Creating a dfs1 file\n\nCreate a datetime index\nCreate a data array with dimensions (time, x)\n\nIn this example the grid consist of two points (west and east), but the same approach can be used for any number of points.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport mikeio\n\nt = pd.date_range(\"2021-01-01\", periods=100, freq=\"15min\")\nt_rel = (t - t[0]).total_seconds().values\n\nwest = np.sin(t_rel / 3600)\neast = 1.2*np.sin(t_rel / 3600 + 0.5)\n\ndata = np.column_stack((west, east))\ndata.shape\n\n(100, 2)\n\n\n\nCreate a Grid1D geometry with the number of points in the x-direction and the spacing.\n\n\ngeometry = mikeio.Grid1D(nx=2, dx=1)\ngeometry\n\n&lt;mikeio.Grid1D&gt;\nx: [0, 1] (nx=2, dx=1)\n\n\n\nCreate a DataArray object with the data, time, geometry, and item information.\n\n\nda = mikeio.DataArray(\n    data,\n    time=t,\n    geometry=geometry,\n    item=mikeio.ItemInfo(\"Water level\", mikeio.EUMType.Water_Level),\n)\nda\n\n&lt;mikeio.DataArray&gt;\nname: Water level\ndims: (time:100, x:2)\ntime: 2021-01-01 00:00:00 - 2021-01-02 00:45:00 (100 records)\ngeometry: Grid1D (n=2, dx=1)\n\n\n\nda.plot.timeseries()\n\n\n\n\n\n\n\n\nOptional, repeat step 4 for additional items to create a Dataset.\n\nWrite to a dfs1 file.\n\n\nda.to_dfs(\"boundary.dfs1\")",
    "crumbs": [
      "User Guide",
      "Dfs1"
    ]
  },
  {
    "objectID": "user-guide/data-structures.html",
    "href": "user-guide/data-structures.html",
    "title": "Data Structures",
    "section": "",
    "text": "Data Structures\nMIKE IO has these primary data structures:\n\nDataset - a collection of DataArrays corresponding to the contents of a dfs file; typically obtained from mikeio.read\nDataArray - data and metadata corresponding to one “item” in a dfs file.\nGeometry - spatial description of the data in a dfs file; comes in different flavours: Grid1D, Grid2D, Grid3D, GeometryFM2D, GeometryFM3D, etc. corresponding to different types of dfs files.\nDfs - an object returned by dfs = mikeio.open() containing the metadata (=header) of a dfs file ready for reading the data (which can be done with dfs.read()); exists in different specialized versions: Dfs0, Dfs1, Dfs2, Dfs3, Dfsu2DH, Dfsu3D, Dfsu2DV, DfsuSpectral.",
    "crumbs": [
      "User Guide",
      "Data Structures"
    ]
  },
  {
    "objectID": "user-guide/dataset.html",
    "href": "user-guide/dataset.html",
    "title": "Dataset",
    "section": "",
    "text": "The Dataset is the MIKE IO data structure for data from dfs files. The mikeio.read methods returns a Dataset as a container of DataArray (Dfs items). Each DataArray has the properties, item, time, geometry and values. The time and geometry are common to all DataArrays in the Dataset.\nThe Dataset has the following primary properties:\nUse Dataset’s string representation to get an overview of the Dataset\nimport mikeio\nds = mikeio.read(\"../data/HD2D.dfsu\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:9, element:884)\ntime: 1985-08-06 07:00:00 - 1985-08-07 03:00:00 (9 records)\ngeometry: Dfsu2D (884 elements, 529 nodes)\nitems:\n  0:  Surface elevation &lt;Surface Elevation&gt; (meter)\n  1:  U velocity &lt;u velocity component&gt; (meter per sec)\n  2:  V velocity &lt;v velocity component&gt; (meter per sec)\n  3:  Current speed &lt;Current Speed&gt; (meter per sec)",
    "crumbs": [
      "User Guide",
      "Dataset"
    ]
  },
  {
    "objectID": "user-guide/dataset.html#selecting-items",
    "href": "user-guide/dataset.html#selecting-items",
    "title": "Dataset",
    "section": " Selecting items",
    "text": "Selecting items\nSelecting a specific item “itemA” (at position 0) from a Dataset ds can be done with:\n\nds[[\"itemA\"]] - returns a new Dataset with “itemA”\nds[\"itemA\"] - returns “itemA” DataArray\nds[[0]] - returns a new Dataset with “itemA”\nds[0] - returns “itemA” DataArray\nds.itemA - returns “itemA” DataArray\n\nWe recommend to use named items for readability.\n\nds.Surface_elevation\n\n&lt;mikeio.DataArray&gt;\nname: Surface elevation\ndims: (time:9, element:884)\ntime: 1985-08-06 07:00:00 - 1985-08-07 03:00:00 (9 records)\ngeometry: Dfsu2D (884 elements, 529 nodes)\n\n\nNegative index e.g. ds[-1] can also be used to select from the end. Several items (“itemA” at 0 and “itemC” at 2) can be selected with the notation:\n\nds[[\"itemA\", \"itemC\"]]\nds[[0, 2]]\n\nNote that this behavior is similar to pandas and xarray.",
    "crumbs": [
      "User Guide",
      "Dataset"
    ]
  },
  {
    "objectID": "user-guide/dataset.html#temporal-selection",
    "href": "user-guide/dataset.html#temporal-selection",
    "title": "Dataset",
    "section": " Temporal selection",
    "text": "Temporal selection\nA time slice of a Dataset can be selected in several different ways.\n\nds.sel(time=\"1985-08-06 12:00\")\n\n&lt;mikeio.Dataset&gt;\ndims: (element:884)\ntime: 1985-08-06 12:00:00 (time-invariant)\ngeometry: Dfsu2D (884 elements, 529 nodes)\nitems:\n  0:  Surface elevation &lt;Surface Elevation&gt; (meter)\n  1:  U velocity &lt;u velocity component&gt; (meter per sec)\n  2:  V velocity &lt;v velocity component&gt; (meter per sec)\n  3:  Current speed &lt;Current Speed&gt; (meter per sec)\n\n\n\nds.sel(time=slice(\"1985-08-06 12:00\", \"1985-08-06 17:00\"))\n\n&lt;mikeio.Dataset&gt;\ndims: (time:3, element:884)\ntime: 1985-08-06 12:00:00 - 1985-08-06 17:00:00 (3 records)\ngeometry: Dfsu2D (884 elements, 529 nodes)\nitems:\n  0:  Surface elevation &lt;Surface Elevation&gt; (meter)\n  1:  U velocity &lt;u velocity component&gt; (meter per sec)\n  2:  V velocity &lt;v velocity component&gt; (meter per sec)\n  3:  Current speed &lt;Current Speed&gt; (meter per sec)\n\n\n\nds.isel(time=2)\n\n&lt;mikeio.Dataset&gt;\ndims: (element:884)\ntime: 1985-08-06 12:00:00 (time-invariant)\ngeometry: Dfsu2D (884 elements, 529 nodes)\nitems:\n  0:  Surface elevation &lt;Surface Elevation&gt; (meter)\n  1:  U velocity &lt;u velocity component&gt; (meter per sec)\n  2:  V velocity &lt;v velocity component&gt; (meter per sec)\n  3:  Current speed &lt;Current Speed&gt; (meter per sec)\n\n\n\nds.isel(time=range(2, ds.n_timesteps, 2))\n\n&lt;mikeio.Dataset&gt;\ndims: (time:4, element:884)\ntime: 1985-08-06 12:00:00 - 1985-08-07 03:00:00 (4 records)\ngeometry: Dfsu2D (884 elements, 529 nodes)\nitems:\n  0:  Surface elevation &lt;Surface Elevation&gt; (meter)\n  1:  U velocity &lt;u velocity component&gt; (meter per sec)\n  2:  V velocity &lt;v velocity component&gt; (meter per sec)\n  3:  Current speed &lt;Current Speed&gt; (meter per sec)",
    "crumbs": [
      "User Guide",
      "Dataset"
    ]
  },
  {
    "objectID": "user-guide/dataset.html#spatial-selection",
    "href": "user-guide/dataset.html#spatial-selection",
    "title": "Dataset",
    "section": " Spatial selection",
    "text": "Spatial selection\nThe sel method finds a single element.\n\nds.sel(x=607002, y=6906734)\n\n&lt;mikeio.Dataset&gt;\ndims: (time:9)\ntime: 1985-08-06 07:00:00 - 1985-08-07 03:00:00 (9 records)\ngeometry: GeometryPoint2D(x=607002.7094112666, y=6906734.833048992)\nitems:\n  0:  Surface elevation &lt;Surface Elevation&gt; (meter)\n  1:  U velocity &lt;u velocity component&gt; (meter per sec)\n  2:  V velocity &lt;v velocity component&gt; (meter per sec)\n  3:  Current speed &lt;Current Speed&gt; (meter per sec)",
    "crumbs": [
      "User Guide",
      "Dataset"
    ]
  },
  {
    "objectID": "user-guide/dataset.html#plotting",
    "href": "user-guide/dataset.html#plotting",
    "title": "Dataset",
    "section": " Plotting",
    "text": "Plotting\nIn most cases, you will not plot the Dataset, but rather it’s DataArrays. But there are two exceptions:\n\ndfs0-Dataset : plot all items as timeseries with ds.plot()\nscatter : compare two items using ds.plot.scatter(x=“itemA”, y=“itemB”)\n\nSee details in the Dataset Plotter API.",
    "crumbs": [
      "User Guide",
      "Dataset"
    ]
  },
  {
    "objectID": "user-guide/dataset.html#add-a-new-item",
    "href": "user-guide/dataset.html#add-a-new-item",
    "title": "Dataset",
    "section": "Add a new item",
    "text": "Add a new item\nA common workflow is to create a new item based on existing items in a dataset.\nThis can be in done in several ways. Let’s try one of the options.\n\nds = mikeio.read(\"../data/NorthSea_HD_and_windspeed.dfsu\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:67, element:958)\ntime: 2017-10-27 00:00:00 - 2017-10-29 18:00:00 (67 records)\ngeometry: Dfsu2D (958 elements, 570 nodes)\nitems:\n  0:  Surface elevation &lt;Surface Elevation&gt; (meter)\n  1:  Wind speed &lt;Wind speed&gt; (meter per sec)\n\n\n\nCreate a copy of the DataArray\n\n\nws2 = ds.Wind_speed.copy()\nws2.plot.hist();\n\n\n\n\n\n\n\n\n\nMake the modifications, in this case we will clip the values to the interval 1-18 m/s.\n\n\nimport numpy as np\nws2.values = np.clip(ws2.to_numpy(), 1,18)\nws2.plot.hist();\n\n\n\n\n\n\n\n\n\nAssign it to a new name in the dataset\n\n\nds[\"Wind_speed_clipped\"] = ws2\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:67, element:958)\ntime: 2017-10-27 00:00:00 - 2017-10-29 18:00:00 (67 records)\ngeometry: Dfsu2D (958 elements, 570 nodes)\nitems:\n  0:  Surface elevation &lt;Surface Elevation&gt; (meter)\n  1:  Wind speed &lt;Wind speed&gt; (meter per sec)\n  2:  Wind_speed_clipped &lt;Wind speed&gt; (meter per sec)\n\n\n\nReorder items if necessary (See selecting items above)\n\n\nds2 = ds[[\"Wind_speed_clipped\", \"Surface elevation\", \"Wind speed\"]]\nds2\n\n&lt;mikeio.Dataset&gt;\ndims: (time:67, element:958)\ntime: 2017-10-27 00:00:00 - 2017-10-29 18:00:00 (67 records)\ngeometry: Dfsu2D (958 elements, 570 nodes)\nitems:\n  0:  Wind_speed_clipped &lt;Wind speed&gt; (meter per sec)\n  1:  Surface elevation &lt;Surface Elevation&gt; (meter)\n  2:  Wind speed &lt;Wind speed&gt; (meter per sec)\n\n\n\nWrite the new dataset to a new file\n\n\nds2.to_dfs(\"modified.dfsu\")",
    "crumbs": [
      "User Guide",
      "Dataset"
    ]
  },
  {
    "objectID": "user-guide/dataset.html#properties",
    "href": "user-guide/dataset.html#properties",
    "title": "Dataset",
    "section": "Properties",
    "text": "Properties\nThe Dataset (and DataArray) has several properties:\n\nn_items - Number of items\nn_timesteps - Number of timesteps\nn_elements - Number of elements\nstart_time - First time instance (as datetime)\nend_time - Last time instance (as datetime)\nis_equidistant - Is the time series equidistant in time\ntimestep - Time step in seconds (if is_equidistant)\nshape - Shape of each item\ndeletevalue - File delete value (NaN value)",
    "crumbs": [
      "User Guide",
      "Dataset"
    ]
  },
  {
    "objectID": "user-guide/dataset.html#methods",
    "href": "user-guide/dataset.html#methods",
    "title": "Dataset",
    "section": "Methods",
    "text": "Methods\nDataset (and DataArray) has several useful methods for working with data, including different ways of selecting data:\n\nsel() - Select subset along an axis\nisel() - Select subset along an axis with an integer\n\nAggregations along an axis:\n\nmean() - Mean value along an axis\nnanmean() - Mean value along an axis (NaN removed)\nmax() - Max value along an axis\nnanmax() - Max value along an axis (NaN removed)\nmin() - Min value along an axis\nnanmin() - Min value along an axis (NaN removed)\naverage() - Compute the weighted average along the specified axis.\naggregate() - Aggregate along an axis\nquantile() - Quantiles along an axis\nnanquantile() - Quantiles along an axis (NaN ignored)\n\n\n Mathematical operations\n\nds + value\nds - value\nds * value\nds / value and between two Datasets (if number of items and shapes conform):\nds1 + ds2\nds1 - ds2\nds1 * ds2\nds1 / ds2\n\nOther methods that also return a Dataset:\n\ninterp_like - Spatio (temporal) interpolation (see Dfsu interpolation notebook\ninterp_time() - Temporal interpolation (see Time interpolation notebook)\ndropna() - Remove time steps where all items are NaN\nfillna() - Fill missing values with a constant value\nsqueeze() - Remove axes of length 1\n\n\n\nConversion:\n\nto_dataframe() - Convert Dataset to a pandas.DataFrame.\nto_xarray() - Convert Dataset to a xarray.Dataset (great for Dfs2, Dfs3).\nto_dfs() - Write Dataset to a Dfs file",
    "crumbs": [
      "User Guide",
      "Dataset"
    ]
  },
  {
    "objectID": "user-guide/dfs2.html",
    "href": "user-guide/dfs2.html",
    "title": "Dfs2",
    "section": "",
    "text": "A dfs2 file is also called a grid series file. Values in a dfs2 file are ‘element based’, i.e. values are defined in the centre of each grid cell.\nimport mikeio\nds = mikeio.read(\"../data/gebco_sound.dfs2\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1, y:264, x:216)\ntime: 2020-05-15 11:04:52 (time-invariant)\ngeometry: Grid2D (ny=264, nx=216)\nitems:\n  0:  Elevation &lt;Total Water Depth&gt; (meter)",
    "crumbs": [
      "User Guide",
      "Dfs2"
    ]
  },
  {
    "objectID": "user-guide/dfs2.html#subset-in-space",
    "href": "user-guide/dfs2.html#subset-in-space",
    "title": "Dfs2",
    "section": "Subset in space",
    "text": "Subset in space\nThe most convenient way to subset in space is to use the sel method, which returns a new (smaller) dataset, which can be further processed or written to disk using the to_dfs method.\n\nds.geometry\n\n&lt;mikeio.Grid2D&gt;\nx: [12.2, 12.21, ..., 13.1] (nx=216, dx=0.004167)\ny: [55.2, 55.21, ..., 56.3] (ny=264, dy=0.004167)\nprojection: LONG/LAT\n\n\n\nds_aoi = ds.sel(x=slice(12.5, 13.0), y=slice(55.5, 56.0))\nds_aoi.geometry\n\n&lt;mikeio.Grid2D&gt;\nx: [12.5, 12.5, ..., 12.99] (nx=120, dx=0.004167)\ny: [55.5, 55.5, ..., 55.99] (ny=120, dy=0.004167)\nprojection: LONG/LAT\n\n\nIn order to specify an open-ended subset (i.e. where the end of the subset is the end of the domain), use None as the end of the slice.\n\nds.sel(x=slice(None, 13.0))\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1, y:264, x:191)\ntime: 2020-05-15 11:04:52 (time-invariant)\ngeometry: Grid2D (ny=264, nx=191)\nitems:\n  0:  Elevation &lt;Total Water Depth&gt; (meter)",
    "crumbs": [
      "User Guide",
      "Dfs2"
    ]
  },
  {
    "objectID": "user-guide/dfs2.html#grid2d",
    "href": "user-guide/dfs2.html#grid2d",
    "title": "Dfs2",
    "section": "Grid2D",
    "text": "Grid2D\nThe spatial information is available in the geometry attribute (accessible from Dfs2, Dataset, and DataArray), which in the case of a dfs2 file is a Grid2D geometry.\n\nds.geometry\n\n&lt;mikeio.Grid2D&gt;\nx: [12.2, 12.21, ..., 13.1] (nx=216, dx=0.004167)\ny: [55.2, 55.21, ..., 56.3] (ny=264, dy=0.004167)\nprojection: LONG/LAT\n\n\nGrid2D’s primary properties and methods are:\n\nx\nnx\ndx\ny\nny\ndy\norigin\nprojection\nxy\nbbox\ncontains()\nfind_index()\nisel()\nto_mesh()\n\nSee API specification for details.",
    "crumbs": [
      "User Guide",
      "Dfs2"
    ]
  },
  {
    "objectID": "user-guide/dfs2.html#dfs2-resources",
    "href": "user-guide/dfs2.html#dfs2-resources",
    "title": "Dfs2",
    "section": "Dfs2 resources",
    "text": "Dfs2 resources\n\nDfs2 | getting-started-with-mikeio\nDfs2-Bathymetry - GEBCO NetCDF/xarray to dfs2\nDfs2-GFS - GFS NetCDF/xarray to dfs2",
    "crumbs": [
      "User Guide",
      "Dfs2"
    ]
  },
  {
    "objectID": "user-guide/mesh.html",
    "href": "user-guide/mesh.html",
    "title": "Mesh",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport mikeio",
    "crumbs": [
      "User Guide",
      "Mesh"
    ]
  },
  {
    "objectID": "user-guide/mesh.html#read-mesh-file",
    "href": "user-guide/mesh.html#read-mesh-file",
    "title": "Mesh",
    "section": "Read mesh file",
    "text": "Read mesh file\n\nmsh = mikeio.Mesh(\"../data/odense_rough.mesh\")\nmsh\n\n&lt;Mesh&gt;\nnumber of nodes: 399\nnumber of elements: 654\nprojection: UTM-33",
    "crumbs": [
      "User Guide",
      "Mesh"
    ]
  },
  {
    "objectID": "user-guide/mesh.html#plot-mesh",
    "href": "user-guide/mesh.html#plot-mesh",
    "title": "Mesh",
    "section": "Plot mesh",
    "text": "Plot mesh\n\nmsh.plot()\n\n\n\n\n\n\n\n\n\nmsh.plot.boundary_nodes(boundary_names=['Land','Open boundary']);",
    "crumbs": [
      "User Guide",
      "Mesh"
    ]
  },
  {
    "objectID": "user-guide/mesh.html#convert-mesh-to-shapely",
    "href": "user-guide/mesh.html#convert-mesh-to-shapely",
    "title": "Mesh",
    "section": "Convert mesh to shapely",
    "text": "Convert mesh to shapely\nConvert mesh to shapely MultiPolygon object, requires that the shapely library is installed.\n\nmp = msh.to_shapely()\nmp\n\n\n\n\n\n\n\n\nNow a lot of methods are available\n\nmp.area\n\n68931409.58160606\n\n\n\nmp.bounds\n\n(211068.501175313, 6153077.66681803, 224171.617336507, 6164499.42751662)\n\n\n\ndomain = mp.buffer(0)\ndomain\n\n\n\n\n\n\n\n\n\nopen_water = domain.buffer(-500)\n\ncoastalzone = domain - open_water\ncoastalzone",
    "crumbs": [
      "User Guide",
      "Mesh"
    ]
  },
  {
    "objectID": "user-guide/mesh.html#check-if-points-are-inside-the-domain",
    "href": "user-guide/mesh.html#check-if-points-are-inside-the-domain",
    "title": "Mesh",
    "section": "Check if points are inside the domain",
    "text": "Check if points are inside the domain\n\nfrom shapely.geometry import Point\n\np1 = Point(216000, 6162000)\np2 = Point(220000, 6156000)\nprint(mp.contains(p1))\nprint(mp.contains(p2))\n\nTrue\nFalse\n\n\nWe can get similar functionality from the .geometry attribute of the mesh object.\n\np1p2 = [[216000, 6162000], [220000, 6156000]]\nmsh.geometry.contains(p1p2)\n\narray([ True, False])\n\n\n\nax = msh.plot()\nax.scatter(p1.x, p1.y, marker=\"*\", s=200, c=\"red\", label=\"inside\")\nax.scatter(p2.x, p2.y, marker=\"+\", s=200, c=\"green\", label=\"outside\")\nax.legend();",
    "crumbs": [
      "User Guide",
      "Mesh"
    ]
  },
  {
    "objectID": "user-guide/mesh.html#change-z-values-and-boundary-code",
    "href": "user-guide/mesh.html#change-z-values-and-boundary-code",
    "title": "Mesh",
    "section": "Change z values and boundary code",
    "text": "Change z values and boundary code\nAssume that we want to have a minimum depth of 2 meters and change the open boundary (code 2) to a closed one (code 1).\n\ng = msh.geometry\nprint(f'max z before: {g.node_coordinates[:,2].max()}')\nzc = g.node_coordinates[:,2]\nzc[zc&gt;-2] = -2\ng.node_coordinates[:,2] = zc\nprint(f'max z after: {g.node_coordinates[:,2].max()}')\n\nmax z before: -0.200000002980232\nmax z after: -2.0\n\n\n\nc = g.codes\nc[c==2] = 1\ng.codes = c",
    "crumbs": [
      "User Guide",
      "Mesh"
    ]
  },
  {
    "objectID": "user-guide/mesh.html#save-the-modfied-geometry-to-a-new-mesh-file",
    "href": "user-guide/mesh.html#save-the-modfied-geometry-to-a-new-mesh-file",
    "title": "Mesh",
    "section": "Save the modfied geometry to a new mesh file",
    "text": "Save the modfied geometry to a new mesh file\n\ng.to_mesh(\"new_mesh.mesh\")\n\nCleanup\n\nimport os\n\nos.remove(\"new_mesh.mesh\")",
    "crumbs": [
      "User Guide",
      "Mesh"
    ]
  },
  {
    "objectID": "user-guide/generic.html",
    "href": "user-guide/generic.html",
    "title": "Generic",
    "section": "",
    "text": "The generic module contains functionality that works for all types of dfs (dfs0, dfs1, dfs2, dfs3, dfsu) files:\n\nconcat() - Concatenates files along the time axis\nextract() - Extract timesteps and/or items to a new dfs file\ndiff() - Calculate difference between two dfs files with identical geometry\nsum() - Calculate the sum of two dfs files\nscale() - Apply scaling to any dfs file\navg_time() - Create a temporally averaged dfs file\nquantile() - Create a dfs file with temporal quantiles\n\n\n\n\nThe processing is not tied to the spatial dimension of the data\nWhen the files are large and you want to avoid reading the entire file into memory\n\n\n\n\n\nWhen you need processing depending on the spatial information in the file. For example, spatial interpolation, subsetting, etc.\nWhen you need more complex processing, not covered by the generic module\nWhen the input files data are not dfs files\nWhen the end result is not a dfs file\n\n\n\n\n&gt;&gt;&gt; from mikeio import generic\n&gt;&gt;&gt; generic.concat([\"fileA.dfs2\", \"fileB.dfs2\"], \"new_file.dfs2\")\n\n\n\nSee the Generic notebook for more examples.",
    "crumbs": [
      "User Guide",
      "Generic"
    ]
  },
  {
    "objectID": "user-guide/generic.html#when-to-use-the-generic-module",
    "href": "user-guide/generic.html#when-to-use-the-generic-module",
    "title": "Generic",
    "section": "",
    "text": "The processing is not tied to the spatial dimension of the data\nWhen the files are large and you want to avoid reading the entire file into memory",
    "crumbs": [
      "User Guide",
      "Generic"
    ]
  },
  {
    "objectID": "user-guide/generic.html#when-not-to-use-the-generic-module",
    "href": "user-guide/generic.html#when-not-to-use-the-generic-module",
    "title": "Generic",
    "section": "",
    "text": "When you need processing depending on the spatial information in the file. For example, spatial interpolation, subsetting, etc.\nWhen you need more complex processing, not covered by the generic module\nWhen the input files data are not dfs files\nWhen the end result is not a dfs file",
    "crumbs": [
      "User Guide",
      "Generic"
    ]
  },
  {
    "objectID": "user-guide/generic.html#example",
    "href": "user-guide/generic.html#example",
    "title": "Generic",
    "section": "",
    "text": "&gt;&gt;&gt; from mikeio import generic\n&gt;&gt;&gt; generic.concat([\"fileA.dfs2\", \"fileB.dfs2\"], \"new_file.dfs2\")",
    "crumbs": [
      "User Guide",
      "Generic"
    ]
  },
  {
    "objectID": "user-guide/generic.html#more-examples",
    "href": "user-guide/generic.html#more-examples",
    "title": "Generic",
    "section": "",
    "text": "See the Generic notebook for more examples.",
    "crumbs": [
      "User Guide",
      "Generic"
    ]
  },
  {
    "objectID": "design.html",
    "href": "design.html",
    "title": "Design philosophy",
    "section": "",
    "text": "Common operations such as reading a file should only need a few lines of code.\nMake extensive use of existing standard libraries for scientific computing such as numpy, matplotlib and pandas.\n\n\n\nMIKE IO aims to use a syntax familiar to users of scientific computing libraries such as NumPy, Pandas and xarray.\n\n\n\n$ pip install mikeio\n\n\n\nBy providing many examples to cut/paste from.\nExamples are available in two forms:\n\nExample notebooks\nUnit tests\n\n\n\n\nMIKE IO is an open source project licensed under the BSD-3 license. The software is provided free of charge with the source code available for inspection and modification.\nContributions are welcome!\n\n\n\nBy developing MIKE IO on GitHub along with a completely open discussion, we believe that the collaboration between developers and end-users results in a useful library.\n\n\n\nBy providing the historical versions of MIKE IO on PyPI it is possible to reproduce the behaviour of an older existing system, based on an older version.\nInstall specific version\npip install mikeio==1.4.0\n\n\n\nFeatures are being added all the time, by developers at DHI in offices all around the globe as well as external contributors using MIKE IO in their work. These new features are always available from the main branch on GitHub and thanks to automated testing, it is always possible to verify that the tests passes before downloading a new development version.\nInstall development version\n$ pip install https://github.com/DHI/mikeio/archive/main.zip",
    "crumbs": [
      "User Guide",
      "Design philosophy"
    ]
  },
  {
    "objectID": "design.html#easy-to-use",
    "href": "design.html#easy-to-use",
    "title": "Design philosophy",
    "section": "",
    "text": "Common operations such as reading a file should only need a few lines of code.\nMake extensive use of existing standard libraries for scientific computing such as numpy, matplotlib and pandas.",
    "crumbs": [
      "User Guide",
      "Design philosophy"
    ]
  },
  {
    "objectID": "design.html#familiar",
    "href": "design.html#familiar",
    "title": "Design philosophy",
    "section": "",
    "text": "MIKE IO aims to use a syntax familiar to users of scientific computing libraries such as NumPy, Pandas and xarray.",
    "crumbs": [
      "User Guide",
      "Design philosophy"
    ]
  },
  {
    "objectID": "design.html#easy-to-install",
    "href": "design.html#easy-to-install",
    "title": "Design philosophy",
    "section": "",
    "text": "$ pip install mikeio",
    "crumbs": [
      "User Guide",
      "Design philosophy"
    ]
  },
  {
    "objectID": "design.html#easy-to-get-started",
    "href": "design.html#easy-to-get-started",
    "title": "Design philosophy",
    "section": "",
    "text": "By providing many examples to cut/paste from.\nExamples are available in two forms:\n\nExample notebooks\nUnit tests",
    "crumbs": [
      "User Guide",
      "Design philosophy"
    ]
  },
  {
    "objectID": "design.html#open-source",
    "href": "design.html#open-source",
    "title": "Design philosophy",
    "section": "",
    "text": "MIKE IO is an open source project licensed under the BSD-3 license. The software is provided free of charge with the source code available for inspection and modification.\nContributions are welcome!",
    "crumbs": [
      "User Guide",
      "Design philosophy"
    ]
  },
  {
    "objectID": "design.html#easy-to-collaborate",
    "href": "design.html#easy-to-collaborate",
    "title": "Design philosophy",
    "section": "",
    "text": "By developing MIKE IO on GitHub along with a completely open discussion, we believe that the collaboration between developers and end-users results in a useful library.",
    "crumbs": [
      "User Guide",
      "Design philosophy"
    ]
  },
  {
    "objectID": "design.html#reproducible",
    "href": "design.html#reproducible",
    "title": "Design philosophy",
    "section": "",
    "text": "By providing the historical versions of MIKE IO on PyPI it is possible to reproduce the behaviour of an older existing system, based on an older version.\nInstall specific version\npip install mikeio==1.4.0",
    "crumbs": [
      "User Guide",
      "Design philosophy"
    ]
  },
  {
    "objectID": "design.html#easy-access-to-new-features",
    "href": "design.html#easy-access-to-new-features",
    "title": "Design philosophy",
    "section": "",
    "text": "Features are being added all the time, by developers at DHI in offices all around the globe as well as external contributors using MIKE IO in their work. These new features are always available from the main branch on GitHub and thanks to automated testing, it is always possible to verify that the tests passes before downloading a new development version.\nInstall development version\n$ pip install https://github.com/DHI/mikeio/archive/main.zip",
    "crumbs": [
      "User Guide",
      "Design philosophy"
    ]
  },
  {
    "objectID": "index.html#set-up-in-5-minutes",
    "href": "index.html#set-up-in-5-minutes",
    "title": "MIKE IO: input/output of MIKE files in Python",
    "section": " Set up in 5 minutes",
    "text": "Set up in 5 minutes\nInstall MIKE IO with pip and get up and running in minutes\nGetting started"
  },
  {
    "objectID": "index.html#its-just-python",
    "href": "index.html#its-just-python",
    "title": "MIKE IO: input/output of MIKE files in Python",
    "section": " It’s just Python",
    "text": "It’s just Python\nRead, write and manipulate dfs0, dfs1, dfs2, dfs3, dfsu and mesh files.\nAPI Reference"
  },
  {
    "objectID": "index.html#associated-package",
    "href": "index.html#associated-package",
    "title": "MIKE IO: input/output of MIKE files in Python",
    "section": " Associated package",
    "text": "Associated package\nSee our sister library MIKE IO 1D for .res1d and .xns11 files.\nMIKE IO 1D"
  },
  {
    "objectID": "index.html#open-source",
    "href": "index.html#open-source",
    "title": "MIKE IO: input/output of MIKE files in Python",
    "section": " Open Source",
    "text": "Open Source\nMIKE IO is licensed under BSD-3-Clause license and available on GitHub\nDesign philosophy"
  },
  {
    "objectID": "examples/dfs0/relative_time.html",
    "href": "examples/dfs0/relative_time.html",
    "title": "Dfs0 - Relative time axis",
    "section": "",
    "text": "MIKE IO uses a pandas DatetimeIndex to represent the time dimension in dfs files. If the Dfs file has a relative time axis it will be converted to DatetimeIndex by using 1970-1-1 00:00:00 as start time.\nimport mikeio\nds = mikeio.read(\"../../data/eq_relative.dfs0\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:504)\ntime: 0 days 00:00:00 - 0 days 00:00:56.236909 (504 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  Item 1 &lt;Undefined&gt; (undefined)\n  1:  Item 2 &lt;Undefined&gt; (undefined)\n  2:  Item 3 &lt;Undefined&gt; (undefined)\n  3:  Item 4 &lt;Undefined&gt; (undefined)\n  4:  Item 5 &lt;Undefined&gt; (undefined)\ndf = ds.to_dataframe()\ndf.head()\n\n\n\n\n\n\n\n\nItem 1\nItem 2\nItem 3\nItem 4\nItem 5\n\n\n\n\n0 days 00:00:00\n-0.006862\n-0.000611\n0.177047\n32.484425\n-304.720428\n\n\n0 days 00:00:00.111803\n-0.011746\n-0.000611\n0.189257\n32.292774\n-308.553406\n\n\n0 days 00:00:00.223606\n-0.006862\n-0.000611\n0.189257\n32.292774\n-308.553406\n\n\n0 days 00:00:00.335409\n-0.001978\n0.004273\n0.189257\n32.292774\n-300.887482\n\n\n0 days 00:00:00.447212\n0.002906\n0.009157\n0.177047\n32.292774\n-300.887482\nCorrecting the dataframe index by subtracting start time to get relative time axis.\ndf.index = (df.index - df.index[0]).total_seconds()\ndf.index.name = \"Relative time (s)\"\ndf.head()\n\n\n\n\n\n\n\n\nItem 1\nItem 2\nItem 3\nItem 4\nItem 5\n\n\nRelative time (s)\n\n\n\n\n\n\n\n\n\n0.000000\n-0.006862\n-0.000611\n0.177047\n32.484425\n-304.720428\n\n\n0.111803\n-0.011746\n-0.000611\n0.189257\n32.292774\n-308.553406\n\n\n0.223606\n-0.006862\n-0.000611\n0.189257\n32.292774\n-308.553406\n\n\n0.335409\n-0.001978\n0.004273\n0.189257\n32.292774\n-300.887482\n\n\n0.447212\n0.002906\n0.009157\n0.177047\n32.292774\n-300.887482\ndf['Item 5'].plot();"
  },
  {
    "objectID": "examples/dfs0/relative_time.html#mikecore",
    "href": "examples/dfs0/relative_time.html#mikecore",
    "title": "Dfs0 - Relative time axis",
    "section": "mikecore",
    "text": "mikecore\nAn alternative is to use the underlying library mikecore to read the file.\n\nfrom mikecore.DfsFileFactory import DfsFileFactory\n\ndfs = DfsFileFactory.DfsGenericOpen(\"../../data/eq_relative.dfs0\")\n\nUsing the ReadDfs0DataDouble method you get the data as a numpy array, with the time axis or other type of as the first column,\n\ndata = dfs.ReadDfs0DataDouble()\n\ntype(data)\n\nnumpy.ndarray\n\n\nwhich can be converted to a pandas dataframe. First we extract the name of items (which in this example hapeens to be not very creative).\n\nindex_name = \"time\"\nitems = [i.Name for i in dfs.ItemInfo]\nitems = [index_name] + items\nitems\n\n['time', 'Item 1', 'Item 2', 'Item 3', 'Item 4', 'Item 5']\n\n\n\nimport pandas as df\n\ndf = df.DataFrame(data, columns=items).set_index(index_name)\ndf.head()\n\n\n\n\n\n\n\n\nItem 1\nItem 2\nItem 3\nItem 4\nItem 5\n\n\ntime\n\n\n\n\n\n\n\n\n\n0.000000\n-0.006862\n-0.000611\n0.177047\n32.484425\n-304.720428\n\n\n0.111803\n-0.011746\n-0.000611\n0.189257\n32.292774\n-308.553406\n\n\n0.223606\n-0.006862\n-0.000611\n0.189257\n32.292774\n-308.553406\n\n\n0.335409\n-0.001978\n0.004273\n0.189257\n32.292774\n-300.887482\n\n\n0.447212\n0.002906\n0.009157\n0.177047\n32.292774\n-300.887482"
  },
  {
    "objectID": "examples/dfs0/index.html",
    "href": "examples/dfs0/index.html",
    "title": "Dfs0 examples",
    "section": "",
    "text": "A collection of specific examples of working with dfs0 files. For a general introduction to dfs0 see the user guide and the API reference.\n\nCMEMS In-situ data\nRelative time",
    "crumbs": [
      "Examples",
      "Dfs0 examples"
    ]
  },
  {
    "objectID": "examples/dfs2/bathy.html",
    "href": "examples/dfs2/bathy.html",
    "title": "Dfs2 - Bathymetric data",
    "section": "",
    "text": "GEBCO Compilation Group (2020) GEBCO 2020 Grid (doi:10.5285/a29c5465-b138-234d-e053-6c86abc040b9)\n\nimport xarray\nimport mikeio\n\n\nds = xarray.open_dataset(\"../../data/gebco_2020_n56.3_s55.2_w12.2_e13.1.nc\")\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 118kB\nDimensions:    (lat: 264, lon: 216)\nCoordinates: (2)\nData variables:\n    elevation  (lat, lon) int16 114kB ...\nAttributes: (8)xarray.DatasetDimensions:lat: 264lon: 216Coordinates: (2)lat(lat)float6455.2 55.21 55.21 ... 56.29 56.3standard_name :latitudelong_name :latitudeunits :degrees_northaxis :Ysdn_parameter_urn :SDN:P01::ALATZZ01sdn_parameter_name :Latitude northsdn_uom_urn :SDN:P06::DEGNsdn_uom_name :Degrees northarray([55.202083, 55.20625 , 55.210417, ..., 56.289583, 56.29375 , 56.297917],\n      shape=(264,))lon(lon)float6412.2 12.21 12.21 ... 13.09 13.1standard_name :longitudelong_name :longitudeunits :degrees_eastaxis :Xsdn_parameter_urn :SDN:P01::ALONZZ01sdn_parameter_name :Longitude eastsdn_uom_urn :SDN:P06::DEGEsdn_uom_name :Degrees eastarray([12.202083, 12.20625 , 12.210417, ..., 13.089583, 13.09375 , 13.097917],\n      shape=(216,))Data variables: (1)elevation(lat, lon)int16...standard_name :height_above_reference_ellipsoidlong_name :Elevation relative to sea levelunits :msdn_parameter_urn :SDN:P01::BATHHGHTsdn_parameter_name :Sea floor height (above mean sea level) {bathymetric height}sdn_uom_urn :SDN:P06::ULAAsdn_uom_name :Metres[57024 values with dtype=int16]Attributes: (8)Conventions :CF-1.6title :The GEBCO_2020 Grid - a continuous terrain model for oceans and land at 15 arc-second intervalsinstitution :On behalf of the General Bathymetric Chart of the Oceans (GEBCO), the data are held at the British Oceanographic Data Centre (BODC).source :The GEBCO_2020 Grid is the latest global bathymetric product released by the General Bathymetric Chart of the Oceans (GEBCO) and has been developed through the Nippon Foundation-GEBCO Seabed 2030 Project. This is a collaborative project between the Nippon Foundation of Japan and GEBCO. The Seabed 2030 Project aims to bring together all available bathymetric data to produce the definitive map of the world ocean floor and make it available to all.history :Information on the development of the data set and the source data sets included in the grid can be found in the data set documentation available from https://www.gebco.netreferences :DOI: 10.5285/a29c5465-b138-234d-e053-6c86abc040b9comment :The data in the GEBCO_2020 Grid should not be used for navigation or any purpose relating to safety at sea.node_offset :1.0\n\n\n\nds.elevation.plot();\n\n\n\n\n\n\n\n\n\nds.elevation.sel(lon=12.74792, lat=55.865, method=\"nearest\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'elevation' ()&gt; Size: 2B\n[1 values with dtype=int16]\nCoordinates: (2)\nAttributes: (7)xarray.DataArray'elevation'...[1 values with dtype=int16]Coordinates: (2)lat()float6455.86standard_name :latitudelong_name :latitudeunits :degrees_northaxis :Ysdn_parameter_urn :SDN:P01::ALATZZ01sdn_parameter_name :Latitude northsdn_uom_urn :SDN:P06::DEGNsdn_uom_name :Degrees northarray(55.86458333)lon()float6412.75standard_name :longitudelong_name :longitudeunits :degrees_eastaxis :Xsdn_parameter_urn :SDN:P01::ALONZZ01sdn_parameter_name :Longitude eastsdn_uom_urn :SDN:P06::DEGEsdn_uom_name :Degrees eastarray(12.74791667)Attributes: (7)standard_name :height_above_reference_ellipsoidlong_name :Elevation relative to sea levelunits :msdn_parameter_urn :SDN:P01::BATHHGHTsdn_parameter_name :Sea floor height (above mean sea level) {bathymetric height}sdn_uom_urn :SDN:P06::ULAAsdn_uom_name :Metres\n\n\nCheck ordering of dimensions, should be (y,x)\n\nds.elevation.dims\n\n('lat', 'lon')\n\n\n\nel = ds.elevation.values\nel.shape\n\n(264, 216)\n\n\nCheck that axes are increasing, S-&gt;N W-&gt;E\n\nds.lat.values[0],ds.lat.values[-1] \n\n(np.float64(55.20208333333332), np.float64(56.29791666666665))\n\n\n\nds.lat.values[0] &lt; ds.lat.values[-1] \n\nnp.True_\n\n\n\nds.lon.values[0],ds.lon.values[-1] \n\n(np.float64(12.20208333333332), np.float64(13.097916666666663))\n\n\n\nel[0,0] # Bottom left\n\nnp.int16(-8)\n\n\n\nel[-1,0] # Top Left\n\nnp.int16(-31)\n\n\n\ngeometry = mikeio.Grid2D(x=ds.lon.values, y=ds.lat.values, projection=\"LONG/LAT\")\ngeometry\n\n&lt;mikeio.Grid2D&gt;\nx: [12.2, 12.21, ..., 13.1] (nx=216, dx=0.004167)\ny: [55.2, 55.21, ..., 56.3] (ny=264, dy=0.004167)\nprojection: LONG/LAT\n\n\n\nda = mikeio.DataArray(data=el,\n               item=mikeio.ItemInfo(\"Elevation\", mikeio.EUMType.Total_Water_Depth),\n               geometry=geometry,\n               dims=(\"y\",\"x\") # No time dimension\n               )\nda\n\n&lt;mikeio.DataArray&gt;\nname: Elevation\ndims: (y:264, x:216)\ntime: 2018-01-01 00:00:00 (time-invariant)\ngeometry: Grid2D (ny=264, nx=216)\n\n\n\nda.plot();\n\n\n\n\n\n\n\n\n\nda.plot(cmap='coolwarm', vmin=-100, vmax=100);\n\n\n\n\n\n\n\n\n\nda.to_dfs(\"gebco.dfs2\")\n\n\nds = mikeio.read(\"gebco.dfs2\")\nds.Elevation.plot()\n\n\n\n\n\n\n\n\n\nClean up\n\nimport os\n\nos.remove(\"gebco.dfs2\")"
  },
  {
    "objectID": "examples/dfs2/index.html",
    "href": "examples/dfs2/index.html",
    "title": "Dfs2 examples",
    "section": "",
    "text": "A collection of specific examples of working with dfs2 files. For a general introduction to dfsu see the user guide and the API reference.\n\nBathymetry\nMeteo data",
    "crumbs": [
      "Examples",
      "Dfs2 examples"
    ]
  },
  {
    "objectID": "examples/dfsu/spatial_interpolation.html",
    "href": "examples/dfsu/spatial_interpolation.html",
    "title": "Dfsu - 2D interpolation",
    "section": "",
    "text": "import mikeio\nds = mikeio.read(\"../../data/wind_north_sea.dfsu\", items=\"Wind speed\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:6, element:958)\ntime: 2017-10-27 00:00:00 - 2017-10-27 05:00:00 (6 records)\ngeometry: Dfsu2D (958 elements, 570 nodes)\nitems:\n  0:  Wind speed &lt;Wind speed&gt; (meter per sec)\nda = ds.Wind_speed\nda.plot();"
  },
  {
    "objectID": "examples/dfsu/spatial_interpolation.html#interpolate-to-grid",
    "href": "examples/dfsu/spatial_interpolation.html#interpolate-to-grid",
    "title": "Dfsu - 2D interpolation",
    "section": "Interpolate to grid",
    "text": "Interpolate to grid\n\nGet an overset grid covering the domain\nThen interpolate all data to the new grid and plot.\nThe interpolated data is then saved to a dfs2 file.\n\n\ng = da.geometry.get_overset_grid(dx=0.1)\ng\n\n&lt;mikeio.Grid2D&gt;\nx: [-1.563, -1.463, ..., 8.837] (nx=105, dx=0.1)\ny: [49.9, 50, ..., 55.3] (ny=55, dy=0.1)\nprojection: LONG/LAT\n\n\n\nda_grid = da.interp_like(g)\nda_grid\n\n&lt;mikeio.DataArray&gt;\nname: Wind speed\ndims: (time:6, y:55, x:105)\ntime: 2017-10-27 00:00:00 - 2017-10-27 05:00:00 (6 records)\ngeometry: Grid2D (ny=55, nx=105)\n\n\n\nda_grid.plot();"
  },
  {
    "objectID": "examples/dfsu/spatial_interpolation.html#save-to-dfs2-file",
    "href": "examples/dfsu/spatial_interpolation.html#save-to-dfs2-file",
    "title": "Dfsu - 2D interpolation",
    "section": "Save to dfs2 file",
    "text": "Save to dfs2 file\nda_grid.to_dfs(\"wind_north_sea_interpolated.dfs2\")"
  },
  {
    "objectID": "examples/dfsu/spatial_interpolation.html#save-to-netcdf",
    "href": "examples/dfsu/spatial_interpolation.html#save-to-netcdf",
    "title": "Dfsu - 2D interpolation",
    "section": "Save to NetCDF",
    "text": "Save to NetCDF\nxr_da = da_grid.to_xarray()\nxr_da.to_netcdf(\"wind_north_sea_interpolated.nc\")"
  },
  {
    "objectID": "examples/dfsu/spatial_interpolation.html#save-to-geotiff",
    "href": "examples/dfsu/spatial_interpolation.html#save-to-geotiff",
    "title": "Dfsu - 2D interpolation",
    "section": "Save to GeoTiff",
    "text": "Save to GeoTiff\n\n\n\n\n\n\nNote\n\n\n\nThis section requires the rasterio package.\n\n\nimport numpy as np\nimport rasterio\nfrom rasterio.transform import from_origin\n\nwith rasterio.open(\n     fp='wind.tif',\n     mode='w',\n     driver='GTiff',\n     height=g.ny,\n     width=g.nx,\n     count=1,\n     dtype=da.dtype,\n     crs='+proj=latlong', # adjust accordingly for projected coordinate systems\n     transform=from_origin(g.bbox.left, g.bbox.top, g.dx, g.dy)\n     ) as dst:\n        dst.write(np.flipud(da_grid[0].to_numpy()), 1) # first time_step"
  },
  {
    "objectID": "examples/dfsu/spatial_interpolation.html#interpolate-scatter-data-to-mesh",
    "href": "examples/dfsu/spatial_interpolation.html#interpolate-scatter-data-to-mesh",
    "title": "Dfsu - 2D interpolation",
    "section": "Interpolate scatter data to mesh",
    "text": "Interpolate scatter data to mesh\nWe want to interpolate scatter data onto an existing mesh and create a new dfsu with the interpolated data.\nThis uses lower level private utility methods not part of the public API.\n\nfrom mikeio.spatial._distance import dist_in_meters\nfrom mikeio._interpolation import get_idw_interpolant\n\n\ndfs = mikeio.open(\"../../data/wind_north_sea.dfsu\")\n\n\ndfs.geometry.plot.mesh();\n\n\n\n\n\n\n\n\n\n# scatter data: x,y,value for 4 points\nscatter= np.array([[1,50,1], [4, 52, 3], [8, 55, 2], [-1, 55, 1.5]])\nscatter\n\narray([[ 1. , 50. ,  1. ],\n       [ 4. , 52. ,  3. ],\n       [ 8. , 55. ,  2. ],\n       [-1. , 55. ,  1.5]])\n\n\nLet’s first try the approx for a single element:\n\ncalc distance to all interpolation points\ncalc IDW interpolatant weights\nInterpolate\n\n\ndist = dist_in_meters(scatter[:,:2], dfs.geometry.element_coordinates[0,:2], is_geo=dfs.geometry.is_geo)\ndist\n\narray([442832.13895391, 276021.47121657, 435601.93113657, 197561.02073575])\n\n\n\nw = get_idw_interpolant(dist, p=2)\nw\n\narray([0.10382406, 0.26723327, 0.10729925, 0.52164343])\n\n\n\nnp.dot(scatter[:,2], w) # interpolated value in element 0\n\nnp.float64(1.902587501462453)\n\n\nLet’s do the same for all points in the mesh and plot in the end\n\ndati = np.zeros((1, dfs.geometry.n_elements))\nfor j in range(dfs.geometry.n_elements):\n    dist = dist_in_meters(scatter[:, :2], dfs.geometry.element_coordinates[j, :2], is_geo=dfs.geometry.is_geo)\n    w = get_idw_interpolant(dist, p=2)\n    dati[0, j] = np.dot(scatter[:, 2], w)\n\n\nda = mikeio.DataArray(data=dati, geometry=dfs.geometry, time=dfs.start_time)\nda\n\n&lt;mikeio.DataArray&gt;\nname: NoName\ndims: (time:1, element:958)\ntime: 2017-10-27 00:00:00 (time-invariant)\ngeometry: Dfsu2D (958 elements, 570 nodes)\n\n\n\nax = da.plot(title=\"Interpolated scatter data\");\nax.scatter(scatter[:,0], scatter[:,1], s=30, c='red')\n\n\n\n\n\n\n\n\nda.to_dfs(\"interpolated_scatter.dfsu\")"
  },
  {
    "objectID": "examples/index.html",
    "href": "examples/index.html",
    "title": "Examples",
    "section": "",
    "text": "TipExample data\n\n\n\n\n\nIf you want to try one of the examples you need to first download some data files.\nThe files are stored on GitHub in the MIKE IO repo.\nThe easiest way is to the download the repo as a zip file and extract the files you need.\nIn the zip file you find the files in tests/testdata/ folder\nSome of the files:\ntests/testdata\n├── FakeLake.dfsu\n├── HD2D.dfsu\n├── NorthSea_HD_and_windspeed.dfsu\n├── consistency\n│   ├── oresundHD.dfs2\n│   └── oresundHD.dfsu\n├── gebco_2020_n56.3_s55.2_w12.2_e13.1.nc\n├── gfs_wind.nc\n├── odense_rough.mesh\n├── oresund_sigma_z.dfsu\n├── pfs\n│   ├── concat.mzt\n│   └── t1_t0.mzt\n├── tide1.dfs1\n├── tide2.dfs1\n\n\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\n\n\n\n\n\nDfs0 - CMEMS in-situ data\n\n\n\n\n\n\n\n\n\nDfs0 - Relative time axis\n\n\n\n\n\n\n\n\n\nDfs0 examples\n\n\n\n\n\n\n\n\n\nDfs2 - Bathymetric data\n\n\nConvert GEBCO 2020 NetCDF to dfs2\n\n\n\n\n\n\nDfs2 - Meteo data\n\n\nConversion of NetCDF from Global Forecasting System to Dfs2\n\n\n\n\n\n\nDfs2 examples\n\n\n\n\n\n\n\n\n\nDfsu - 2D interpolation\n\n\nInterpolate dfsu data to a grid, save as dfs2 and geotiff. Interpolate dfsu data to another mesh.\n\n\n\n\n\n\nDfsu examples\n\n\n\n\n\n\n\n\n\nGeneric dfs processing\n\n\n\n\n\n\n\n\n\nMerging subdomain dfsu files\n\n\n\n\n\n\n\n\n\nTime interpolation\n\n\nInterpolate data to a specific time axis\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Examples"
    ]
  },
  {
    "objectID": "api/dfsu.DfsuSpectral.html",
    "href": "api/dfsu.DfsuSpectral.html",
    "title": "dfsu.DfsuSpectral",
    "section": "",
    "text": "dfsu.DfsuSpectral(self, filename)\nDfsu for Spectral data.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nPath to dfsu file\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndeletevalue\nFile delete value.\n\n\ndirections\nDirectional axis.\n\n\nend_time\nFile end time.\n\n\nfrequencies\nFrequency axis.\n\n\ngeometry\nGeometry.\n\n\nitems\nList of items.\n\n\nn_directions\nNumber of directions.\n\n\nn_frequencies\nNumber of frequencies.\n\n\nn_items\nNumber of items.\n\n\nn_timesteps\nNumber of time steps.\n\n\nstart_time\nFile start time.\n\n\ntimestep\nTime step size in seconds.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncalc_Hm0_from_spectrum\nCalculate significant wave height (Hm0) from spectrum.\n\n\nread\nRead data from a spectral dfsu file.\n\n\n\n\n\ndfsu.DfsuSpectral.calc_Hm0_from_spectrum(spectrum, tail=True)\nCalculate significant wave height (Hm0) from spectrum.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspectrum\n(np.ndarray, DataArray)\nfrequency or direction-frequency spectrum\nrequired\n\n\ntail\nbool\nShould a parametric spectral tail be added in the computations? by default True\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nsignificant wave height values\n\n\n\n\n\n\n\ndfsu.DfsuSpectral.read(\n    items=None,\n    time=None,\n    elements=None,\n    nodes=None,\n    area=None,\n    x=None,\n    y=None,\n    keepdims=False,\n    dtype=np.float32,\n)\nRead data from a spectral dfsu file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\nkeepdims\nbool\nWhen reading a single time step only, should the time-dimension be kept in the returned Dataset? by default: False\nFalse\n\n\narea\ntuple[float, float, float, float] | None\nRead only data inside (horizontal) area (spectral area files only) given as a bounding box (tuple with left, lower, right, upper) or as list of coordinates for a polygon, by default None\nNone\n\n\nx\nfloat | None\nRead only data for elements containing the (x,y) points(s), by default None\nNone\n\n\ny\nfloat | None\nRead only data for elements containing the (x,y) points(s), by default None\nNone\n\n\nelements\nSequence[int] | np.ndarray | int | None\nRead only selected element ids (spectral area files only)\nNone\n\n\nnodes\nSequence[int] | np.ndarray | int | None\nRead only selected node ids (spectral line files only)\nNone\n\n\ndtype\nAny\nData type to read. Default is np.float32\nnp.float32\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nA Dataset with dimensions [t,elements/nodes,frequencies,directions]\n\n\n\n\n\n\n&gt;&gt;&gt; mikeio.read(\"tests/testdata/spectra/line_spectra.dfsu\")\n&lt;mikeio.Dataset&gt;\ndims: (time:4, node:10, direction:16, frequency:25)\ntime: 2017-10-27 00:00:00 - 2017-10-27 05:00:00 (4 records)\ngeometry: DfsuSpectral1D (9 elements, 10 nodes)\nitems:\n  0:  Energy density &lt;Wave energy density&gt; (meter pow 2 sec per deg)\n&gt;&gt;&gt; mikeio.read(\"tests/testdata/spectra/area_spectra.dfsu\", time=-1)\n&lt;mikeio.Dataset&gt;\ndims: (element:40, direction:16, frequency:25)\ntime: 2017-10-27 05:00:00 (time-invariant)\ngeometry: DfsuSpectral2D (40 elements, 33 nodes)\nitems:\n  0:  Energy density &lt;Wave energy density&gt; (meter pow 2 sec per deg)"
  },
  {
    "objectID": "api/dfsu.DfsuSpectral.html#parameters",
    "href": "api/dfsu.DfsuSpectral.html#parameters",
    "title": "dfsu.DfsuSpectral",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nPath to dfsu file\nrequired"
  },
  {
    "objectID": "api/dfsu.DfsuSpectral.html#attributes",
    "href": "api/dfsu.DfsuSpectral.html#attributes",
    "title": "dfsu.DfsuSpectral",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndeletevalue\nFile delete value.\n\n\ndirections\nDirectional axis.\n\n\nend_time\nFile end time.\n\n\nfrequencies\nFrequency axis.\n\n\ngeometry\nGeometry.\n\n\nitems\nList of items.\n\n\nn_directions\nNumber of directions.\n\n\nn_frequencies\nNumber of frequencies.\n\n\nn_items\nNumber of items.\n\n\nn_timesteps\nNumber of time steps.\n\n\nstart_time\nFile start time.\n\n\ntimestep\nTime step size in seconds."
  },
  {
    "objectID": "api/dfsu.DfsuSpectral.html#methods",
    "href": "api/dfsu.DfsuSpectral.html#methods",
    "title": "dfsu.DfsuSpectral",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncalc_Hm0_from_spectrum\nCalculate significant wave height (Hm0) from spectrum.\n\n\nread\nRead data from a spectral dfsu file.\n\n\n\n\n\ndfsu.DfsuSpectral.calc_Hm0_from_spectrum(spectrum, tail=True)\nCalculate significant wave height (Hm0) from spectrum.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspectrum\n(np.ndarray, DataArray)\nfrequency or direction-frequency spectrum\nrequired\n\n\ntail\nbool\nShould a parametric spectral tail be added in the computations? by default True\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nsignificant wave height values\n\n\n\n\n\n\n\ndfsu.DfsuSpectral.read(\n    items=None,\n    time=None,\n    elements=None,\n    nodes=None,\n    area=None,\n    x=None,\n    y=None,\n    keepdims=False,\n    dtype=np.float32,\n)\nRead data from a spectral dfsu file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\nkeepdims\nbool\nWhen reading a single time step only, should the time-dimension be kept in the returned Dataset? by default: False\nFalse\n\n\narea\ntuple[float, float, float, float] | None\nRead only data inside (horizontal) area (spectral area files only) given as a bounding box (tuple with left, lower, right, upper) or as list of coordinates for a polygon, by default None\nNone\n\n\nx\nfloat | None\nRead only data for elements containing the (x,y) points(s), by default None\nNone\n\n\ny\nfloat | None\nRead only data for elements containing the (x,y) points(s), by default None\nNone\n\n\nelements\nSequence[int] | np.ndarray | int | None\nRead only selected element ids (spectral area files only)\nNone\n\n\nnodes\nSequence[int] | np.ndarray | int | None\nRead only selected node ids (spectral line files only)\nNone\n\n\ndtype\nAny\nData type to read. Default is np.float32\nnp.float32\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nA Dataset with dimensions [t,elements/nodes,frequencies,directions]\n\n\n\n\n\n\n&gt;&gt;&gt; mikeio.read(\"tests/testdata/spectra/line_spectra.dfsu\")\n&lt;mikeio.Dataset&gt;\ndims: (time:4, node:10, direction:16, frequency:25)\ntime: 2017-10-27 00:00:00 - 2017-10-27 05:00:00 (4 records)\ngeometry: DfsuSpectral1D (9 elements, 10 nodes)\nitems:\n  0:  Energy density &lt;Wave energy density&gt; (meter pow 2 sec per deg)\n&gt;&gt;&gt; mikeio.read(\"tests/testdata/spectra/area_spectra.dfsu\", time=-1)\n&lt;mikeio.Dataset&gt;\ndims: (element:40, direction:16, frequency:25)\ntime: 2017-10-27 05:00:00 (time-invariant)\ngeometry: DfsuSpectral2D (40 elements, 33 nodes)\nitems:\n  0:  Energy density &lt;Wave energy density&gt; (meter pow 2 sec per deg)"
  },
  {
    "objectID": "api/open.html",
    "href": "api/open.html",
    "title": "open",
    "section": "",
    "text": "open(filename, **kwargs)\nOpen a dfs/mesh file (and read the header).\nThe typical workflow for small dfs files is to read all data with mikeio.read instead of using this function. For big files, however, it can be convenient to open the file first with dfs=mikeio.open(…) to inspect it’s content (items, time and shape) and then decide what to read using dfs.read(…)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nfull path and file name to the dfs file.\nrequired\n\n\ntype\nstr\nDfs2 only. Additional information about the file, e.g. “spectral” for spectral dfs2 files. By default: None.\nrequired\n\n\n**kwargs\nAny\nAdditional keyword arguments, e.g. type=“spectral”\n{}\n\n\n\n\n\n\nmikeio.read - read data from a dfs file\n\n\n\n&gt;&gt;&gt; dfs = mikeio.open(\"wl.dfs1\")\n&gt;&gt;&gt; dfs = mikeio.open(\"HD2D.dfsu\")\n&gt;&gt;&gt; ds = dfs.read(items=\"Salinity\", time=\"2016-01\")\n&gt;&gt;&gt; dfs = mikeio.open(\"pt_spectra.dfs2\", type=\"spectral\")"
  },
  {
    "objectID": "api/open.html#parameters",
    "href": "api/open.html#parameters",
    "title": "open",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nfull path and file name to the dfs file.\nrequired\n\n\ntype\nstr\nDfs2 only. Additional information about the file, e.g. “spectral” for spectral dfs2 files. By default: None.\nrequired\n\n\n**kwargs\nAny\nAdditional keyword arguments, e.g. type=“spectral”\n{}"
  },
  {
    "objectID": "api/open.html#see-also",
    "href": "api/open.html#see-also",
    "title": "open",
    "section": "",
    "text": "mikeio.read - read data from a dfs file"
  },
  {
    "objectID": "api/open.html#examples",
    "href": "api/open.html#examples",
    "title": "open",
    "section": "",
    "text": "&gt;&gt;&gt; dfs = mikeio.open(\"wl.dfs1\")\n&gt;&gt;&gt; dfs = mikeio.open(\"HD2D.dfsu\")\n&gt;&gt;&gt; ds = dfs.read(items=\"Salinity\", time=\"2016-01\")\n&gt;&gt;&gt; dfs = mikeio.open(\"pt_spectra.dfs2\", type=\"spectral\")"
  },
  {
    "objectID": "api/Dfs1.html",
    "href": "api/Dfs1.html",
    "title": "Dfs1",
    "section": "",
    "text": "Dfs1(self, filename)\nClass for reading/writing dfs1 files.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nPath to dfs1 file\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndeletevalue\nFile delete value.\n\n\ndx\nStep size in x direction.\n\n\nend_time\nFile end time.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nitems\nList of items.\n\n\nlatitude\nOrigin latitude.\n\n\nlongitude\nOrigin longitude.\n\n\nn_items\nNumber of items.\n\n\nn_timesteps\nNumber of time steps.\n\n\nnx\nNumber of node values.\n\n\norientation\nOrientation (in own CRS).\n\n\norigin\nOrigin (in own CRS).\n\n\nstart_time\nFile start time.\n\n\ntimestep\nTime step size in seconds.\n\n\nx0\nStart point of x values (often 0).\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nread\nRead data from a dfs1 file.\n\n\n\n\n\nDfs1.read(items=None, time=None, keepdims=False, dtype=np.float32)\nRead data from a dfs1 file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\nkeepdims\nbool\nWhen reading a single time step only, should the time-dimension be kept in the returned Dataset? by default: False\nFalse\n\n\ndtype\nAny\nDefine the dtype of the returned dataset (default = np.float32)\nnp.float32\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset"
  },
  {
    "objectID": "api/Dfs1.html#parameters",
    "href": "api/Dfs1.html#parameters",
    "title": "Dfs1",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nPath to dfs1 file\nrequired"
  },
  {
    "objectID": "api/Dfs1.html#attributes",
    "href": "api/Dfs1.html#attributes",
    "title": "Dfs1",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndeletevalue\nFile delete value.\n\n\ndx\nStep size in x direction.\n\n\nend_time\nFile end time.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nitems\nList of items.\n\n\nlatitude\nOrigin latitude.\n\n\nlongitude\nOrigin longitude.\n\n\nn_items\nNumber of items.\n\n\nn_timesteps\nNumber of time steps.\n\n\nnx\nNumber of node values.\n\n\norientation\nOrientation (in own CRS).\n\n\norigin\nOrigin (in own CRS).\n\n\nstart_time\nFile start time.\n\n\ntimestep\nTime step size in seconds.\n\n\nx0\nStart point of x values (often 0)."
  },
  {
    "objectID": "api/Dfs1.html#methods",
    "href": "api/Dfs1.html#methods",
    "title": "Dfs1",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nread\nRead data from a dfs1 file.\n\n\n\n\n\nDfs1.read(items=None, time=None, keepdims=False, dtype=np.float32)\nRead data from a dfs1 file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\nkeepdims\nbool\nWhen reading a single time step only, should the time-dimension be kept in the returned Dataset? by default: False\nFalse\n\n\ndtype\nAny\nDefine the dtype of the returned dataset (default = np.float32)\nnp.float32\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset"
  },
  {
    "objectID": "api/Grid1D.html",
    "href": "api/Grid1D.html",
    "title": "Grid1D",
    "section": "",
    "text": "Grid1D(\n    self,\n    x=None,\n    *,\n    x0=0.0,\n    dx=None,\n    nx=None,\n    projection='NON-UTM',\n    origin=(0.0, 0.0),\n    orientation=0.0,\n    node_coordinates=None,\n    axis_name='x',\n)\n1d spatial grid.\nThe axis is increasing and equidistant"
  },
  {
    "objectID": "api/Grid1D.html#parameters",
    "href": "api/Grid1D.html#parameters",
    "title": "Grid1D",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\narray_like\nnode coordinates\nNone\n\n\nx0\nfloat\nfirst node coordinate\n0.0\n\n\ndx\nfloat\ngrid spacing\nNone\n\n\nnx\nint\nnumber of nodes\nNone\n\n\nprojection\nstr\nprojection string\n'NON-UTM'\n\n\norigin\n(float, float)\nnot commonly used\n(0.0, 0.0)\n\n\norientation\nfloat\nnot commonly used\n0.0\n\n\nnode_coordinates\narray_like\ncoordinates of nodes in 2D or 3D space\nNone\n\n\naxis_name\nstr\nname of axis, by default “x”\n'x'"
  },
  {
    "objectID": "api/Grid1D.html#examples",
    "href": "api/Grid1D.html#examples",
    "title": "Grid1D",
    "section": "Examples",
    "text": "Examples\n\nimport mikeio\nmikeio.Grid1D(nx=3,dx=0.1)\n\n&lt;mikeio.Grid1D&gt;\nx: [0, 0.1, 0.2] (nx=3, dx=0.1)\n\n\n\nmikeio.Grid1D(x=[0.1, 0.5, 0.9])\n\n&lt;mikeio.Grid1D&gt;\nx: [0.1, 0.5, 0.9] (nx=3, dx=0.4)"
  },
  {
    "objectID": "api/Grid1D.html#attributes",
    "href": "api/Grid1D.html#attributes",
    "title": "Grid1D",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\ndx\nGrid spacing.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nnx\nNumber of grid points.\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string.\n\n\nx\nArray of node coordinates."
  },
  {
    "objectID": "api/Grid1D.html#methods",
    "href": "api/Grid1D.html#methods",
    "title": "Grid1D",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nfind_index\nFind nearest point.\n\n\nisel\nGet a subset geometry from this geometry.\n\n\n\n\nfind_index\nGrid1D.find_index(x, **kwargs)\nFind nearest point.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nfloat\nx-coordinate of point\nrequired\n\n\n**kwargs\nAny\nNot used\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nint\nindex of nearest point\n\n\n\n\n\nSee Also\nmikeio.Dataset.sel\n\n\n\nisel\nGrid1D.isel(idx, axis=None)\nGet a subset geometry from this geometry.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\nint or slice\nindex or slice\nrequired\n\n\naxis\nint\nNot used for Grid1D, by default None\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nGeometryPoint2D or GeometryPoint3D or GeometryUndefined\nThe geometry of the selected point\n\n\n\n\n\nExamples\n\nimport mikeio\ng = mikeio.Grid1D(nx=3,dx=0.1)\ng\n\n&lt;mikeio.Grid1D&gt;\nx: [0, 0.1, 0.2] (nx=3, dx=0.1)\n\n\n\ng.isel([1,2])\n\n&lt;mikeio.Grid1D&gt;\nx: [0.1, 0.2] (nx=2, dx=0.1)\n\n\n\ng.isel(1)\n\nGeometryUndefined()"
  },
  {
    "objectID": "api/PfsDocument.html",
    "href": "api/PfsDocument.html",
    "title": "PfsDocument",
    "section": "",
    "text": "PfsDocument(self, data, *, encoding='cp1252', unique_keywords=False)\nCreate a PfsDocument object for reading, writing and manipulating pfs files."
  },
  {
    "objectID": "api/PfsDocument.html#parameters",
    "href": "api/PfsDocument.html#parameters",
    "title": "PfsDocument",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nTextIO | Mapping[str | PfsSection, Any] | Sequence[PfsSection] | str | Path\nEither a file name (including full path) to the pfs file to be read or dictionary-like structure.\nrequired\n\n\nencoding\nstr\nHow is the pfs file encoded? By default cp1252\n'cp1252'\n\n\nunique_keywords\nbool\nShould the keywords in a section be unique? Some tools e.g. the MIKE Plot Composer allows non-unique keywords. If True: warnings will be issued if non-unique keywords are present and the first occurence will be used by default False\nFalse"
  },
  {
    "objectID": "api/PfsDocument.html#attributes",
    "href": "api/PfsDocument.html#attributes",
    "title": "PfsDocument",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\nis_unique\nAre the target (root) names unique?\n\n\nn_targets\nNumber of targets (root sections).\n\n\nnames\nNames of the targets (root sections) as a list.\n\n\ntargets\nList of targets (root sections)."
  },
  {
    "objectID": "api/PfsDocument.html#methods",
    "href": "api/PfsDocument.html#methods",
    "title": "PfsDocument",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nclear\nRemove all items from the PfsSection.\n\n\ncopy\nReturn a deep copy of the PfsDocument.\n\n\nfind_replace\nUpdate recursively all old_value with new_value.\n\n\nfrom_dataframe\nCreate a PfsSection from a DataFrame.\n\n\nfrom_text\nCreate a PfsDocument from a string.\n\n\nget\nReturn the value for key if key is in the PfsSection,\n\n\nitems\nReturn a new view of the PfsDocument’s items ((key, value) pairs).\n\n\nkeys\nReturn a list of the PfsDocument’s keys (target names).\n\n\npop\nIf key is in the dictionary, remove it and return its\n\n\nsearch\nFind recursively all keys, sections or parameters\n\n\nto_dataframe\nOutput enumerated subsections to a DataFrame.\n\n\nto_dict\nConvert to (nested) dict (as a copy).\n\n\nvalues\nReturn a list of the PfsDocument’s values (targets).\n\n\nwrite\nWrite object to a pfs file.\n\n\n\n\nclear\nPfsDocument.clear()\nRemove all items from the PfsSection.\n\n\ncopy\nPfsDocument.copy()\nReturn a deep copy of the PfsDocument.\n\n\nfind_replace\nPfsDocument.find_replace(old_value, new_value)\nUpdate recursively all old_value with new_value.\n\n\nfrom_dataframe\nPfsDocument.from_dataframe(df, prefix)\nCreate a PfsSection from a DataFrame.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\ndata\nrequired\n\n\nprefix\nstr\nsection header prefix\nrequired\n\n\n\n\n\nExamples\n\nimport pandas as pd\nimport mikeio\ndf = pd.DataFrame(dict(station=[\"Foo\", \"Bar\"],include=[0,1]), index=[1,2])\ndf\n\n\n\n\n\n\n\n\nstation\ninclude\n\n\n\n\n1\nFoo\n0\n\n\n2\nBar\n1\n\n\n\n\n\n\n\n\nmikeio.PfsSection.from_dataframe(df,\"STATION_\")\n\n[STATION_1]\n   station = 'Foo'\n   include = 0\nEndSect  // STATION_1\n[STATION_2]\n   station = 'Bar'\n   include = 1\nEndSect  // STATION_2\n\n\n\n\n\nfrom_text\nPfsDocument.from_text(text)\nCreate a PfsDocument from a string.\n\n\nget\nPfsDocument.get(key, default=None)\nReturn the value for key if key is in the PfsSection, else default. If default is not given, it defaults to None, so that this method never raises a KeyError.\n\n\nitems\nPfsDocument.items()\nReturn a new view of the PfsDocument’s items ((key, value) pairs).\n\n\nkeys\nPfsDocument.keys()\nReturn a list of the PfsDocument’s keys (target names).\n\n\npop\nPfsDocument.pop(key, default=None)\nIf key is in the dictionary, remove it and return its value, else return default. If default is not given and key is not in the dictionary, a KeyError is raised.\n\n\nsearch\nPfsDocument.search(text=None, *, key=None, section=None, param=None, case=False)\nFind recursively all keys, sections or parameters matching a pattern.\nNOTE: logical OR between multiple conditions\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntext\nstr\nSearch for text in either key, section or parameter, by default None\nNone\n\n\nkey\nstr\ntext pattern to seach for in keywords, by default None\nNone\n\n\nsection\nstr\ntext pattern to seach for in sections, by default None\nNone\n\n\nparam\n(str, bool, float, int)\ntext or value in a parameter, by default None\nNone\n\n\ncase\nbool\nshould the text search be case-sensitive?, by default False\nFalse\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nPfsSection\nSearch result as a nested PfsSection\n\n\n\n\n\n\nto_dataframe\nPfsDocument.to_dataframe(prefix=None)\nOutput enumerated subsections to a DataFrame.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprefix\nstr\nThe prefix of the enumerated sections, e.g. “OUTPUT_”, which can be supplied if it fails without this argument, by default None (will try to “guess” the prefix)\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\nThe enumerated subsections as a DataFrame\n\n\n\n\n\nExamples\n\npfs = mikeio.read_pfs(\"../data/pfs/lake.sw\")\npfs.SW.OUTPUTS.to_dataframe(prefix=\"OUTPUT_\")\n\n\n\n\n\n\n\n\nTouched\ninclude\ntitle\nfile_name\ntype\nformat\nflood_and_dry\ncoordinate_type\nzone\ninput_file_name\n...\nlast_time_step\ntime_step_frequency\nnumber_of_points\nPOINT_1\nLINE\nAREA\nINTEGRAL_WAVE_PARAMETERS\nINPUT_PARAMETERS\nMODEL_PARAMETERS\nSPECTRAL_PARAMETERS\n\n\n\n\n1\n1\n1\nWave parameters in domain\nWave_parameters.dfsu\n1\n2\n2\nUTM-32\n0\n||\n...\n450\n10\n1\n{'name': 'POINT_1', 'x': 20000.0, 'y': 20000.0}\n{'npoints': 3, 'x_first': 0.0, 'y_first': 0.0,...\n{'number_of_points': 4, 'POINT_1': {'x': -400....\n{'Touched': 1, 'type_of_spectrum': 1, 'minimum...\n{'Touched': 1, 'Surface_elevation': 0, 'Water_...\n{'Touched': 1, 'Wind_friction_speed': 0, 'Roug...\n{'Touched': 1, 'separation_of_wind_sea_and_swe...\n\n\n2\n1\n0\nWave parameters along line\nWave_line.dfs1\n1\n1\n2\nUTM-32\n0\n||\n...\n450\n10\n1\n{'name': 'POINT_1', 'x': 20000.0, 'y': 20000.0}\n{'npoints': 41, 'x_first': 0.0, 'y_first': 200...\n{'number_of_points': 4, 'POINT_1': {'x': -400....\n{'Touched': 1, 'type_of_spectrum': 1, 'minimum...\n{'Touched': 1, 'Surface_elevation': 0, 'Water_...\n{'Touched': 1, 'Wind_friction_speed': 0, 'Roug...\n{'Touched': 1, 'separation_of_wind_sea_and_swe...\n\n\n3\n1\n1\nWave parameters  in a point\nWaves_x20km_y20km.dfs0\n1\n0\n2\nUTM-32\n0\n||\n...\n450\n1\n1\n{'name': 'POINT_1', 'x': 38000.0, 'y': 20000.0}\n{'npoints': 3, 'x_first': 0.0, 'y_first': 0.0,...\n{'number_of_points': 4, 'POINT_1': {'x': -400....\n{'Touched': 1, 'type_of_spectrum': 1, 'minimum...\n{'Touched': 1, 'Surface_elevation': 0, 'Water_...\n{'Touched': 1, 'Wind_friction_speed': 0, 'Roug...\n{'Touched': 1, 'separation_of_wind_sea_and_swe...\n\n\n4\n1\n1\nSpectrum in a point\nspectrum_x20km_y20km.dfsu\n4\n0\n2\nUTM-32\n0\n||\n...\n450\n10\n1\n{'name': 'POINT_1', 'x': 38000.0, 'y': 20000.0}\n{'npoints': 3, 'x_first': 0.0, 'y_first': 0.0,...\n{'number_of_points': 4, 'POINT_1': {'x': -400....\n{'Touched': 1, 'type_of_spectrum': 1, 'minimum...\n{'Touched': 1, 'Surface_elevation': 0, 'Water_...\n{'Touched': 1, 'Wind_friction_speed': 0, 'Roug...\n{'Touched': 1, 'separation_of_wind_sea_and_swe...\n\n\n\n\n4 rows × 24 columns\n\n\n\n\n\n\nto_dict\nPfsDocument.to_dict()\nConvert to (nested) dict (as a copy).\n\n\nvalues\nPfsDocument.values()\nReturn a list of the PfsDocument’s values (targets).\n\n\nwrite\nPfsDocument.write(filename)\nWrite object to a pfs file.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nFull path and filename of pfs to be created.\nrequired\n\n\n\n\n\nNotes\nTo return the content as a string, use repr()"
  },
  {
    "objectID": "api/PfsSection.html",
    "href": "api/PfsSection.html",
    "title": "PfsSection",
    "section": "",
    "text": "PfsSection(self, dictionary, **kwargs)\nClass for reading/writing sections in a pfs file."
  },
  {
    "objectID": "api/PfsSection.html#methods",
    "href": "api/PfsSection.html#methods",
    "title": "PfsSection",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nclear\nRemove all items from the PfsSection.\n\n\ncopy\nReturn a copy of the PfsSection.\n\n\nfind_replace\nUpdate recursively all old_value with new_value.\n\n\nfrom_dataframe\nCreate a PfsSection from a DataFrame.\n\n\nget\nReturn the value for key if key is in the PfsSection,\n\n\nitems\nReturn a new view of the PfsSection’s items ((key, value) pairs).\n\n\nkeys\nReturn a new view of the PfsSection’s keys.\n\n\npop\nIf key is in the dictionary, remove it and return its\n\n\nsearch\nFind recursively all keys, sections or parameters\n\n\nto_dataframe\nOutput enumerated subsections to a DataFrame.\n\n\nto_dict\nConvert to (nested) dict (as a copy).\n\n\nvalues\nReturn a new view of the PfsSection’s values.\n\n\n\n\nclear\nPfsSection.clear()\nRemove all items from the PfsSection.\n\n\ncopy\nPfsSection.copy()\nReturn a copy of the PfsSection.\n\n\nfind_replace\nPfsSection.find_replace(old_value, new_value)\nUpdate recursively all old_value with new_value.\n\n\nfrom_dataframe\nPfsSection.from_dataframe(df, prefix)\nCreate a PfsSection from a DataFrame.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\ndata\nrequired\n\n\nprefix\nstr\nsection header prefix\nrequired\n\n\n\n\n\nExamples\n\nimport pandas as pd\nimport mikeio\ndf = pd.DataFrame(dict(station=[\"Foo\", \"Bar\"],include=[0,1]), index=[1,2])\ndf\n\n\n\n\n\n\n\n\nstation\ninclude\n\n\n\n\n1\nFoo\n0\n\n\n2\nBar\n1\n\n\n\n\n\n\n\n\nmikeio.PfsSection.from_dataframe(df,\"STATION_\")\n\n[STATION_1]\n   station = 'Foo'\n   include = 0\nEndSect  // STATION_1\n[STATION_2]\n   station = 'Bar'\n   include = 1\nEndSect  // STATION_2\n\n\n\n\n\nget\nPfsSection.get(key, default=None)\nReturn the value for key if key is in the PfsSection, else default. If default is not given, it defaults to None, so that this method never raises a KeyError.\n\n\nitems\nPfsSection.items()\nReturn a new view of the PfsSection’s items ((key, value) pairs).\n\n\nkeys\nPfsSection.keys()\nReturn a new view of the PfsSection’s keys.\n\n\npop\nPfsSection.pop(key, default=None)\nIf key is in the dictionary, remove it and return its value, else return default. If default is not given and key is not in the dictionary, a KeyError is raised.\n\n\nsearch\nPfsSection.search(text=None, *, key=None, section=None, param=None, case=False)\nFind recursively all keys, sections or parameters matching a pattern.\nNOTE: logical OR between multiple conditions\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntext\nstr\nSearch for text in either key, section or parameter, by default None\nNone\n\n\nkey\nstr\ntext pattern to seach for in keywords, by default None\nNone\n\n\nsection\nstr\ntext pattern to seach for in sections, by default None\nNone\n\n\nparam\n(str, bool, float, int)\ntext or value in a parameter, by default None\nNone\n\n\ncase\nbool\nshould the text search be case-sensitive?, by default False\nFalse\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nPfsSection\nSearch result as a nested PfsSection\n\n\n\n\n\n\nto_dataframe\nPfsSection.to_dataframe(prefix=None)\nOutput enumerated subsections to a DataFrame.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprefix\nstr\nThe prefix of the enumerated sections, e.g. “OUTPUT_”, which can be supplied if it fails without this argument, by default None (will try to “guess” the prefix)\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\nThe enumerated subsections as a DataFrame\n\n\n\n\n\nExamples\n\npfs = mikeio.read_pfs(\"../data/pfs/lake.sw\")\npfs.SW.OUTPUTS.to_dataframe(prefix=\"OUTPUT_\")\n\n\n\n\n\n\n\n\nTouched\ninclude\ntitle\nfile_name\ntype\nformat\nflood_and_dry\ncoordinate_type\nzone\ninput_file_name\n...\nlast_time_step\ntime_step_frequency\nnumber_of_points\nPOINT_1\nLINE\nAREA\nINTEGRAL_WAVE_PARAMETERS\nINPUT_PARAMETERS\nMODEL_PARAMETERS\nSPECTRAL_PARAMETERS\n\n\n\n\n1\n1\n1\nWave parameters in domain\nWave_parameters.dfsu\n1\n2\n2\nUTM-32\n0\n||\n...\n450\n10\n1\n{'name': 'POINT_1', 'x': 20000.0, 'y': 20000.0}\n{'npoints': 3, 'x_first': 0.0, 'y_first': 0.0,...\n{'number_of_points': 4, 'POINT_1': {'x': -400....\n{'Touched': 1, 'type_of_spectrum': 1, 'minimum...\n{'Touched': 1, 'Surface_elevation': 0, 'Water_...\n{'Touched': 1, 'Wind_friction_speed': 0, 'Roug...\n{'Touched': 1, 'separation_of_wind_sea_and_swe...\n\n\n2\n1\n0\nWave parameters along line\nWave_line.dfs1\n1\n1\n2\nUTM-32\n0\n||\n...\n450\n10\n1\n{'name': 'POINT_1', 'x': 20000.0, 'y': 20000.0}\n{'npoints': 41, 'x_first': 0.0, 'y_first': 200...\n{'number_of_points': 4, 'POINT_1': {'x': -400....\n{'Touched': 1, 'type_of_spectrum': 1, 'minimum...\n{'Touched': 1, 'Surface_elevation': 0, 'Water_...\n{'Touched': 1, 'Wind_friction_speed': 0, 'Roug...\n{'Touched': 1, 'separation_of_wind_sea_and_swe...\n\n\n3\n1\n1\nWave parameters  in a point\nWaves_x20km_y20km.dfs0\n1\n0\n2\nUTM-32\n0\n||\n...\n450\n1\n1\n{'name': 'POINT_1', 'x': 38000.0, 'y': 20000.0}\n{'npoints': 3, 'x_first': 0.0, 'y_first': 0.0,...\n{'number_of_points': 4, 'POINT_1': {'x': -400....\n{'Touched': 1, 'type_of_spectrum': 1, 'minimum...\n{'Touched': 1, 'Surface_elevation': 0, 'Water_...\n{'Touched': 1, 'Wind_friction_speed': 0, 'Roug...\n{'Touched': 1, 'separation_of_wind_sea_and_swe...\n\n\n4\n1\n1\nSpectrum in a point\nspectrum_x20km_y20km.dfsu\n4\n0\n2\nUTM-32\n0\n||\n...\n450\n10\n1\n{'name': 'POINT_1', 'x': 38000.0, 'y': 20000.0}\n{'npoints': 3, 'x_first': 0.0, 'y_first': 0.0,...\n{'number_of_points': 4, 'POINT_1': {'x': -400....\n{'Touched': 1, 'type_of_spectrum': 1, 'minimum...\n{'Touched': 1, 'Surface_elevation': 0, 'Water_...\n{'Touched': 1, 'Wind_friction_speed': 0, 'Roug...\n{'Touched': 1, 'separation_of_wind_sea_and_swe...\n\n\n\n\n4 rows × 24 columns\n\n\n\n\n\n\nto_dict\nPfsSection.to_dict()\nConvert to (nested) dict (as a copy).\n\n\nvalues\nPfsSection.values()\nReturn a new view of the PfsSection’s values."
  },
  {
    "objectID": "api/dfsu.Dfsu3D.html",
    "href": "api/dfsu.Dfsu3D.html",
    "title": "dfsu.Dfsu3D",
    "section": "",
    "text": "dfsu.Dfsu3D(self, filename)\nClass for reading/writing dfsu 3d files.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nPath to dfsu file\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndeletevalue\nFile delete value.\n\n\nend_time\nFile end time.\n\n\ngeometry2d\nThe 2d geometry for a 3d object.\n\n\nitems\nList of items.\n\n\nn_items\nNumber of items.\n\n\nn_layers\nMaximum number of layers.\n\n\nn_sigma_layers\nNumber of sigma layers.\n\n\nn_timesteps\nNumber of time steps.\n\n\nn_z_layers\nMaximum number of z-layers.\n\n\nstart_time\nFile start time.\n\n\ntimestep\nTime step size in seconds.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nappend\nAppend data to a dfsu file.\n\n\nextract_surface_elevation_from_3d\nExtract surface elevation from a 3d dfsu file (based on zn)\n\n\nread\nRead data from a dfsu file.\n\n\n\n\n\ndfsu.Dfsu3D.append(ds, validate=True)\nAppend data to a dfsu file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nDataset to append\nrequired\n\n\nvalidate\nbool\nValidate that the dataset to append has the same geometry and items, by default True\nTrue\n\n\n\n\n\n\n\ndfsu.Dfsu3D.extract_surface_elevation_from_3d(n_nearest=4)\nExtract surface elevation from a 3d dfsu file (based on zn) to a new 2d dfsu file with a surface elevation item.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn_nearest\nint\nnumber of points for spatial interpolation (inverse_distance), default=4\n4\n\n\n\n\n\n\n\ndfsu.Dfsu3D.read(\n    items=None,\n    time=None,\n    elements=None,\n    area=None,\n    x=None,\n    y=None,\n    z=None,\n    layers=None,\n    keepdims=False,\n    dtype=np.float32,\n    error_bad_data=True,\n    fill_bad_data_value=np.nan,\n)\nRead data from a dfsu file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\nkeepdims\nbool\nWhen reading a single time step only, should the time-dimension be kept in the returned Dataset? by default: False\nFalse\n\n\narea\ntuple[float, float, float, float] | None\nRead only data inside (horizontal) area given as a bounding box (tuple with left, lower, right, upper) or as list of coordinates for a polygon, by default None\nNone\n\n\nx\nfloat | None\nRead only data for elements containing the (x,y,z) points(s)\nNone\n\n\ny\nfloat | None\nRead only data for elements containing the (x,y,z) points(s)\nNone\n\n\nz\nfloat | None\nRead only data for elements containing the (x,y,z) points(s)\nNone\n\n\nlayers\nint | Layer | Sequence[int] | None\nRead only data for specific layers, by default None\nNone\n\n\nelements\nSequence[int] | np.ndarray | None\nRead only selected element ids, by default None\nNone\n\n\nerror_bad_data\nbool\nraise error if data is corrupt, by default True,\nTrue\n\n\nfill_bad_data_value\nfloat\nfill value for to impute corrupt data, used in conjunction with error_bad_data=False default np.nan\nnp.nan\n\n\ndtype\nAny\nData type to read, by default np.float32\nnp.float32\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nA Dataset with data dimensions [t,elements]"
  },
  {
    "objectID": "api/dfsu.Dfsu3D.html#parameters",
    "href": "api/dfsu.Dfsu3D.html#parameters",
    "title": "dfsu.Dfsu3D",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nPath to dfsu file\nrequired"
  },
  {
    "objectID": "api/dfsu.Dfsu3D.html#attributes",
    "href": "api/dfsu.Dfsu3D.html#attributes",
    "title": "dfsu.Dfsu3D",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndeletevalue\nFile delete value.\n\n\nend_time\nFile end time.\n\n\ngeometry2d\nThe 2d geometry for a 3d object.\n\n\nitems\nList of items.\n\n\nn_items\nNumber of items.\n\n\nn_layers\nMaximum number of layers.\n\n\nn_sigma_layers\nNumber of sigma layers.\n\n\nn_timesteps\nNumber of time steps.\n\n\nn_z_layers\nMaximum number of z-layers.\n\n\nstart_time\nFile start time.\n\n\ntimestep\nTime step size in seconds."
  },
  {
    "objectID": "api/dfsu.Dfsu3D.html#methods",
    "href": "api/dfsu.Dfsu3D.html#methods",
    "title": "dfsu.Dfsu3D",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nappend\nAppend data to a dfsu file.\n\n\nextract_surface_elevation_from_3d\nExtract surface elevation from a 3d dfsu file (based on zn)\n\n\nread\nRead data from a dfsu file.\n\n\n\n\n\ndfsu.Dfsu3D.append(ds, validate=True)\nAppend data to a dfsu file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nDataset to append\nrequired\n\n\nvalidate\nbool\nValidate that the dataset to append has the same geometry and items, by default True\nTrue\n\n\n\n\n\n\n\ndfsu.Dfsu3D.extract_surface_elevation_from_3d(n_nearest=4)\nExtract surface elevation from a 3d dfsu file (based on zn) to a new 2d dfsu file with a surface elevation item.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn_nearest\nint\nnumber of points for spatial interpolation (inverse_distance), default=4\n4\n\n\n\n\n\n\n\ndfsu.Dfsu3D.read(\n    items=None,\n    time=None,\n    elements=None,\n    area=None,\n    x=None,\n    y=None,\n    z=None,\n    layers=None,\n    keepdims=False,\n    dtype=np.float32,\n    error_bad_data=True,\n    fill_bad_data_value=np.nan,\n)\nRead data from a dfsu file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\nkeepdims\nbool\nWhen reading a single time step only, should the time-dimension be kept in the returned Dataset? by default: False\nFalse\n\n\narea\ntuple[float, float, float, float] | None\nRead only data inside (horizontal) area given as a bounding box (tuple with left, lower, right, upper) or as list of coordinates for a polygon, by default None\nNone\n\n\nx\nfloat | None\nRead only data for elements containing the (x,y,z) points(s)\nNone\n\n\ny\nfloat | None\nRead only data for elements containing the (x,y,z) points(s)\nNone\n\n\nz\nfloat | None\nRead only data for elements containing the (x,y,z) points(s)\nNone\n\n\nlayers\nint | Layer | Sequence[int] | None\nRead only data for specific layers, by default None\nNone\n\n\nelements\nSequence[int] | np.ndarray | None\nRead only selected element ids, by default None\nNone\n\n\nerror_bad_data\nbool\nraise error if data is corrupt, by default True,\nTrue\n\n\nfill_bad_data_value\nfloat\nfill value for to impute corrupt data, used in conjunction with error_bad_data=False default np.nan\nnp.nan\n\n\ndtype\nAny\nData type to read, by default np.float32\nnp.float32\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nA Dataset with data dimensions [t,elements]"
  },
  {
    "objectID": "api/spatial.GeometryFMAreaSpectrum.html",
    "href": "api/spatial.GeometryFMAreaSpectrum.html",
    "title": "spatial.GeometryFMAreaSpectrum",
    "section": "",
    "text": "spatial.GeometryFMAreaSpectrum(\n    self,\n    node_coordinates,\n    element_table,\n    codes=None,\n    projection='LONG/LAT',\n    dfsu_type=None,\n    element_ids=None,\n    node_ids=None,\n    validate=True,\n    frequencies=None,\n    directions=None,\n    reindex=False,\n)\nFlexible mesh area spectrum geometry.\n\n\n\n\n\nName\nDescription\n\n\n\n\nboundary_codes\nUnique list of boundary codes.\n\n\nboundary_polygons\nLists of polygons defining domain outline.\n\n\ncodes\nNode codes of all nodes (0=water, 1=land, 2…=open boundaries).\n\n\ndirections\nDirectional axis.\n\n\nelement_coordinates\nCenter coordinates of each element.\n\n\nfrequencies\nFrequency axis.\n\n\nis_2d\nType is either mesh or Dfsu2D (2 horizontal dimensions).\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nis_spectral\nType is spectral dfsu (point, line or area spectrum).\n\n\nis_tri_only\nDoes the mesh consist of triangles only.\n\n\nmax_nodes_per_element\nThe maximum number of nodes for an element.\n\n\nn_directions\nNumber of directions.\n\n\nn_elements\nNumber of elements.\n\n\nn_frequencies\nNumber of frequencies.\n\n\nn_nodes\nNumber of nodes.\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncontains\nTest if a list of points are contained by mesh.\n\n\nelements_to_geometry\nexport a selection of elements to new flexible file geometry\n\n\nfind_index\nFind a set of element indicies for a number of points or within an area.\n\n\nfind_nearest_elements\nFind index of nearest elements (optionally for a list).\n\n\nget_2d_interpolant\nIDW interpolant for list of coordinates.\n\n\nget_element_area\nCalculate the horizontal area of each element.\n\n\nget_node_centered_data\nConvert cell-centered data to node-centered by pseudo-laplacian method.\n\n\nget_overset_grid\nGet a 2d grid that covers the domain by specifying spacing or shape.\n\n\nto_mesh\nExport geometry to new mesh file.\n\n\nto_shapely\nExport mesh as shapely MultiPolygon.\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.contains(points)\nTest if a list of points are contained by mesh.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npoints\narray-like n-by-2\nx,y-coordinates of n points to be tested\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool array\nTrue for points inside, False otherwise\n\n\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.elements_to_geometry(elements, keepdims=False)\nexport a selection of elements to new flexible file geometry\n\n\nelements : list(int) list of element ids keepdims: bool Not used\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nGeometryFMAreaSpectrum or GeometryFMPointSpectrum\nwhich can be used for further extraction or saved to file\n\n\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.find_index(\n    x=None,\n    y=None,\n    coords=None,\n    area=None,\n)\nFind a set of element indicies for a number of points or within an area.\nThe returned indices returned are the unique, unordered set of element indices that contain the points or area.\nThis method will return elements containing the argument points/area, which is not necessarily the same as the nearest.\nTypically not called directly, but by Dataset/DataArray’s sel() method.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nfloat | np.ndarray | None\nX coordinate(s) (easting or longitude)\nNone\n\n\ny\nfloat | np.ndarray | None\nY coordinate(s) (northing or latitude)\nNone\n\n\ncoords\nnp.array(float, float)\nAs an alternative to specifying x, and y individually, the argument coords can be used instead. (x,y)-coordinates of points to be found, by default None\nNone\n\n\narea\n(float, float, float, float)\nBounding box of coordinates (left lower and right upper) to be selected, by default None\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array\nindicies of containing elements\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nif any point is outside the domain\n\n\n\n\n\n\n&gt;&gt;&gt; g = dfs.geometry\n&gt;&gt;&gt; id = dfs.find_index(x=3.1, y=4.3)\n\n\n\nisel : get subset geometry for specific indicies find_nearest_elements : find nearest instead of containing elements\n\n\n\n\nspatial.GeometryFMAreaSpectrum.find_nearest_elements(\n    x,\n    y=None,\n    n_nearest=1,\n    return_distances=False,\n)\nFind index of nearest elements (optionally for a list).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nfloat | np.ndarray\nX coordinate(s) (easting or longitude)\nrequired\n\n\ny\nfloat | np.ndarray | None\nY coordinate(s) (northing or latitude)\nNone\n\n\nn_nearest\nint\nreturn this many (horizontally) nearest points for each coordinate set, default=1\n1\n\n\nreturn_distances\nbool\nshould the horizontal distances to each point be returned? default=False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array\nelement ids of nearest element(s)\n\n\n\n(np.array, optional)\nhorizontal distances\n\n\n\n\n\n\n&gt;&gt;&gt; g = dfs.geometry\n&gt;&gt;&gt; id = g.find_nearest_elements(3, 4)\n&gt;&gt;&gt; ids = g.find_nearest_elements([3, 8], [4, 6])\n&gt;&gt;&gt; ids = g.find_nearest_elements(xy)\n&gt;&gt;&gt; ids = g.find_nearest_elements(3, 4, n_nearest=4)\n&gt;&gt;&gt; ids, d = g.find_nearest_elements(xy, return_distances=True)\n\n\n\nfind_index : find element indicies for points or an area\n\n\n\n\nspatial.GeometryFMAreaSpectrum.get_2d_interpolant(\n    xy,\n    n_nearest=5,\n    extrapolate=False,\n    p=2,\n    radius=None,\n)\nIDW interpolant for list of coordinates.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nxy\narray - like\nx,y coordinates of new points\nrequired\n\n\nn_nearest\nint\nnumber of nearest elements used for IDW, by default 5\n5\n\n\nextrapolate\nbool\nallow extrapolation, by default False\nFalse\n\n\np\nfloat\npower of inverse distance weighting, default=2\n2\n\n\nradius\nfloat | None\nan alternative to extrapolate=False, only include elements within radius\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nInterpolant\nelement ids and weights\n\n\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.get_element_area()\nCalculate the horizontal area of each element.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array(float)\nareas in m2\n\n\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.get_node_centered_data(data, extrapolate=True)\nConvert cell-centered data to node-centered by pseudo-laplacian method.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nnp.array(float)\ncell-centered data\nrequired\n\n\nextrapolate\nbool\nallow the method to extrapolate, default:True\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array(float)\nnode-centered data\n\n\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.get_overset_grid(\n    dx=None,\n    dy=None,\n    nx=None,\n    ny=None,\n    buffer=0.0,\n)\nGet a 2d grid that covers the domain by specifying spacing or shape.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndx\nfloat or (float, float)\ngrid resolution in x-direction (or in x- and y-direction)\nNone\n\n\ndy\nfloat\ngrid resolution in y-direction\nNone\n\n\nnx\nint\nnumber of points in x-direction, by default None, (the value will be inferred)\nNone\n\n\nny\nint\nnumber of points in y-direction, by default None, (the value will be inferred)\nNone\n\n\nbuffer\nfloat\npositive to make the area larger, default=0 can be set to a small negative value to avoid NaN values all around the domain.\n0.0\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n&lt;mikeio.Grid2D&gt;\n2d grid\n\n\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.to_mesh(outfilename)\nExport geometry to new mesh file.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noutfilename\nstr\npath to file to be written\nrequired\n\n\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.to_shapely()\nExport mesh as shapely MultiPolygon.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nshapely.geometry.MultiPolygon\npolygons with mesh elements"
  },
  {
    "objectID": "api/spatial.GeometryFMAreaSpectrum.html#attributes",
    "href": "api/spatial.GeometryFMAreaSpectrum.html#attributes",
    "title": "spatial.GeometryFMAreaSpectrum",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nboundary_codes\nUnique list of boundary codes.\n\n\nboundary_polygons\nLists of polygons defining domain outline.\n\n\ncodes\nNode codes of all nodes (0=water, 1=land, 2…=open boundaries).\n\n\ndirections\nDirectional axis.\n\n\nelement_coordinates\nCenter coordinates of each element.\n\n\nfrequencies\nFrequency axis.\n\n\nis_2d\nType is either mesh or Dfsu2D (2 horizontal dimensions).\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nis_spectral\nType is spectral dfsu (point, line or area spectrum).\n\n\nis_tri_only\nDoes the mesh consist of triangles only.\n\n\nmax_nodes_per_element\nThe maximum number of nodes for an element.\n\n\nn_directions\nNumber of directions.\n\n\nn_elements\nNumber of elements.\n\n\nn_frequencies\nNumber of frequencies.\n\n\nn_nodes\nNumber of nodes.\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string."
  },
  {
    "objectID": "api/spatial.GeometryFMAreaSpectrum.html#methods",
    "href": "api/spatial.GeometryFMAreaSpectrum.html#methods",
    "title": "spatial.GeometryFMAreaSpectrum",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncontains\nTest if a list of points are contained by mesh.\n\n\nelements_to_geometry\nexport a selection of elements to new flexible file geometry\n\n\nfind_index\nFind a set of element indicies for a number of points or within an area.\n\n\nfind_nearest_elements\nFind index of nearest elements (optionally for a list).\n\n\nget_2d_interpolant\nIDW interpolant for list of coordinates.\n\n\nget_element_area\nCalculate the horizontal area of each element.\n\n\nget_node_centered_data\nConvert cell-centered data to node-centered by pseudo-laplacian method.\n\n\nget_overset_grid\nGet a 2d grid that covers the domain by specifying spacing or shape.\n\n\nto_mesh\nExport geometry to new mesh file.\n\n\nto_shapely\nExport mesh as shapely MultiPolygon.\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.contains(points)\nTest if a list of points are contained by mesh.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npoints\narray-like n-by-2\nx,y-coordinates of n points to be tested\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool array\nTrue for points inside, False otherwise\n\n\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.elements_to_geometry(elements, keepdims=False)\nexport a selection of elements to new flexible file geometry\n\n\nelements : list(int) list of element ids keepdims: bool Not used\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nGeometryFMAreaSpectrum or GeometryFMPointSpectrum\nwhich can be used for further extraction or saved to file\n\n\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.find_index(\n    x=None,\n    y=None,\n    coords=None,\n    area=None,\n)\nFind a set of element indicies for a number of points or within an area.\nThe returned indices returned are the unique, unordered set of element indices that contain the points or area.\nThis method will return elements containing the argument points/area, which is not necessarily the same as the nearest.\nTypically not called directly, but by Dataset/DataArray’s sel() method.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nfloat | np.ndarray | None\nX coordinate(s) (easting or longitude)\nNone\n\n\ny\nfloat | np.ndarray | None\nY coordinate(s) (northing or latitude)\nNone\n\n\ncoords\nnp.array(float, float)\nAs an alternative to specifying x, and y individually, the argument coords can be used instead. (x,y)-coordinates of points to be found, by default None\nNone\n\n\narea\n(float, float, float, float)\nBounding box of coordinates (left lower and right upper) to be selected, by default None\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array\nindicies of containing elements\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nif any point is outside the domain\n\n\n\n\n\n\n&gt;&gt;&gt; g = dfs.geometry\n&gt;&gt;&gt; id = dfs.find_index(x=3.1, y=4.3)\n\n\n\nisel : get subset geometry for specific indicies find_nearest_elements : find nearest instead of containing elements\n\n\n\n\nspatial.GeometryFMAreaSpectrum.find_nearest_elements(\n    x,\n    y=None,\n    n_nearest=1,\n    return_distances=False,\n)\nFind index of nearest elements (optionally for a list).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nfloat | np.ndarray\nX coordinate(s) (easting or longitude)\nrequired\n\n\ny\nfloat | np.ndarray | None\nY coordinate(s) (northing or latitude)\nNone\n\n\nn_nearest\nint\nreturn this many (horizontally) nearest points for each coordinate set, default=1\n1\n\n\nreturn_distances\nbool\nshould the horizontal distances to each point be returned? default=False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array\nelement ids of nearest element(s)\n\n\n\n(np.array, optional)\nhorizontal distances\n\n\n\n\n\n\n&gt;&gt;&gt; g = dfs.geometry\n&gt;&gt;&gt; id = g.find_nearest_elements(3, 4)\n&gt;&gt;&gt; ids = g.find_nearest_elements([3, 8], [4, 6])\n&gt;&gt;&gt; ids = g.find_nearest_elements(xy)\n&gt;&gt;&gt; ids = g.find_nearest_elements(3, 4, n_nearest=4)\n&gt;&gt;&gt; ids, d = g.find_nearest_elements(xy, return_distances=True)\n\n\n\nfind_index : find element indicies for points or an area\n\n\n\n\nspatial.GeometryFMAreaSpectrum.get_2d_interpolant(\n    xy,\n    n_nearest=5,\n    extrapolate=False,\n    p=2,\n    radius=None,\n)\nIDW interpolant for list of coordinates.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nxy\narray - like\nx,y coordinates of new points\nrequired\n\n\nn_nearest\nint\nnumber of nearest elements used for IDW, by default 5\n5\n\n\nextrapolate\nbool\nallow extrapolation, by default False\nFalse\n\n\np\nfloat\npower of inverse distance weighting, default=2\n2\n\n\nradius\nfloat | None\nan alternative to extrapolate=False, only include elements within radius\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nInterpolant\nelement ids and weights\n\n\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.get_element_area()\nCalculate the horizontal area of each element.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array(float)\nareas in m2\n\n\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.get_node_centered_data(data, extrapolate=True)\nConvert cell-centered data to node-centered by pseudo-laplacian method.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nnp.array(float)\ncell-centered data\nrequired\n\n\nextrapolate\nbool\nallow the method to extrapolate, default:True\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array(float)\nnode-centered data\n\n\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.get_overset_grid(\n    dx=None,\n    dy=None,\n    nx=None,\n    ny=None,\n    buffer=0.0,\n)\nGet a 2d grid that covers the domain by specifying spacing or shape.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndx\nfloat or (float, float)\ngrid resolution in x-direction (or in x- and y-direction)\nNone\n\n\ndy\nfloat\ngrid resolution in y-direction\nNone\n\n\nnx\nint\nnumber of points in x-direction, by default None, (the value will be inferred)\nNone\n\n\nny\nint\nnumber of points in y-direction, by default None, (the value will be inferred)\nNone\n\n\nbuffer\nfloat\npositive to make the area larger, default=0 can be set to a small negative value to avoid NaN values all around the domain.\n0.0\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n&lt;mikeio.Grid2D&gt;\n2d grid\n\n\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.to_mesh(outfilename)\nExport geometry to new mesh file.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noutfilename\nstr\npath to file to be written\nrequired\n\n\n\n\n\n\n\nspatial.GeometryFMAreaSpectrum.to_shapely()\nExport mesh as shapely MultiPolygon.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nshapely.geometry.MultiPolygon\npolygons with mesh elements"
  },
  {
    "objectID": "api/dataset._DataArrayPlotterGrid1D.html",
    "href": "api/dataset._DataArrayPlotterGrid1D.html",
    "title": "dataset._DataArrayPlotterGrid1D",
    "section": "",
    "text": "dataset._DataArrayPlotterGrid1D(self, da)\nPlot a DataArray with a Grid1D geometry."
  },
  {
    "objectID": "api/dataset._DataArrayPlotterGrid1D.html#examples",
    "href": "api/dataset._DataArrayPlotterGrid1D.html#examples",
    "title": "dataset._DataArrayPlotterGrid1D",
    "section": "Examples",
    "text": "Examples\n\nimport mikeio\nda = mikeio.read(\"../data/tide1.dfs1\")[\"Level\"]\nda.plot()\n\n\n\n\n\n\n\n\n\nda.plot.timeseries()"
  },
  {
    "objectID": "api/dataset._DataArrayPlotterGrid1D.html#methods",
    "href": "api/dataset._DataArrayPlotterGrid1D.html#methods",
    "title": "dataset._DataArrayPlotterGrid1D",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nhist\nPlot DataArray as histogram (using ax.hist).\n\n\nimshow\nPlot as 2d.\n\n\nline\nPlot as spatial lines.\n\n\npcolormesh\nPlot multiple lines as 2d color plot.\n\n\ntimeseries\nPlot as timeseries.\n\n\n\n\nhist\ndataset._DataArrayPlotterGrid1D.hist(\n    bins=None,\n    ax=None,\n    figsize=None,\n    title=None,\n    **kwargs,\n)\nPlot DataArray as histogram (using ax.hist).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbins\n(int or sequence or str)\nIf bins is an integer, it defines the number of equal-width bins in the range. If bins is a sequence, it defines the bin edges, including the left edge of the first bin and the right edge of the last bin. by default: rcParams[“hist.bins”] (default: 10)\nNone\n\n\nax\nAxes | None\nAdding to existing axis, instead of creating new fig\nNone\n\n\nfigsize\ntuple[float, float] | None\nspecify size of figure\nNone\n\n\ntitle\nstr | None\naxes title\nNone\n\n\n**kwargs\nAny\nadditional arguments passed to the plotting function\n{}\n\n\n\n\n\nSee Also\nmatplotlib.pyplot.hist\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n&lt;matplotlib.axes&gt;\n\n\n\n\n\n\n\nimshow\ndataset._DataArrayPlotterGrid1D.imshow(ax=None, figsize=None, **kwargs)\nPlot as 2d.\n\n\nline\ndataset._DataArrayPlotterGrid1D.line(ax=None, figsize=None, **kwargs)\nPlot as spatial lines.\n\n\npcolormesh\ndataset._DataArrayPlotterGrid1D.pcolormesh(\n    ax=None,\n    figsize=None,\n    title=None,\n    **kwargs,\n)\nPlot multiple lines as 2d color plot.\n\n\ntimeseries\ndataset._DataArrayPlotterGrid1D.timeseries(ax=None, figsize=None, **kwargs)\nPlot as timeseries."
  },
  {
    "objectID": "api/spatial.GeometryFM3D.html",
    "href": "api/spatial.GeometryFM3D.html",
    "title": "spatial.GeometryFM3D",
    "section": "",
    "text": "spatial.GeometryFM3D(\n    self,\n    *,\n    node_coordinates,\n    element_table,\n    codes=None,\n    projection='LONG/LAT',\n    dfsu_type=DfsuFileType.Dfsu3DSigma,\n    element_ids=None,\n    node_ids=None,\n    n_layers=1,\n    n_sigma=None,\n    validate=True,\n    reindex=False,\n)\nFlexible 3d mesh geometry.\n\n\n\n\n\nName\nDescription\n\n\n\n\nbottom_elements\nList of 3d element ids of bottom layer.\n\n\nboundary_codes\nUnique list of boundary codes.\n\n\ncodes\nNode codes of all nodes (0=water, 1=land, 2…=open boundaries).\n\n\ne2_e3_table\nThe 2d-to-3d element connectivity table for a 3d object.\n\n\nelem2d_ids\nThe associated 2d element id for each 3d element.\n\n\nelement_coordinates\nCenter coordinates of each element.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nlayer_ids\nThe layer number (0=bottom, 1, 2, …) for each 3d element.\n\n\nmax_nodes_per_element\nThe maximum number of nodes for an element.\n\n\nn_elements\nNumber of 3d elements.\n\n\nn_layers\nMaximum number of layers.\n\n\nn_layers_per_column\nList of number of layers for each column.\n\n\nn_sigma_layers\nNumber of sigma layers.\n\n\nn_z_layers\nMaximum number of z-layers.\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string.\n\n\ntop_elements\nList of 3d element ids of surface layer.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_layer_elements\n3d element ids for one (or more) specific layer(s).\n\n\nto_2d_geometry\nextract 2d geometry from 3d geometry.\n\n\n\n\n\nspatial.GeometryFM3D.get_layer_elements(layers)\n3d element ids for one (or more) specific layer(s).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlayers\nint or list(int)\nlayer between 0 (bottom) and n_layers-1 (top) (can also be negative counting from -1 at the top layer)\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array(int)\nelement ids\n\n\n\n\n\n\n\nspatial.GeometryFM3D.to_2d_geometry()\nextract 2d geometry from 3d geometry.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nGeometryFM2D\n2d geometry"
  },
  {
    "objectID": "api/spatial.GeometryFM3D.html#attributes",
    "href": "api/spatial.GeometryFM3D.html#attributes",
    "title": "spatial.GeometryFM3D",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nbottom_elements\nList of 3d element ids of bottom layer.\n\n\nboundary_codes\nUnique list of boundary codes.\n\n\ncodes\nNode codes of all nodes (0=water, 1=land, 2…=open boundaries).\n\n\ne2_e3_table\nThe 2d-to-3d element connectivity table for a 3d object.\n\n\nelem2d_ids\nThe associated 2d element id for each 3d element.\n\n\nelement_coordinates\nCenter coordinates of each element.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nlayer_ids\nThe layer number (0=bottom, 1, 2, …) for each 3d element.\n\n\nmax_nodes_per_element\nThe maximum number of nodes for an element.\n\n\nn_elements\nNumber of 3d elements.\n\n\nn_layers\nMaximum number of layers.\n\n\nn_layers_per_column\nList of number of layers for each column.\n\n\nn_sigma_layers\nNumber of sigma layers.\n\n\nn_z_layers\nMaximum number of z-layers.\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string.\n\n\ntop_elements\nList of 3d element ids of surface layer."
  },
  {
    "objectID": "api/spatial.GeometryFM3D.html#methods",
    "href": "api/spatial.GeometryFM3D.html#methods",
    "title": "spatial.GeometryFM3D",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_layer_elements\n3d element ids for one (or more) specific layer(s).\n\n\nto_2d_geometry\nextract 2d geometry from 3d geometry.\n\n\n\n\n\nspatial.GeometryFM3D.get_layer_elements(layers)\n3d element ids for one (or more) specific layer(s).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlayers\nint or list(int)\nlayer between 0 (bottom) and n_layers-1 (top) (can also be negative counting from -1 at the top layer)\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array(int)\nelement ids\n\n\n\n\n\n\n\nspatial.GeometryFM3D.to_2d_geometry()\nextract 2d geometry from 3d geometry.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nGeometryFM2D\n2d geometry"
  },
  {
    "objectID": "api/spatial.GeometryFM2D.html",
    "href": "api/spatial.GeometryFM2D.html",
    "title": "spatial.GeometryFM2D",
    "section": "",
    "text": "spatial.GeometryFM2D(\n    self,\n    node_coordinates,\n    element_table,\n    codes=None,\n    projection='LONG/LAT',\n    dfsu_type=DfsuFileType.Dfsu2D,\n    element_ids=None,\n    node_ids=None,\n    validate=True,\n    reindex=False,\n)\nFlexible 2d mesh geometry.\n\n\n\n\n\nName\nDescription\n\n\n\n\nboundary_codes\nUnique list of boundary codes.\n\n\nboundary_polygons\nLists of polygons defining domain outline.\n\n\ncodes\nNode codes of all nodes (0=water, 1=land, 2…=open boundaries).\n\n\nelement_coordinates\nCenter coordinates of each element.\n\n\nis_2d\nType is either mesh or Dfsu2D (2 horizontal dimensions).\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_layered\nType is layered dfsu (3d, vertical profile or vertical column).\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nis_spectral\nType is spectral dfsu (point, line or area spectrum).\n\n\nis_tri_only\nDoes the mesh consist of triangles only.\n\n\nmax_nodes_per_element\nThe maximum number of nodes for an element.\n\n\nn_elements\nNumber of elements.\n\n\nn_nodes\nNumber of nodes.\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncontains\nTest if a list of points are contained by mesh.\n\n\nfind_index\nFind a set of element indicies for a number of points or within an area.\n\n\nfind_nearest_elements\nFind index of nearest elements (optionally for a list).\n\n\nget_2d_interpolant\nIDW interpolant for list of coordinates.\n\n\nget_element_area\nCalculate the horizontal area of each element.\n\n\nget_node_centered_data\nConvert cell-centered data to node-centered by pseudo-laplacian method.\n\n\nget_overset_grid\nGet a 2d grid that covers the domain by specifying spacing or shape.\n\n\nisel\nExport a selection of elements to a new geometry.\n\n\nto_mesh\nExport geometry to new mesh file.\n\n\nto_shapely\nExport mesh as shapely MultiPolygon.\n\n\n\n\n\nspatial.GeometryFM2D.contains(points)\nTest if a list of points are contained by mesh.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npoints\narray-like n-by-2\nx,y-coordinates of n points to be tested\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool array\nTrue for points inside, False otherwise\n\n\n\n\n\n\n\nspatial.GeometryFM2D.find_index(x=None, y=None, coords=None, area=None)\nFind a set of element indicies for a number of points or within an area.\nThe returned indices returned are the unique, unordered set of element indices that contain the points or area.\nThis method will return elements containing the argument points/area, which is not necessarily the same as the nearest.\nTypically not called directly, but by Dataset/DataArray’s sel() method.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nfloat | np.ndarray | None\nX coordinate(s) (easting or longitude)\nNone\n\n\ny\nfloat | np.ndarray | None\nY coordinate(s) (northing or latitude)\nNone\n\n\ncoords\nnp.array(float, float)\nAs an alternative to specifying x, and y individually, the argument coords can be used instead. (x,y)-coordinates of points to be found, by default None\nNone\n\n\narea\n(float, float, float, float)\nBounding box of coordinates (left lower and right upper) to be selected, by default None\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array\nindicies of containing elements\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nif any point is outside the domain\n\n\n\n\n\n\n&gt;&gt;&gt; g = dfs.geometry\n&gt;&gt;&gt; id = dfs.find_index(x=3.1, y=4.3)\n\n\n\nisel : get subset geometry for specific indicies find_nearest_elements : find nearest instead of containing elements\n\n\n\n\nspatial.GeometryFM2D.find_nearest_elements(\n    x,\n    y=None,\n    n_nearest=1,\n    return_distances=False,\n)\nFind index of nearest elements (optionally for a list).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nfloat | np.ndarray\nX coordinate(s) (easting or longitude)\nrequired\n\n\ny\nfloat | np.ndarray | None\nY coordinate(s) (northing or latitude)\nNone\n\n\nn_nearest\nint\nreturn this many (horizontally) nearest points for each coordinate set, default=1\n1\n\n\nreturn_distances\nbool\nshould the horizontal distances to each point be returned? default=False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array\nelement ids of nearest element(s)\n\n\n\n(np.array, optional)\nhorizontal distances\n\n\n\n\n\n\n&gt;&gt;&gt; g = dfs.geometry\n&gt;&gt;&gt; id = g.find_nearest_elements(3, 4)\n&gt;&gt;&gt; ids = g.find_nearest_elements([3, 8], [4, 6])\n&gt;&gt;&gt; ids = g.find_nearest_elements(xy)\n&gt;&gt;&gt; ids = g.find_nearest_elements(3, 4, n_nearest=4)\n&gt;&gt;&gt; ids, d = g.find_nearest_elements(xy, return_distances=True)\n\n\n\nfind_index : find element indicies for points or an area\n\n\n\n\nspatial.GeometryFM2D.get_2d_interpolant(\n    xy,\n    n_nearest=5,\n    extrapolate=False,\n    p=2,\n    radius=None,\n)\nIDW interpolant for list of coordinates.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nxy\narray - like\nx,y coordinates of new points\nrequired\n\n\nn_nearest\nint\nnumber of nearest elements used for IDW, by default 5\n5\n\n\nextrapolate\nbool\nallow extrapolation, by default False\nFalse\n\n\np\nfloat\npower of inverse distance weighting, default=2\n2\n\n\nradius\nfloat | None\nan alternative to extrapolate=False, only include elements within radius\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nInterpolant\nelement ids and weights\n\n\n\n\n\n\n\nspatial.GeometryFM2D.get_element_area()\nCalculate the horizontal area of each element.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array(float)\nareas in m2\n\n\n\n\n\n\n\nspatial.GeometryFM2D.get_node_centered_data(data, extrapolate=True)\nConvert cell-centered data to node-centered by pseudo-laplacian method.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nnp.array(float)\ncell-centered data\nrequired\n\n\nextrapolate\nbool\nallow the method to extrapolate, default:True\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array(float)\nnode-centered data\n\n\n\n\n\n\n\nspatial.GeometryFM2D.get_overset_grid(\n    dx=None,\n    dy=None,\n    nx=None,\n    ny=None,\n    buffer=0.0,\n)\nGet a 2d grid that covers the domain by specifying spacing or shape.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndx\nfloat or (float, float)\ngrid resolution in x-direction (or in x- and y-direction)\nNone\n\n\ndy\nfloat\ngrid resolution in y-direction\nNone\n\n\nnx\nint\nnumber of points in x-direction, by default None, (the value will be inferred)\nNone\n\n\nny\nint\nnumber of points in y-direction, by default None, (the value will be inferred)\nNone\n\n\nbuffer\nfloat\npositive to make the area larger, default=0 can be set to a small negative value to avoid NaN values all around the domain.\n0.0\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n&lt;mikeio.Grid2D&gt;\n2d grid\n\n\n\n\n\n\n\nspatial.GeometryFM2D.isel(idx, keepdims=False, **kwargs)\nExport a selection of elements to a new geometry.\nTypically not called directly, but by Dataset/DataArray’s isel() or sel() methods.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\nlist(int)\ncollection of element indicies\nrequired\n\n\nkeepdims\nbool\nShould the original Geometry type be kept (keepdims=True) or should it be reduced e.g. to a GeometryPoint2D if possible (keepdims=False), by default False\nFalse\n\n\n**kwargs\nAny\nNot used\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nGeometry\ngeometry subset\n\n\n\n\n\n\nfind_index : find element indicies for points or an area\n\n\n\n\nspatial.GeometryFM2D.to_mesh(outfilename)\nExport geometry to new mesh file.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noutfilename\nstr\npath to file to be written\nrequired\n\n\n\n\n\n\n\nspatial.GeometryFM2D.to_shapely()\nExport mesh as shapely MultiPolygon.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nshapely.geometry.MultiPolygon\npolygons with mesh elements"
  },
  {
    "objectID": "api/spatial.GeometryFM2D.html#attributes",
    "href": "api/spatial.GeometryFM2D.html#attributes",
    "title": "spatial.GeometryFM2D",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nboundary_codes\nUnique list of boundary codes.\n\n\nboundary_polygons\nLists of polygons defining domain outline.\n\n\ncodes\nNode codes of all nodes (0=water, 1=land, 2…=open boundaries).\n\n\nelement_coordinates\nCenter coordinates of each element.\n\n\nis_2d\nType is either mesh or Dfsu2D (2 horizontal dimensions).\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_layered\nType is layered dfsu (3d, vertical profile or vertical column).\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nis_spectral\nType is spectral dfsu (point, line or area spectrum).\n\n\nis_tri_only\nDoes the mesh consist of triangles only.\n\n\nmax_nodes_per_element\nThe maximum number of nodes for an element.\n\n\nn_elements\nNumber of elements.\n\n\nn_nodes\nNumber of nodes.\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string."
  },
  {
    "objectID": "api/spatial.GeometryFM2D.html#methods",
    "href": "api/spatial.GeometryFM2D.html#methods",
    "title": "spatial.GeometryFM2D",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncontains\nTest if a list of points are contained by mesh.\n\n\nfind_index\nFind a set of element indicies for a number of points or within an area.\n\n\nfind_nearest_elements\nFind index of nearest elements (optionally for a list).\n\n\nget_2d_interpolant\nIDW interpolant for list of coordinates.\n\n\nget_element_area\nCalculate the horizontal area of each element.\n\n\nget_node_centered_data\nConvert cell-centered data to node-centered by pseudo-laplacian method.\n\n\nget_overset_grid\nGet a 2d grid that covers the domain by specifying spacing or shape.\n\n\nisel\nExport a selection of elements to a new geometry.\n\n\nto_mesh\nExport geometry to new mesh file.\n\n\nto_shapely\nExport mesh as shapely MultiPolygon.\n\n\n\n\n\nspatial.GeometryFM2D.contains(points)\nTest if a list of points are contained by mesh.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npoints\narray-like n-by-2\nx,y-coordinates of n points to be tested\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool array\nTrue for points inside, False otherwise\n\n\n\n\n\n\n\nspatial.GeometryFM2D.find_index(x=None, y=None, coords=None, area=None)\nFind a set of element indicies for a number of points or within an area.\nThe returned indices returned are the unique, unordered set of element indices that contain the points or area.\nThis method will return elements containing the argument points/area, which is not necessarily the same as the nearest.\nTypically not called directly, but by Dataset/DataArray’s sel() method.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nfloat | np.ndarray | None\nX coordinate(s) (easting or longitude)\nNone\n\n\ny\nfloat | np.ndarray | None\nY coordinate(s) (northing or latitude)\nNone\n\n\ncoords\nnp.array(float, float)\nAs an alternative to specifying x, and y individually, the argument coords can be used instead. (x,y)-coordinates of points to be found, by default None\nNone\n\n\narea\n(float, float, float, float)\nBounding box of coordinates (left lower and right upper) to be selected, by default None\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array\nindicies of containing elements\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nif any point is outside the domain\n\n\n\n\n\n\n&gt;&gt;&gt; g = dfs.geometry\n&gt;&gt;&gt; id = dfs.find_index(x=3.1, y=4.3)\n\n\n\nisel : get subset geometry for specific indicies find_nearest_elements : find nearest instead of containing elements\n\n\n\n\nspatial.GeometryFM2D.find_nearest_elements(\n    x,\n    y=None,\n    n_nearest=1,\n    return_distances=False,\n)\nFind index of nearest elements (optionally for a list).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nfloat | np.ndarray\nX coordinate(s) (easting or longitude)\nrequired\n\n\ny\nfloat | np.ndarray | None\nY coordinate(s) (northing or latitude)\nNone\n\n\nn_nearest\nint\nreturn this many (horizontally) nearest points for each coordinate set, default=1\n1\n\n\nreturn_distances\nbool\nshould the horizontal distances to each point be returned? default=False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array\nelement ids of nearest element(s)\n\n\n\n(np.array, optional)\nhorizontal distances\n\n\n\n\n\n\n&gt;&gt;&gt; g = dfs.geometry\n&gt;&gt;&gt; id = g.find_nearest_elements(3, 4)\n&gt;&gt;&gt; ids = g.find_nearest_elements([3, 8], [4, 6])\n&gt;&gt;&gt; ids = g.find_nearest_elements(xy)\n&gt;&gt;&gt; ids = g.find_nearest_elements(3, 4, n_nearest=4)\n&gt;&gt;&gt; ids, d = g.find_nearest_elements(xy, return_distances=True)\n\n\n\nfind_index : find element indicies for points or an area\n\n\n\n\nspatial.GeometryFM2D.get_2d_interpolant(\n    xy,\n    n_nearest=5,\n    extrapolate=False,\n    p=2,\n    radius=None,\n)\nIDW interpolant for list of coordinates.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nxy\narray - like\nx,y coordinates of new points\nrequired\n\n\nn_nearest\nint\nnumber of nearest elements used for IDW, by default 5\n5\n\n\nextrapolate\nbool\nallow extrapolation, by default False\nFalse\n\n\np\nfloat\npower of inverse distance weighting, default=2\n2\n\n\nradius\nfloat | None\nan alternative to extrapolate=False, only include elements within radius\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nInterpolant\nelement ids and weights\n\n\n\n\n\n\n\nspatial.GeometryFM2D.get_element_area()\nCalculate the horizontal area of each element.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array(float)\nareas in m2\n\n\n\n\n\n\n\nspatial.GeometryFM2D.get_node_centered_data(data, extrapolate=True)\nConvert cell-centered data to node-centered by pseudo-laplacian method.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nnp.array(float)\ncell-centered data\nrequired\n\n\nextrapolate\nbool\nallow the method to extrapolate, default:True\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array(float)\nnode-centered data\n\n\n\n\n\n\n\nspatial.GeometryFM2D.get_overset_grid(\n    dx=None,\n    dy=None,\n    nx=None,\n    ny=None,\n    buffer=0.0,\n)\nGet a 2d grid that covers the domain by specifying spacing or shape.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndx\nfloat or (float, float)\ngrid resolution in x-direction (or in x- and y-direction)\nNone\n\n\ndy\nfloat\ngrid resolution in y-direction\nNone\n\n\nnx\nint\nnumber of points in x-direction, by default None, (the value will be inferred)\nNone\n\n\nny\nint\nnumber of points in y-direction, by default None, (the value will be inferred)\nNone\n\n\nbuffer\nfloat\npositive to make the area larger, default=0 can be set to a small negative value to avoid NaN values all around the domain.\n0.0\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n&lt;mikeio.Grid2D&gt;\n2d grid\n\n\n\n\n\n\n\nspatial.GeometryFM2D.isel(idx, keepdims=False, **kwargs)\nExport a selection of elements to a new geometry.\nTypically not called directly, but by Dataset/DataArray’s isel() or sel() methods.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\nlist(int)\ncollection of element indicies\nrequired\n\n\nkeepdims\nbool\nShould the original Geometry type be kept (keepdims=True) or should it be reduced e.g. to a GeometryPoint2D if possible (keepdims=False), by default False\nFalse\n\n\n**kwargs\nAny\nNot used\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nGeometry\ngeometry subset\n\n\n\n\n\n\nfind_index : find element indicies for points or an area\n\n\n\n\nspatial.GeometryFM2D.to_mesh(outfilename)\nExport geometry to new mesh file.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noutfilename\nstr\npath to file to be written\nrequired\n\n\n\n\n\n\n\nspatial.GeometryFM2D.to_shapely()\nExport mesh as shapely MultiPolygon.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nshapely.geometry.MultiPolygon\npolygons with mesh elements"
  },
  {
    "objectID": "api/EUMUnit.html",
    "href": "api/EUMUnit.html",
    "title": "EUMUnit",
    "section": "",
    "text": "EUMUnit(self, code)\nEUM unit."
  },
  {
    "objectID": "api/EUMUnit.html#examples",
    "href": "api/EUMUnit.html#examples",
    "title": "EUMUnit",
    "section": "Examples",
    "text": "Examples\n\nimport mikeio\nmikeio.EUMUnit.degree_Kelvin\n\ndegree Kelvin"
  },
  {
    "objectID": "api/EUMUnit.html#attributes",
    "href": "api/EUMUnit.html#attributes",
    "title": "EUMUnit",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\ndisplay_name\nDisplay friendly name."
  },
  {
    "objectID": "api/Dfs2.html",
    "href": "api/Dfs2.html",
    "title": "Dfs2",
    "section": "",
    "text": "Dfs2(self, filename, type='horizontal')\nClass for reading/writing dfs2 files.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nPath to dfs2 file\nrequired\n\n\ntype\nLiteral['horizontal', 'spectral', 'vertical']\nhorizontal, spectral or vertical, default horizontal\n'horizontal'\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndeletevalue\nFile delete value.\n\n\ndx\nStep size in x direction.\n\n\ndy\nStep size in y direction.\n\n\nend_time\nFile end time.\n\n\ngeometry\nSpatial information.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nitems\nList of items.\n\n\nlatitude\nOrigin latitude.\n\n\nlongitude\nOrigin longitude.\n\n\nn_items\nNumber of items.\n\n\nn_timesteps\nNumber of time steps.\n\n\nnx\nNumber of values in the x-direction.\n\n\nny\nNumber of values in the y-direction.\n\n\norientation\nOrientation (in own CRS).\n\n\norigin\nOrigin (in own CRS).\n\n\nshape\nTuple with number of values in the t-, y-, x-direction.\n\n\nstart_time\nFile start time.\n\n\ntimestep\nTime step size in seconds.\n\n\nx0\nStart point of x values (often 0).\n\n\ny0\nStart point of y values (often 0).\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nappend\nAppend a Dataset to an existing dfs2 file.\n\n\nread\nRead data from a dfs2 file.\n\n\n\n\n\nDfs2.append(ds, validate=True)\nAppend a Dataset to an existing dfs2 file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nDataset to append\nrequired\n\n\nvalidate\nbool\nCheck if the dataset to append has the same geometry and items as the original file, by default True\nTrue\n\n\n\n\n\n\nThe original file is modified.\n\n\n\n\nDfs2.read(items=None, time=None, area=None, keepdims=False, dtype=np.float32)\nRead data from a dfs2 file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\nkeepdims\nbool\nWhen reading a single time step only, should the time-dimension be kept in the returned Dataset? by default: False\nFalse\n\n\narea\ntuple[float, float, float, float] | None\nRead only data inside (horizontal) area given as a bounding box (tuple with left, lower, right, upper) coordinates\nNone\n\n\ndtype\nAny\nDefine the dtype of the returned dataset (default = np.float32)\nnp.float32\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset"
  },
  {
    "objectID": "api/Dfs2.html#parameters",
    "href": "api/Dfs2.html#parameters",
    "title": "Dfs2",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nPath to dfs2 file\nrequired\n\n\ntype\nLiteral['horizontal', 'spectral', 'vertical']\nhorizontal, spectral or vertical, default horizontal\n'horizontal'"
  },
  {
    "objectID": "api/Dfs2.html#attributes",
    "href": "api/Dfs2.html#attributes",
    "title": "Dfs2",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndeletevalue\nFile delete value.\n\n\ndx\nStep size in x direction.\n\n\ndy\nStep size in y direction.\n\n\nend_time\nFile end time.\n\n\ngeometry\nSpatial information.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nitems\nList of items.\n\n\nlatitude\nOrigin latitude.\n\n\nlongitude\nOrigin longitude.\n\n\nn_items\nNumber of items.\n\n\nn_timesteps\nNumber of time steps.\n\n\nnx\nNumber of values in the x-direction.\n\n\nny\nNumber of values in the y-direction.\n\n\norientation\nOrientation (in own CRS).\n\n\norigin\nOrigin (in own CRS).\n\n\nshape\nTuple with number of values in the t-, y-, x-direction.\n\n\nstart_time\nFile start time.\n\n\ntimestep\nTime step size in seconds.\n\n\nx0\nStart point of x values (often 0).\n\n\ny0\nStart point of y values (often 0)."
  },
  {
    "objectID": "api/Dfs2.html#methods",
    "href": "api/Dfs2.html#methods",
    "title": "Dfs2",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nappend\nAppend a Dataset to an existing dfs2 file.\n\n\nread\nRead data from a dfs2 file.\n\n\n\n\n\nDfs2.append(ds, validate=True)\nAppend a Dataset to an existing dfs2 file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nDataset to append\nrequired\n\n\nvalidate\nbool\nCheck if the dataset to append has the same geometry and items as the original file, by default True\nTrue\n\n\n\n\n\n\nThe original file is modified.\n\n\n\n\nDfs2.read(items=None, time=None, area=None, keepdims=False, dtype=np.float32)\nRead data from a dfs2 file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\nkeepdims\nbool\nWhen reading a single time step only, should the time-dimension be kept in the returned Dataset? by default: False\nFalse\n\n\narea\ntuple[float, float, float, float] | None\nRead only data inside (horizontal) area given as a bounding box (tuple with left, lower, right, upper) coordinates\nNone\n\n\ndtype\nAny\nDefine the dtype of the returned dataset (default = np.float32)\nnp.float32\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset"
  },
  {
    "objectID": "api/dataset._DatasetPlotter.html",
    "href": "api/dataset._DatasetPlotter.html",
    "title": "dataset._DatasetPlotter",
    "section": "",
    "text": "dataset._DatasetPlotter(self, ds)\nClass for plotting scatter plots from datasets."
  },
  {
    "objectID": "api/dataset._DatasetPlotter.html#methods",
    "href": "api/dataset._DatasetPlotter.html#methods",
    "title": "dataset._DatasetPlotter",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nscatter\nPlot data from two DataArrays against each other in a scatter plot.\n\n\n\n\nscatter\ndataset._DatasetPlotter.scatter(\n    x,\n    y,\n    ax=None,\n    figsize=None,\n    title=None,\n    **kwargs,\n)\nPlot data from two DataArrays against each other in a scatter plot.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nstr or int\nIdentifier for first DataArray\nrequired\n\n\ny\nstr or int\nIdentifier for second DataArray\nrequired\n\n\nax\nAxes | None\nAdding to existing axis, instead of creating new fig\nNone\n\n\nfigsize\ntuple[float, float] | None\nspecify size of figure\nNone\n\n\ntitle\nstr | None\naxes title\nNone\n\n\n**kwargs\nAny\nadditional kwargs will be passed to ax.scatter()\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n&lt;matplotlib.axes&gt;\n\n\n\n\n\n\nExamples\n\nimport mikeio\nds = mikeio.read(\"../data/oresund_sigma_z.dfsu\")\nds.plot.scatter(x=\"Salinity\", y=\"Temperature\", title=\"S-vs-T\")\n\n\n\n\n\n\n\n\n\nds.plot.scatter(x=0, y=1, figsize=(9,9), marker='*')"
  },
  {
    "objectID": "api/generic.html",
    "href": "api/generic.html",
    "title": "generic",
    "section": "",
    "text": "generic\nGeneric functions for working with all types of dfs files.\n\n\n\n\n\nName\nDescription\n\n\n\n\navg_time\nCreate a temporally averaged dfs file.\n\n\nchange_datatype\nChange datatype of a DFS file.\n\n\nconcat\nConcatenates files along the time axis.\n\n\ndiff\nCalculate difference between two dfs files (a-b).\n\n\nextract\nExtract timesteps and/or items to a new dfs file.\n\n\nfill_corrupt\nReplace corrupt (unreadable) data with fill_value, default delete value.\n\n\nquantile\nCreate temporal quantiles of all items in dfs file.\n\n\nscale\nApply scaling to any dfs file.\n\n\nsum\nSum two dfs files (a+b).\n\n\n\n\n\ngeneric.avg_time(infilename, outfilename, skipna=True)\nCreate a temporally averaged dfs file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninfilename\nstr | pathlib.Path\ninput filename\nrequired\n\n\noutfilename\nstr | pathlib.Path\noutput filename\nrequired\n\n\nskipna\nbool\nexclude NaN/delete values when computing the result, default True\nTrue\n\n\n\n\n\n\n\ngeneric.change_datatype(infilename, outfilename, datatype)\nChange datatype of a DFS file.\nThe data type tag is used to classify the file within a specific modeling context, such as MIKE 21. There is no global standard for these tags—they are interpreted locally within a model setup.\nApplication developers can use these tags to classify files such as bathymetries, input data, or result files according to their own conventions.\nDefault data type values assigned by MikeIO when creating new files are: - dfs0: datatype=1 - dfs1-3: datatype=0 - dfsu: datatype=2001\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninfilename\nstr | pathlib.Path\ninput filename\nrequired\n\n\noutfilename\nstr | pathlib.Path\noutput filename\nrequired\n\n\ndatatype\nint\nDataType to be used for the output file\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; change_datatype(\"in.dfsu\", \"out.dfsu\", datatype=107)\n\n\n\n\ngeneric.concat(infilenames, outfilename, keep='last')\nConcatenates files along the time axis.\nOverlap handling is defined by the keep argument, by default the last one will be used.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninfilenames\nSequence[str | pathlib.Path]\nfilenames to concatenate\nrequired\n\n\noutfilename\nstr | pathlib.Path\nfilename of output\nrequired\n\n\nkeep\nstr\neither ‘first’ (keep older), ‘last’ (keep newer) or ‘average’ can be selected. By default ‘last’\n'last'\n\n\n\n\n\n\nThe list of input files have to be sorted, i.e. in chronological order\n\n\n\n\ngeneric.diff(infilename_a, infilename_b, outfilename)\nCalculate difference between two dfs files (a-b).\n\n\n\ngeneric.extract(infilename, outfilename, start=0, end=-1, step=1, items=None)\nExtract timesteps and/or items to a new dfs file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninfilename\nstr | pathlib.Path\npath to input dfs file\nrequired\n\n\noutfilename\nstr | pathlib.Path\npath to output dfs file\nrequired\n\n\nstart\n(int, float, str or datetime)\nstart of extraction as either step, relative seconds or datetime/str, by default 0 (start of file)\n0\n\n\nend\n(int, float, str or datetime)\nend of extraction as either step, relative seconds or datetime/str, by default -1 (end of file)\n-1\n\n\nstep\nint\njump this many step, by default 1 (every step between start and end)\n1\n\n\nitems\n(int, list(int), str, list(str))\nitems to be extracted to new file\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; extract('f_in.dfs0', 'f_out.dfs0', start='2018-1-1')\n&gt;&gt;&gt; extract('f_in.dfs2', 'f_out.dfs2', end=-3)\n&gt;&gt;&gt; extract('f_in.dfsu', 'f_out.dfsu', start=1800.0, end=3600.0)\n&gt;&gt;&gt; extract('f_hourly.dfsu', 'f_daily.dfsu', step=24)\n&gt;&gt;&gt; extract('f_in.dfsu', 'f_out.dfsu', items=[2, 0])\n&gt;&gt;&gt; extract('f_in.dfsu', 'f_out.dfsu', items=\"Salinity\")\n&gt;&gt;&gt; extract('f_in.dfsu', 'f_out.dfsu', end='2018-2-1 00:00', items=\"Salinity\")\n\n\n\n\ngeneric.fill_corrupt(infilename, outfilename, fill_value=np.nan, items=None)\nReplace corrupt (unreadable) data with fill_value, default delete value.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninfilename\nstr | pathlib.Path\nfull path to the input file\nrequired\n\n\noutfilename\nstr | pathlib.Path\nfull path to the output file\nrequired\n\n\nfill_value\nfloat\nvalue to use where data is corrupt, default delete value\nnp.nan\n\n\nitems\nSequence[str | int] | None\nProcess only selected items, by number (0-based) or name, by default: all\nNone\n\n\n\n\n\n\n\ngeneric.quantile(\n    infilename,\n    outfilename,\n    q,\n    *,\n    items=None,\n    skipna=True,\n    buffer_size=1000000000.0,\n)\nCreate temporal quantiles of all items in dfs file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninfilename\nstr | pathlib.Path\ninput filename\nrequired\n\n\noutfilename\nstr | pathlib.Path\noutput filename\nrequired\n\n\nq\nfloat | Sequence[float]\nQuantile or sequence of quantiles to compute, which must be between 0 and 1 inclusive.\nrequired\n\n\nitems\nSequence[int | str] | int | str | None\nProcess only selected items, by number (0-based) or name, by default: all\nNone\n\n\nskipna\nbool\nexclude NaN/delete values when computing the result, default True\nTrue\n\n\nbuffer_size\nfloat\nfor huge files the quantiles need to be calculated for chunks of elements. buffer_size gives the maximum amount of memory available for the computation in bytes, by default 1e9 (=1GB)\n1000000000.0\n\n\n\n\n\n\n&gt;&gt;&gt; quantile(\"in.dfsu\", \"IQR.dfsu\", q=[0.25,0.75])\n&gt;&gt;&gt; quantile(\"huge.dfsu\", \"Q01.dfsu\", q=0.1, buffer_size=5.0e9)\n&gt;&gt;&gt; quantile(\"with_nans.dfsu\", \"Q05.dfsu\", q=0.5, skipna=False)\n\n\n\n\ngeneric.scale(infilename, outfilename, offset=0.0, factor=1.0, items=None)\nApply scaling to any dfs file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninfilename\nstr | pathlib.Path\nfull path to the input file\nrequired\n\n\noutfilename\nstr | pathlib.Path\nfull path to the output file\nrequired\n\n\noffset\nfloat\nvalue to add to all items, default 0.0\n0.0\n\n\nfactor\nfloat\nvalue to multiply to all items, default 1.0\n1.0\n\n\nitems\nSequence[int | str] | None\nProcess only selected items, by number (0-based) or name, by default: all\nNone\n\n\n\n\n\n\n\ngeneric.sum(infilename_a, infilename_b, outfilename)\nSum two dfs files (a+b)."
  },
  {
    "objectID": "api/generic.html#functions",
    "href": "api/generic.html#functions",
    "title": "generic",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\navg_time\nCreate a temporally averaged dfs file.\n\n\nchange_datatype\nChange datatype of a DFS file.\n\n\nconcat\nConcatenates files along the time axis.\n\n\ndiff\nCalculate difference between two dfs files (a-b).\n\n\nextract\nExtract timesteps and/or items to a new dfs file.\n\n\nfill_corrupt\nReplace corrupt (unreadable) data with fill_value, default delete value.\n\n\nquantile\nCreate temporal quantiles of all items in dfs file.\n\n\nscale\nApply scaling to any dfs file.\n\n\nsum\nSum two dfs files (a+b).\n\n\n\n\n\ngeneric.avg_time(infilename, outfilename, skipna=True)\nCreate a temporally averaged dfs file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninfilename\nstr | pathlib.Path\ninput filename\nrequired\n\n\noutfilename\nstr | pathlib.Path\noutput filename\nrequired\n\n\nskipna\nbool\nexclude NaN/delete values when computing the result, default True\nTrue\n\n\n\n\n\n\n\ngeneric.change_datatype(infilename, outfilename, datatype)\nChange datatype of a DFS file.\nThe data type tag is used to classify the file within a specific modeling context, such as MIKE 21. There is no global standard for these tags—they are interpreted locally within a model setup.\nApplication developers can use these tags to classify files such as bathymetries, input data, or result files according to their own conventions.\nDefault data type values assigned by MikeIO when creating new files are: - dfs0: datatype=1 - dfs1-3: datatype=0 - dfsu: datatype=2001\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninfilename\nstr | pathlib.Path\ninput filename\nrequired\n\n\noutfilename\nstr | pathlib.Path\noutput filename\nrequired\n\n\ndatatype\nint\nDataType to be used for the output file\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; change_datatype(\"in.dfsu\", \"out.dfsu\", datatype=107)\n\n\n\n\ngeneric.concat(infilenames, outfilename, keep='last')\nConcatenates files along the time axis.\nOverlap handling is defined by the keep argument, by default the last one will be used.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninfilenames\nSequence[str | pathlib.Path]\nfilenames to concatenate\nrequired\n\n\noutfilename\nstr | pathlib.Path\nfilename of output\nrequired\n\n\nkeep\nstr\neither ‘first’ (keep older), ‘last’ (keep newer) or ‘average’ can be selected. By default ‘last’\n'last'\n\n\n\n\n\n\nThe list of input files have to be sorted, i.e. in chronological order\n\n\n\n\ngeneric.diff(infilename_a, infilename_b, outfilename)\nCalculate difference between two dfs files (a-b).\n\n\n\ngeneric.extract(infilename, outfilename, start=0, end=-1, step=1, items=None)\nExtract timesteps and/or items to a new dfs file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninfilename\nstr | pathlib.Path\npath to input dfs file\nrequired\n\n\noutfilename\nstr | pathlib.Path\npath to output dfs file\nrequired\n\n\nstart\n(int, float, str or datetime)\nstart of extraction as either step, relative seconds or datetime/str, by default 0 (start of file)\n0\n\n\nend\n(int, float, str or datetime)\nend of extraction as either step, relative seconds or datetime/str, by default -1 (end of file)\n-1\n\n\nstep\nint\njump this many step, by default 1 (every step between start and end)\n1\n\n\nitems\n(int, list(int), str, list(str))\nitems to be extracted to new file\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; extract('f_in.dfs0', 'f_out.dfs0', start='2018-1-1')\n&gt;&gt;&gt; extract('f_in.dfs2', 'f_out.dfs2', end=-3)\n&gt;&gt;&gt; extract('f_in.dfsu', 'f_out.dfsu', start=1800.0, end=3600.0)\n&gt;&gt;&gt; extract('f_hourly.dfsu', 'f_daily.dfsu', step=24)\n&gt;&gt;&gt; extract('f_in.dfsu', 'f_out.dfsu', items=[2, 0])\n&gt;&gt;&gt; extract('f_in.dfsu', 'f_out.dfsu', items=\"Salinity\")\n&gt;&gt;&gt; extract('f_in.dfsu', 'f_out.dfsu', end='2018-2-1 00:00', items=\"Salinity\")\n\n\n\n\ngeneric.fill_corrupt(infilename, outfilename, fill_value=np.nan, items=None)\nReplace corrupt (unreadable) data with fill_value, default delete value.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninfilename\nstr | pathlib.Path\nfull path to the input file\nrequired\n\n\noutfilename\nstr | pathlib.Path\nfull path to the output file\nrequired\n\n\nfill_value\nfloat\nvalue to use where data is corrupt, default delete value\nnp.nan\n\n\nitems\nSequence[str | int] | None\nProcess only selected items, by number (0-based) or name, by default: all\nNone\n\n\n\n\n\n\n\ngeneric.quantile(\n    infilename,\n    outfilename,\n    q,\n    *,\n    items=None,\n    skipna=True,\n    buffer_size=1000000000.0,\n)\nCreate temporal quantiles of all items in dfs file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninfilename\nstr | pathlib.Path\ninput filename\nrequired\n\n\noutfilename\nstr | pathlib.Path\noutput filename\nrequired\n\n\nq\nfloat | Sequence[float]\nQuantile or sequence of quantiles to compute, which must be between 0 and 1 inclusive.\nrequired\n\n\nitems\nSequence[int | str] | int | str | None\nProcess only selected items, by number (0-based) or name, by default: all\nNone\n\n\nskipna\nbool\nexclude NaN/delete values when computing the result, default True\nTrue\n\n\nbuffer_size\nfloat\nfor huge files the quantiles need to be calculated for chunks of elements. buffer_size gives the maximum amount of memory available for the computation in bytes, by default 1e9 (=1GB)\n1000000000.0\n\n\n\n\n\n\n&gt;&gt;&gt; quantile(\"in.dfsu\", \"IQR.dfsu\", q=[0.25,0.75])\n&gt;&gt;&gt; quantile(\"huge.dfsu\", \"Q01.dfsu\", q=0.1, buffer_size=5.0e9)\n&gt;&gt;&gt; quantile(\"with_nans.dfsu\", \"Q05.dfsu\", q=0.5, skipna=False)\n\n\n\n\ngeneric.scale(infilename, outfilename, offset=0.0, factor=1.0, items=None)\nApply scaling to any dfs file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninfilename\nstr | pathlib.Path\nfull path to the input file\nrequired\n\n\noutfilename\nstr | pathlib.Path\nfull path to the output file\nrequired\n\n\noffset\nfloat\nvalue to add to all items, default 0.0\n0.0\n\n\nfactor\nfloat\nvalue to multiply to all items, default 1.0\n1.0\n\n\nitems\nSequence[int | str] | None\nProcess only selected items, by number (0-based) or name, by default: all\nNone\n\n\n\n\n\n\n\ngeneric.sum(infilename_a, infilename_b, outfilename)\nSum two dfs files (a+b)."
  },
  {
    "objectID": "api/from_pandas.html",
    "href": "api/from_pandas.html",
    "title": "from_pandas",
    "section": "",
    "text": "from_pandas(df, items=None)\nCreate a Dataset from a pandas DataFrame."
  },
  {
    "objectID": "api/from_pandas.html#parameters",
    "href": "api/from_pandas.html#parameters",
    "title": "from_pandas",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame | pd.Series\nDataFrame with time index\nrequired\n\n\nitems\nMapping[str, ItemInfo] | Sequence[ItemInfo] | ItemInfo | None\nMapping of item names to ItemInfo objects, or a sequence of ItemInfo objects, or a single ItemInfo object.\nNone"
  },
  {
    "objectID": "api/from_pandas.html#returns",
    "href": "api/from_pandas.html#returns",
    "title": "from_pandas",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ntime series dataset"
  },
  {
    "objectID": "api/from_pandas.html#examples",
    "href": "api/from_pandas.html#examples",
    "title": "from_pandas",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport mikeio\n\ndf = pd.DataFrame(\n    {\n        \"A\": [1, 2, 3],\n        \"B\": [4, 5, 6],\n    },\n    index=pd.date_range(\"20210101\", periods=3, freq=\"D\"),\n)\nds = mikeio.from_pandas(df, items={\"A\": mikeio.ItemInfo(mikeio.EUMType.Water_Level),\n                                   \"B\": mikeio.ItemInfo(mikeio.EUMType.Discharge)})\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:3)\ntime: 2021-01-01 00:00:00 - 2021-01-03 00:00:00 (3 records)\ngeometry: GeometryUndefined()\nitems:\n  0:  A &lt;Water Level&gt; (meter)\n  1:  B &lt;Discharge&gt; (meter pow 3 per sec)"
  },
  {
    "objectID": "api/Dfs3.html",
    "href": "api/Dfs3.html",
    "title": "Dfs3",
    "section": "",
    "text": "Dfs3(self, filename)\nClass for reading/writing dfs3 files.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nPath to dfs3 file\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndeletevalue\nFile delete value.\n\n\ndx\nStep size in x direction.\n\n\ndy\nStep size in y direction.\n\n\ndz\nStep size in y direction.\n\n\nend_time\nFile end time.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nitems\nList of items.\n\n\nlatitude\nOrigin latitude.\n\n\nlongitude\nOrigin longitude.\n\n\nn_items\nNumber of items.\n\n\nn_timesteps\nNumber of time steps.\n\n\norientation\nOrientation (in own CRS).\n\n\norigin\nOrigin (in own CRS).\n\n\nstart_time\nFile start time.\n\n\ntimestep\nTime step size in seconds.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nappend\nAppend a Dataset to an existing dfs3 file.\n\n\nread\nRead data from a dfs3 file.\n\n\n\n\n\nDfs3.append(ds, validate=True)\nAppend a Dataset to an existing dfs3 file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nDataset to append\nrequired\n\n\nvalidate\nbool\nValidate that the dataset to append has the same geometry and items as the original file\nTrue\n\n\n\n\n\n\nThe original file is modified.\n\n\n\n\nDfs3.read(\n    items=None,\n    time=None,\n    area=None,\n    layers=None,\n    keepdims=False,\n    dtype=np.float32,\n)\nRead data from a dfs3 file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\narea\ntuple[float, float, float, float] | None\nRead only data within the specified rectangular area (x0, x1, y0, y1)\nNone\n\n\nkeepdims\nbool\nWhen reading a single time step or a single layer only, should the singleton dimension be kept in the returned Dataset? by default: False\nFalse\n\n\nlayers\nstr | int | Sequence[int] | None\nRead only data for specific layers, by default None\nNone\n\n\ndtype\nAny\nDefine the dtype of the returned dataset (default = np.float32)\nnp.float32\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset"
  },
  {
    "objectID": "api/Dfs3.html#parameters",
    "href": "api/Dfs3.html#parameters",
    "title": "Dfs3",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nPath to dfs3 file\nrequired"
  },
  {
    "objectID": "api/Dfs3.html#attributes",
    "href": "api/Dfs3.html#attributes",
    "title": "Dfs3",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndeletevalue\nFile delete value.\n\n\ndx\nStep size in x direction.\n\n\ndy\nStep size in y direction.\n\n\ndz\nStep size in y direction.\n\n\nend_time\nFile end time.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nitems\nList of items.\n\n\nlatitude\nOrigin latitude.\n\n\nlongitude\nOrigin longitude.\n\n\nn_items\nNumber of items.\n\n\nn_timesteps\nNumber of time steps.\n\n\norientation\nOrientation (in own CRS).\n\n\norigin\nOrigin (in own CRS).\n\n\nstart_time\nFile start time.\n\n\ntimestep\nTime step size in seconds."
  },
  {
    "objectID": "api/Dfs3.html#methods",
    "href": "api/Dfs3.html#methods",
    "title": "Dfs3",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nappend\nAppend a Dataset to an existing dfs3 file.\n\n\nread\nRead data from a dfs3 file.\n\n\n\n\n\nDfs3.append(ds, validate=True)\nAppend a Dataset to an existing dfs3 file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nDataset to append\nrequired\n\n\nvalidate\nbool\nValidate that the dataset to append has the same geometry and items as the original file\nTrue\n\n\n\n\n\n\nThe original file is modified.\n\n\n\n\nDfs3.read(\n    items=None,\n    time=None,\n    area=None,\n    layers=None,\n    keepdims=False,\n    dtype=np.float32,\n)\nRead data from a dfs3 file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\narea\ntuple[float, float, float, float] | None\nRead only data within the specified rectangular area (x0, x1, y0, y1)\nNone\n\n\nkeepdims\nbool\nWhen reading a single time step or a single layer only, should the singleton dimension be kept in the returned Dataset? by default: False\nFalse\n\n\nlayers\nstr | int | Sequence[int] | None\nRead only data for specific layers, by default None\nNone\n\n\ndtype\nAny\nDefine the dtype of the returned dataset (default = np.float32)\nnp.float32\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset"
  },
  {
    "objectID": "api/spatial.GeometryFMVerticalProfile.html",
    "href": "api/spatial.GeometryFMVerticalProfile.html",
    "title": "spatial.GeometryFMVerticalProfile",
    "section": "",
    "text": "spatial.GeometryFMVerticalProfile(\n    self,\n    node_coordinates,\n    element_table,\n    codes=None,\n    projection='LONG/LAT',\n    dfsu_type=DfsuFileType.Dfsu3DSigma,\n    element_ids=None,\n    node_ids=None,\n    n_layers=1,\n    n_sigma=None,\n    validate=True,\n    reindex=False,\n)\nFlexible mesh 2d vertical profile geometry.\n\n\n\n\n\nName\nDescription\n\n\n\n\nbottom_elements\nList of 3d element ids of bottom layer.\n\n\nboundary_codes\nUnique list of boundary codes.\n\n\ncodes\nNode codes of all nodes (0=water, 1=land, 2…=open boundaries).\n\n\ne2_e3_table\nThe 2d-to-3d element connectivity table for a 3d object.\n\n\nelem2d_ids\nThe associated 2d element id for each 3d element.\n\n\nelement_coordinates\nCenter coordinates of each element.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nlayer_ids\nThe layer number (0=bottom, 1, 2, …) for each 3d element.\n\n\nmax_nodes_per_element\nThe maximum number of nodes for an element.\n\n\nn_elements\nNumber of 3d elements.\n\n\nn_layers\nMaximum number of layers.\n\n\nn_layers_per_column\nList of number of layers for each column.\n\n\nn_sigma_layers\nNumber of sigma layers.\n\n\nn_z_layers\nMaximum number of z-layers.\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string.\n\n\ntop_elements\nList of 3d element ids of surface layer.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_layer_elements\n3d element ids for one (or more) specific layer(s).\n\n\nget_nearest_relative_distance\nFor a point near a transect, find the nearest relative distance\n\n\nto_2d_geometry\nextract 2d geometry from 3d geometry.\n\n\n\n\n\nspatial.GeometryFMVerticalProfile.get_layer_elements(layers)\n3d element ids for one (or more) specific layer(s).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlayers\nint or list(int)\nlayer between 0 (bottom) and n_layers-1 (top) (can also be negative counting from -1 at the top layer)\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array(int)\nelement ids\n\n\n\n\n\n\n\nspatial.GeometryFMVerticalProfile.get_nearest_relative_distance(coords)\nFor a point near a transect, find the nearest relative distance for showing position on transect plot.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncoords\n[float, float]\nx,y-coordinate of point\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nfloat\nrelative distance in meters from start of transect\n\n\n\n\n\n\n\nspatial.GeometryFMVerticalProfile.to_2d_geometry()\nextract 2d geometry from 3d geometry.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nGeometryFM2D\n2d geometry"
  },
  {
    "objectID": "api/spatial.GeometryFMVerticalProfile.html#attributes",
    "href": "api/spatial.GeometryFMVerticalProfile.html#attributes",
    "title": "spatial.GeometryFMVerticalProfile",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nbottom_elements\nList of 3d element ids of bottom layer.\n\n\nboundary_codes\nUnique list of boundary codes.\n\n\ncodes\nNode codes of all nodes (0=water, 1=land, 2…=open boundaries).\n\n\ne2_e3_table\nThe 2d-to-3d element connectivity table for a 3d object.\n\n\nelem2d_ids\nThe associated 2d element id for each 3d element.\n\n\nelement_coordinates\nCenter coordinates of each element.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nlayer_ids\nThe layer number (0=bottom, 1, 2, …) for each 3d element.\n\n\nmax_nodes_per_element\nThe maximum number of nodes for an element.\n\n\nn_elements\nNumber of 3d elements.\n\n\nn_layers\nMaximum number of layers.\n\n\nn_layers_per_column\nList of number of layers for each column.\n\n\nn_sigma_layers\nNumber of sigma layers.\n\n\nn_z_layers\nMaximum number of z-layers.\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string.\n\n\ntop_elements\nList of 3d element ids of surface layer."
  },
  {
    "objectID": "api/spatial.GeometryFMVerticalProfile.html#methods",
    "href": "api/spatial.GeometryFMVerticalProfile.html#methods",
    "title": "spatial.GeometryFMVerticalProfile",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_layer_elements\n3d element ids for one (or more) specific layer(s).\n\n\nget_nearest_relative_distance\nFor a point near a transect, find the nearest relative distance\n\n\nto_2d_geometry\nextract 2d geometry from 3d geometry.\n\n\n\n\n\nspatial.GeometryFMVerticalProfile.get_layer_elements(layers)\n3d element ids for one (or more) specific layer(s).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlayers\nint or list(int)\nlayer between 0 (bottom) and n_layers-1 (top) (can also be negative counting from -1 at the top layer)\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array(int)\nelement ids\n\n\n\n\n\n\n\nspatial.GeometryFMVerticalProfile.get_nearest_relative_distance(coords)\nFor a point near a transect, find the nearest relative distance for showing position on transect plot.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncoords\n[float, float]\nx,y-coordinate of point\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nfloat\nrelative distance in meters from start of transect\n\n\n\n\n\n\n\nspatial.GeometryFMVerticalProfile.to_2d_geometry()\nextract 2d geometry from 3d geometry.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nGeometryFM2D\n2d geometry"
  },
  {
    "objectID": "api/spatial.GeometryFMLineSpectrum.html",
    "href": "api/spatial.GeometryFMLineSpectrum.html",
    "title": "spatial.GeometryFMLineSpectrum",
    "section": "",
    "text": "spatial.GeometryFMLineSpectrum(\n    self,\n    node_coordinates,\n    element_table,\n    codes=None,\n    projection='LONG/LAT',\n    dfsu_type=None,\n    element_ids=None,\n    node_ids=None,\n    validate=True,\n    frequencies=None,\n    directions=None,\n    reindex=False,\n)\nFlexible mesh line spectrum geometry.\n\n\n\n\n\nName\nDescription\n\n\n\n\nboundary_codes\nUnique list of boundary codes.\n\n\ncodes\nNode codes of all nodes (0=water, 1=land, 2…=open boundaries).\n\n\ndirections\nDirectional axis.\n\n\nelement_coordinates\nCenter coordinates of each element.\n\n\nfrequencies\nFrequency axis.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nis_spectral\nType is spectral dfsu (point, line or area spectrum).\n\n\nmax_nodes_per_element\nThe maximum number of nodes for an element.\n\n\nn_directions\nNumber of directions.\n\n\nn_elements\nNumber of elements.\n\n\nn_frequencies\nNumber of frequencies.\n\n\nn_nodes\nNumber of nodes.\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string."
  },
  {
    "objectID": "api/spatial.GeometryFMLineSpectrum.html#attributes",
    "href": "api/spatial.GeometryFMLineSpectrum.html#attributes",
    "title": "spatial.GeometryFMLineSpectrum",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nboundary_codes\nUnique list of boundary codes.\n\n\ncodes\nNode codes of all nodes (0=water, 1=land, 2…=open boundaries).\n\n\ndirections\nDirectional axis.\n\n\nelement_coordinates\nCenter coordinates of each element.\n\n\nfrequencies\nFrequency axis.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nis_spectral\nType is spectral dfsu (point, line or area spectrum).\n\n\nmax_nodes_per_element\nThe maximum number of nodes for an element.\n\n\nn_directions\nNumber of directions.\n\n\nn_elements\nNumber of elements.\n\n\nn_frequencies\nNumber of frequencies.\n\n\nn_nodes\nNumber of nodes.\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string."
  },
  {
    "objectID": "api/from_polars.html",
    "href": "api/from_polars.html",
    "title": "from_polars",
    "section": "",
    "text": "from_polars(df, items=None, datetime_col=None)\nCreate a Dataset from a polars DataFrame."
  },
  {
    "objectID": "api/from_polars.html#parameters",
    "href": "api/from_polars.html#parameters",
    "title": "from_polars",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npl.DataFrame\nDataFrame\nrequired\n\n\nitems\nMapping[str, ItemInfo] | Sequence[ItemInfo] | ItemInfo | None\nMapping of item names to ItemInfo objects, or a sequence of ItemInfo objects, or a single ItemInfo object.\nNone\n\n\ndatetime_col\nstr | None\nName of the column containing datetime information, default is to use the first datetime column found.\nNone"
  },
  {
    "objectID": "api/from_polars.html#returns",
    "href": "api/from_polars.html#returns",
    "title": "from_polars",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ntime series dataset"
  },
  {
    "objectID": "api/from_polars.html#examples",
    "href": "api/from_polars.html#examples",
    "title": "from_polars",
    "section": "Examples",
    "text": "Examples\n\nimport polars as pl\nimport mikeio\nfrom datetime import datetime\n\ndf = pl.DataFrame(\n    {\n        \"time\": [datetime(2021, 1, 1), datetime(2021, 1, 2)],\n        \"A\": [1.0, 2.0],\n        \"B\": [4.0, 5.0],\n    }\n)\n\nds = mikeio.from_polars(\n    df,\n    items={\n        \"A\": mikeio.ItemInfo(mikeio.EUMType.Water_Level),\n        \"B\": mikeio.ItemInfo(mikeio.EUMType.Discharge),\n    },\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:2)\ntime: 2021-01-01 00:00:00 - 2021-01-02 00:00:00 (2 records)\ngeometry: GeometryUndefined()\nitems:\n  0:  A &lt;Water Level&gt; (meter)\n  1:  B &lt;Discharge&gt; (meter pow 3 per sec)"
  },
  {
    "objectID": "api/Mesh.html",
    "href": "api/Mesh.html",
    "title": "Mesh",
    "section": "",
    "text": "Mesh(self, filename)\nThe Mesh class is initialized with a mesh file."
  },
  {
    "objectID": "api/Mesh.html#parameters",
    "href": "api/Mesh.html#parameters",
    "title": "Mesh",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nmesh filename\nrequired"
  },
  {
    "objectID": "api/Mesh.html#attributes",
    "href": "api/Mesh.html#attributes",
    "title": "Mesh",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nType\nDescription\n\n\n\n\ngeometry\nGeometryFM2D\nFlexible Mesh geometry"
  },
  {
    "objectID": "api/Mesh.html#examples",
    "href": "api/Mesh.html#examples",
    "title": "Mesh",
    "section": "Examples",
    "text": "Examples\n\nimport mikeio\nmikeio.Mesh(\"../data/odense_rough.mesh\")\n\n&lt;Mesh&gt;\nnumber of nodes: 399\nnumber of elements: 654\nprojection: UTM-33"
  },
  {
    "objectID": "api/Mesh.html#methods",
    "href": "api/Mesh.html#methods",
    "title": "Mesh",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nto_shapely\nConvert Mesh geometry to shapely MultiPolygon.\n\n\nwrite\nwrite mesh to file.\n\n\n\n\nto_shapely\nMesh.to_shapely()\nConvert Mesh geometry to shapely MultiPolygon.\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nMultiPolygon\nmesh as shapely MultiPolygon\n\n\n\n\n\nExamples\n\nimport mikeio\nmsh = mikeio.open(\"../data/odense_rough.mesh\")\nmsh.to_shapely()\n\n\n\n\n\n\n\n\n\n\n\nwrite\nMesh.write(outfilename)\nwrite mesh to file.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noutfilename\nstr\npath to file\nrequired"
  },
  {
    "objectID": "api/spatial.GeometryFMPointSpectrum.html",
    "href": "api/spatial.GeometryFMPointSpectrum.html",
    "title": "spatial.GeometryFMPointSpectrum",
    "section": "",
    "text": "spatial.GeometryFMPointSpectrum(\n    self,\n    frequencies=None,\n    directions=None,\n    x=None,\n    y=None,\n)\nFlexible mesh point spectrum.\n\n\n\n\n\nName\nDescription\n\n\n\n\ndirections\nDirectional axis.\n\n\nfrequencies\nFrequency axis.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nn_directions\nNumber of directions.\n\n\nn_frequencies\nNumber of frequencies.\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string."
  },
  {
    "objectID": "api/spatial.GeometryFMPointSpectrum.html#attributes",
    "href": "api/spatial.GeometryFMPointSpectrum.html#attributes",
    "title": "spatial.GeometryFMPointSpectrum",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndirections\nDirectional axis.\n\n\nfrequencies\nFrequency axis.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nn_directions\nNumber of directions.\n\n\nn_frequencies\nNumber of frequencies.\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string."
  },
  {
    "objectID": "api/dfsu.Dfsu2DH.html",
    "href": "api/dfsu.Dfsu2DH.html",
    "title": "dfsu.Dfsu2DH",
    "section": "",
    "text": "dfsu.Dfsu2DH(self, filename)\nClass for reading/writing dfsu 2d horizontal files."
  },
  {
    "objectID": "api/dfsu.Dfsu2DH.html#parameters",
    "href": "api/dfsu.Dfsu2DH.html#parameters",
    "title": "dfsu.Dfsu2DH",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nPath to dfsu file\nrequired"
  },
  {
    "objectID": "api/dfsu.Dfsu2DH.html#attributes",
    "href": "api/dfsu.Dfsu2DH.html#attributes",
    "title": "dfsu.Dfsu2DH",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\ndeletevalue\nFile delete value.\n\n\nend_time\nFile end time.\n\n\nitems\nList of items.\n\n\nn_items\nNumber of items.\n\n\nn_timesteps\nNumber of time steps.\n\n\nstart_time\nFile start time.\n\n\ntimestep\nTime step size in seconds."
  },
  {
    "objectID": "api/dfsu.Dfsu2DH.html#methods",
    "href": "api/dfsu.Dfsu2DH.html#methods",
    "title": "dfsu.Dfsu2DH",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nappend\nAppend data to an existing dfsu file.\n\n\nextract_track\nExtract track data from a dfsu file.\n\n\nget_overset_grid\nget a 2d grid that covers the domain by specifying spacing or shape.\n\n\nread\nRead data from a dfsu file.\n\n\n\n\nappend\ndfsu.Dfsu2DH.append(ds, validate=True)\nAppend data to an existing dfsu file.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nDataset to be appended\nrequired\n\n\nvalidate\nbool\nValidate that the items and geometry match, by default True\nTrue\n\n\n\n\n\n\nextract_track\ndfsu.Dfsu2DH.extract_track(\n    track,\n    items=None,\n    method='nearest',\n    dtype=np.float32,\n)\nExtract track data from a dfsu file.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrack\nstr | Path | Dataset | pd.DataFrame\nwith DatetimeIndex and (x, y) of track points as first two columns x,y coordinates must be in same coordinate system as dfsu\nrequired\n\n\nitems\nint | str | Sequence[int | str] | None\nExtract only selected items, by number (0-based), or by name\nNone\n\n\nmethod\nLiteral['nearest', 'inverse_distance']\nSpatial interpolation method (‘nearest’ or ‘inverse_distance’) default=‘nearest’\n'nearest'\n\n\ndtype\nAny\nData type to read, by default np.float32\nnp.float32\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nA dataset with data dimension t The first two items will be x- and y- coordinates of track\n\n\n\n\n\nExamples\n\nimport mikeio\n\nds = (\n    mikeio.open(\"../data/NorthSea_HD_and_windspeed.dfsu\")\n          .extract_track(\"../data/altimetry_NorthSea_20171027.csv\")\n    )\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1115)\ntime: 2017-10-26 04:37:37 - 2017-10-30 20:54:47 (1115 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  Longitude &lt;Latitude longitude&gt; (degree)\n  1:  Latitude &lt;Latitude longitude&gt; (degree)\n  2:  Surface elevation &lt;Surface Elevation&gt; (meter)\n  3:  Wind speed &lt;Wind speed&gt; (meter per sec)\n\n\n\n\n\nget_overset_grid\ndfsu.Dfsu2DH.get_overset_grid(dx=None, dy=None, nx=None, ny=None, buffer=0.0)\nget a 2d grid that covers the domain by specifying spacing or shape.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndx\nfloat\ngrid resolution in x-direction (or in x- and y-direction)\nNone\n\n\ndy\nfloat\ngrid resolution in y-direction\nNone\n\n\nnx\nint\nnumber of points in x-direction, by default None (the value will be inferred)\nNone\n\n\nny\nint\nnumber of points in y-direction, by default None (the value will be inferred)\nNone\n\n\nbuffer\nfloat\npositive to make the area larger, default=0 can be set to a small negative value to avoid NaN values all around the domain.\n0.0\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n&lt;mikeio.Grid2D&gt;\n2d grid\n\n\n\n\n\n\nread\ndfsu.Dfsu2DH.read(\n    items=None,\n    time=None,\n    elements=None,\n    area=None,\n    x=None,\n    y=None,\n    keepdims=False,\n    dtype=np.float32,\n    error_bad_data=True,\n    fill_bad_data_value=np.nan,\n)\nRead data from a dfsu file.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\nkeepdims\nbool\nWhen reading a single time step only, should the time-dimension be kept in the returned Dataset? by default: False\nFalse\n\n\narea\ntuple[float, float, float, float] | Sequence[tuple[float, float]] | None\nRead only data inside (horizontal) area given as a bounding box (tuple with left, lower, right, upper) or as list of coordinates for a polygon, by default None\nNone\n\n\nx\nfloat | Sequence[float] | None\nRead only data for elements containing the (x,y) points(s), by default None\nNone\n\n\ny\nfloat | Sequence[float] | None\nRead only data for elements containing the (x,y) points(s), by default None\nNone\n\n\nelements\nint | Sequence[int] | np.ndarray | None\nRead only selected element ids, by default None\nNone\n\n\nerror_bad_data\nbool\nraise error if data is corrupt, by default True,\nTrue\n\n\nfill_bad_data_value\nfloat\nfill value for to impute corrupt data, used in conjunction with error_bad_data=False default np.nan\nnp.nan\n\n\ndtype\nAny\nData type to read, by default np.float32\nnp.float32\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nA Dataset with data dimensions [t,elements]"
  },
  {
    "objectID": "api/DataArray.html",
    "href": "api/DataArray.html",
    "title": "DataArray",
    "section": "",
    "text": "DataArray(\n    self,\n    data,\n    *,\n    time=None,\n    name=None,\n    type=None,\n    unit=None,\n    item=None,\n    geometry=None,\n    zn=None,\n    dims=None,\n    dt=1.0,\n)\nDataArray with data and metadata for a single item in a dfs file."
  },
  {
    "objectID": "api/DataArray.html#parameters",
    "href": "api/DataArray.html#parameters",
    "title": "DataArray",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nArrayLike\na numpy array containing the data\nrequired\n\n\ntime\npd.DatetimeIndex | pd.TimedeltaIndex | str | None\na pandas.DatetimeIndex with the time instances of the data\nNone\n\n\nname\nstr | None\nName of the array\nNone\n\n\ntype\nEUMType | None\nEUM type\nNone\n\n\nunit\nEUMUnit | None\nEUM unit\nNone\n\n\nitem\nItemInfo | None\nan ItemInfo with name, type and unit, as an alternative to name, type and unit\nNone\n\n\ngeometry\nGeometryType | None\na geometry object e.g. Grid2D or GeometryFM2D\nNone\n\n\nzn\nnp.ndarray | None\nonly relevant for Dfsu3d\nNone\n\n\ndims\nSequence[str] | None\nnamed dimensions\nNone\n\n\ndt\nfloat\nplaceholder timestep\n1.0"
  },
  {
    "objectID": "api/DataArray.html#examples",
    "href": "api/DataArray.html#examples",
    "title": "DataArray",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport mikeio\n\nda = mikeio.DataArray([0.0, 1.0],\n    time=pd.date_range(\"2020-01-01\", periods=2),\n    item=mikeio.ItemInfo(\"Water level\", mikeio.EUMType.Water_Level))\nda\n\n&lt;mikeio.DataArray&gt;\nname: Water level\ndims: (time:2)\ntime: 2020-01-01 00:00:00 - 2020-01-02 00:00:00 (2 records)\ngeometry: GeometryUndefined()\nvalues: [0, 1]"
  },
  {
    "objectID": "api/DataArray.html#attributes",
    "href": "api/DataArray.html#attributes",
    "title": "DataArray",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\ndtype\nData-type of the array elements.\n\n\nend_time\nLast time instance (as datetime).\n\n\nis_equidistant\nIs DataArray equidistant in time?\n\n\nn_timesteps\nNumber of time steps.\n\n\nname\nName of this DataArray (=da.item.name).\n\n\nndim\nNumber of array dimensions.\n\n\nshape\nTuple of array dimensions.\n\n\nstart_time\nFirst time instance (as datetime).\n\n\ntimestep\nTime step in seconds if equidistant (and at\n\n\ntype\nEUMType.\n\n\nunit\nEUMUnit.\n\n\nvalues\nValues as a np.ndarray (equivalent to to_numpy())."
  },
  {
    "objectID": "api/DataArray.html#methods",
    "href": "api/DataArray.html#methods",
    "title": "DataArray",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\naggregate\nAggregate along an axis.\n\n\naverage\nCompute the weighted average along the specified axis.\n\n\nconcat\nConcatenate DataArrays along the time axis.\n\n\ncopy\nMake copy of DataArray.\n\n\ndescribe\nGenerate descriptive statistics by wrapping pandas.DataFrame.describe.\n\n\ndropna\nRemove time steps where values are NaN.\n\n\nextract_track\nExtract data along a moving track.\n\n\nfillna\nFill NA/NaN value.\n\n\nflipud\nFlip upside down (on first non-time axis).\n\n\ninterp\nInterpolate data in time and space.\n\n\ninterp_like\nInterpolate in space (and in time) to other geometry (and time axis).\n\n\ninterp_na\nFill in NaNs by interpolating according to different methods.\n\n\ninterp_time\nTemporal interpolation.\n\n\nisel\nReturn a new DataArray whose data is given by\n\n\nmax\nMax value along an axis.\n\n\nmean\nMean value along an axis.\n\n\nmin\nMin value along an axis.\n\n\nnanmax\nMax value along an axis (NaN removed).\n\n\nnanmean\nMean value along an axis (NaN removed).\n\n\nnanmin\nMin value along an axis (NaN removed).\n\n\nnanquantile\nCompute the q-th quantile of the data along the specified axis, while ignoring nan values.\n\n\nnanstd\nStandard deviation value along an axis (NaN removed).\n\n\nptp\nRange (max - min) a.k.a Peak to Peak along an axis.\n\n\nquantile\nCompute the q-th quantile of the data along the specified axis.\n\n\nsel\nReturn a new DataArray whose data is given by\n\n\nsqueeze\nRemove axes of length 1.\n\n\nstd\nStandard deviation values along an axis.\n\n\nto_dataframe\nConvert to DataFrame.\n\n\nto_dfs\nWrite data to a new dfs file.\n\n\nto_numpy\nValues as a np.ndarray (equivalent to values).\n\n\nto_pandas\nConvert to Pandas Series.\n\n\nto_xarray\nExport to xarray.DataArray.\n\n\n\n\naggregate\nDataArray.aggregate(axis=0, func=np.nanmean, **kwargs)\nAggregate along an axis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str | None\naxis number or “time” or “space”, by default 0\n0\n\n\nfunc\nCallable[…, Any]\ndefault np.nanmean\nnp.nanmean\n\n\n**kwargs\nAny\nAdditional keyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\ndataarray with aggregated values\n\n\n\n\n\nSee Also\nmax : Max values\nnanmax : Max values with NaN values removed\n\n\n\naverage\nDataArray.average(weights, axis=0, **kwargs)\nCompute the weighted average along the specified axis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str\naxis number or “time” or “space”, by default\n0\n\n\nweights\nnp.ndarray\nweights to apply to the values\nrequired\n\n\n**kwargs\nAny\nAdditional keyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\nDataArray with weighted average values\n\n\n\n\n\nSee Also\naggregate : Weighted average\n\n\nExamples\n\nimport mikeio\nda= mikeio.read(\"../data/HD2D.dfsu\")[\"Current speed\"]\narea = da.geometry.get_element_area()\nda.average(axis=\"space\", weights=area)\n\n&lt;mikeio.DataArray&gt;\nname: Current speed\ndims: (time:9)\ntime: 1985-08-06 07:00:00 - 1985-08-07 03:00:00 (9 records)\ngeometry: GeometryUndefined()\nvalues: [0.05017, 0.05321, ..., 0.06071]\n\n\n\n\n\nconcat\nDataArray.concat(dataarrays, keep='last')\nConcatenate DataArrays along the time axis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndataarrays\nSequence[DataArray]\nDataArrays to concatenate\nrequired\n\n\nkeep\nLiteral['last', 'first']\ndefault: last\n'last'\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\nThe concatenated DataArray\n\n\n\n\n\nExamples\n\nda1 = mikeio.read(\"../data/HD2D.dfsu\", time=[0,1])[0]\nda2 = mikeio.read(\"../data/HD2D.dfsu\", time=[2,3])[0]\nda1.time\n\nDatetimeIndex(['1985-08-06 07:00:00', '1985-08-06 09:30:00'], dtype='datetime64[s]', freq=None)\n\n\n\nda3 = mikeio.DataArray.concat([da1,da2])\nda3\n\n&lt;mikeio.DataArray&gt;\nname: Surface elevation\ndims: (time:4, element:884)\ntime: 1985-08-06 07:00:00 - 1985-08-06 14:30:00 (4 records)\ngeometry: Dfsu2D (884 elements, 529 nodes)\n\n\n\n\n\ncopy\nDataArray.copy()\nMake copy of DataArray.\n\n\ndescribe\nDataArray.describe(percentiles=None, include=None, exclude=None)\nGenerate descriptive statistics by wrapping pandas.DataFrame.describe.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npercentiles\nlist-like of numbers\nThe percentiles to include in the output. All should fall between 0 and 1.\nNone\n\n\ninclude\n'all', list-like of dtypes or None (default)\nA white list of data types to include in the result.\nNone\n\n\nexclude\nlist-like of dtypes or None (default)\nA black list of data types to omit from the result.\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\n\n\n\n\n\n\n\ndropna\nDataArray.dropna()\nRemove time steps where values are NaN.\n\n\nextract_track\nDataArray.extract_track(track, method='nearest', dtype=np.float32)\nExtract data along a moving track.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrack\nstr | Path | Dataset | pd.DataFrame\nwith DatetimeIndex and (x, y) of track points as first two columns x,y coordinates must be in same coordinate system\nrequired\n\n\nmethod\nLiteral['nearest', 'inverse_distance']\nSpatial interpolation method (‘nearest’ or ‘inverse_distance’) default=‘nearest’\n'nearest'\n\n\ndtype\nAny\nData type of the output data, default=np.float32\nnp.float32\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nA dataset with data dimension t The first two items will be x- and y- coordinates of track\n\n\n\n\n\n\nfillna\nDataArray.fillna(value=0.0)\nFill NA/NaN value.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nfloat\nValue used to fill missing values. Default is 0.0.\n0.0\n\n\n\n\n\nExamples\n\nimport numpy as np\nimport mikeio\n\nda = mikeio.DataArray([np.nan, 1.0])\nda\n\n&lt;mikeio.DataArray&gt;\nname: NoName\ndims: (x:2)\ntime: 2018-01-01 00:00:00 (time-invariant)\ngeometry: GeometryUndefined()\nvalues: [nan, 1]\n\n\n\nda.fillna(0.0)\n\n&lt;mikeio.DataArray&gt;\nname: NoName\ndims: (x:2)\ntime: 2018-01-01 00:00:00 (time-invariant)\ngeometry: GeometryUndefined()\nvalues: [0, 1]\n\n\n\n\n\nflipud\nDataArray.flipud()\nFlip upside down (on first non-time axis).\n\n\ninterp\nDataArray.interp(time=None, x=None, y=None, z=None, **kwargs)\nInterpolate data in time and space.\nThis method currently has limited functionality for spatial interpolation. It will be extended in the future.\nThe spatial parameters available depend on the geometry of the Dataset:\n\nGrid1D: x\nGrid2D: x, y\nGrid3D: [not yet implemented!]\nGeometryFM: (x,y)\nGeometryFMLayered: (x,y) [surface point will be returned!]\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntime\n(float, pd.DatetimeIndex or DataArray)\ntimestep in seconds or discrete time instances given by pd.DatetimeIndex (typically from another DataArray da2.time), by default None (=don’t interp in time)\nNone\n\n\nx\nfloat\nx-coordinate of point to be interpolated to, by default None\nNone\n\n\ny\nfloat\ny-coordinate of point to be interpolated to, by default None\nNone\n\n\nz\nfloat\nz-coordinate of point to be interpolated to, by default None\nNone\n\n\nn_nearest\nint\nWhen using IDW interpolation, how many nearest points should be used.\nrequired\n\n\ninterpolant\ntuple\nPrecomputed interpolant, by default None\nrequired\n\n\n**kwargs\nAny\nAdditional keyword arguments to be passed to the interpolation\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\nnew DataArray with interped data\n\n\n\n\n\nSee Also\nsel : Select data using label indexing interp_like : Interp to another time/space of another DataArray interp_time : Interp in the time direction only\n\n\nExamples\n\nimport mikeio\n\nda = mikeio.read(\"../data/HD2D.dfsu\")[\"Current speed\"]\nda.interp(x=340000, y=6160000)\n\n&lt;mikeio.DataArray&gt;\nname: Current speed\ndims: (time:9)\ntime: 1985-08-06 07:00:00 - 1985-08-07 03:00:00 (9 records)\ngeometry: GeometryPoint2D(x=340000, y=6160000)\nvalues: [nan, nan, ..., nan]\n\n\n\n\n\ninterp_like\nDataArray.interp_like(other, interpolant=None, **kwargs)\nInterpolate in space (and in time) to other geometry (and time axis).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\n(DataArray, Dataset, Grid2D, GeometryFM2D or pd.DatetimeIndex)\nThe geometry (and time axis) to interpolate to.\nrequired\n\n\ninterpolant\nInterpolant\nPrecomputed interpolant, by default None\nNone\n\n\n**kwargs\nAny\nAdditional keyword arguments to be passed to the interpolation\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\nNew DataArray with interpolated data\n\n\n\n\n\nNotes\nCurrently only supports interpolation from dfsu-2d to dfs2 or other dfsu-2d DataArrays\n\n\n\ninterp_na\nDataArray.interp_na(axis='time', **kwargs)\nFill in NaNs by interpolating according to different methods.\nWrapper of xarray.DataArray.interpolate_na\n\nExamples\n\nimport numpy as np\nimport pandas as pd\ntime = pd.date_range(\"2000\", periods=3, freq=\"D\")\nda = mikeio.DataArray(data=np.array([0.0, np.nan, 2.0]), time=time)\nda\n\n&lt;mikeio.DataArray&gt;\nname: NoName\ndims: (time:3)\ntime: 2000-01-01 00:00:00 - 2000-01-03 00:00:00 (3 records)\ngeometry: GeometryUndefined()\nvalues: [0, nan, 2]\n\n\n\nda.interp_na()\n\n&lt;mikeio.DataArray&gt;\nname: NoName\ndims: (time:3)\ntime: 2000-01-01 00:00:00 - 2000-01-03 00:00:00 (3 records)\ngeometry: GeometryUndefined()\nvalues: [0, 1, 2]\n\n\n\n\n\ninterp_time\nDataArray.interp_time(\n    dt,\n    *,\n    method='linear',\n    extrapolate=True,\n    fill_value=np.nan,\n)\nTemporal interpolation.\nWrapper of scipy.interpolate.interp1d\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndt\npd.DatetimeIndex | DataArray | Dataset | int | float\noutput timestep in seconds or new time axis\nrequired\n\n\nmethod\nstr\nSpecifies the kind of interpolation as a string (‘linear’, ‘nearest’, ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’, ‘previous’, ‘next’, where ‘zero’, ‘slinear’, ‘quadratic’ and ‘cubic’ refer to a spline interpolation of zeroth, first, second or third order; ‘previous’ and ‘next’ simply return the previous or next value of the point) or as an integer specifying the order of the spline interpolator to use. Default is ‘linear’.\n'linear'\n\n\nextrapolate\nbool\nDefault True. If False, a ValueError is raised any time interpolation is attempted on a value outside of the range of x (where extrapolation is necessary). If True, out of bounds values are assigned fill_value\nTrue\n\n\nfill_value\nfloat\nDefault NaN. this value will be used to fill in for points outside of the time range.\nnp.nan\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\n\n\n\n\n\n\n\nisel\nDataArray.isel(\n    idx=None,\n    *,\n    time=None,\n    x=None,\n    y=None,\n    z=None,\n    element=None,\n    node=None,\n    layer=None,\n    frequency=None,\n    direction=None,\n    axis=0,\n)\nReturn a new DataArray whose data is given by integer indexing along the specified dimension(s).\nNote that the data will be a view of the original data if possible (single index or slice), otherwise a copy (fancy indexing) following NumPy convention.\nThe spatial parameters available depend on the dims (i.e. geometry) of the DataArray:\n\nGrid1D: x\nGrid2D: x, y\nGrid3D: x, y, z\nGeometryFM: element\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\nIndexType\nIndex, or indices, along the specified dimension(s)\nNone\n\n\naxis\nAny\naxis number or “time”, by default 0\n0\n\n\ntime\nint\ntime index,by default None\nNone\n\n\nx\nint\nx index, by default None\nNone\n\n\ny\nint\ny index, by default None\nNone\n\n\nz\nint\nz index, by default None\nNone\n\n\nlayer\nIndexType\nlayer index, only used in dfsu 3d\nNone\n\n\ndirection\nIndexType\ndirection index, only used in sprectra\nNone\n\n\nfrequency\nIndexType\nfrequencey index, only used in spectra\nNone\n\n\nnode\nIndexType\nnode index, only used in spectra\nNone\n\n\nelement\nint\nBounding box of coordinates (left lower and right upper) to be selected, by default None\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\nnew DataArray with selected data\n\n\n\n\n\nSee Also\ndims : Get axis names sel : Select data using labels\n\n\nExamples\n\nda = mikeio.read(\"../data/europe_wind_long_lat.dfs2\")[0]\nda\n\n&lt;mikeio.DataArray&gt;\nname: Mean Sea Level Pressure\ndims: (time:1, y:101, x:221)\ntime: 2012-01-01 00:00:00 (time-invariant)\ngeometry: Grid2D (ny=101, nx=221)\n\n\n\nda.isel(time=-1)\n\n&lt;mikeio.DataArray&gt;\nname: Mean Sea Level Pressure\ndims: (y:101, x:221)\ntime: 2012-01-01 00:00:00 (time-invariant)\ngeometry: Grid2D (ny=101, nx=221)\n\n\n\nda.isel(x=slice(10,20), y=slice(40,60))\n\n&lt;mikeio.DataArray&gt;\nname: Mean Sea Level Pressure\ndims: (time:1, y:20, x:10)\ntime: 2012-01-01 00:00:00 (time-invariant)\ngeometry: Grid2D (ny=20, nx=10)\n\n\n\nda = mikeio.read(\"../data/oresund_sigma_z.dfsu\").Temperature\nda.isel(element=range(200))\n\n&lt;mikeio.DataArray&gt;\nname: Temperature\ndims: (time:3, element:200)\ntime: 1997-09-15 21:00:00 - 1997-09-16 03:00:00 (3 records)\ngeometry: Dfsu3DSigmaZ (200 elements, 638 nodes)\n\n\n\n\n\nmax\nDataArray.max(axis=0, **kwargs)\nMax value along an axis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str\naxis number or “time” or “space”, by default 0\n0\n\n\n**kwargs\nAny\nAdditional keyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\narray with max values\n\n\n\n\n\nSee Also\nnanmax : Max values with NaN values removed\n\n\n\nmean\nDataArray.mean(axis=0, **kwargs)\nMean value along an axis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str\naxis number or “time” or “space”, by default 0\n0\n\n\n**kwargs\nAny\nAdditional keyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\narray with mean values\n\n\n\n\n\nSee Also\nnanmean : Mean values with NaN values removed\n\n\n\nmin\nDataArray.min(axis=0, **kwargs)\nMin value along an axis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str\naxis number or “time” or “space”, by default 0\n0\n\n\n**kwargs\nAny\nAdditional keyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\narray with min values\n\n\n\n\n\nSee Also\nnanmin : Min values with NaN values removed\n\n\n\nnanmax\nDataArray.nanmax(axis=0, **kwargs)\nMax value along an axis (NaN removed).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str\naxis number or “time” or “space”, by default 0\n0\n\n\n**kwargs\nAny\nAdditional keyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\narray with max values\n\n\n\n\n\nSee Also\nnanmax : Max values with NaN values removed\n\n\n\nnanmean\nDataArray.nanmean(axis=0, **kwargs)\nMean value along an axis (NaN removed).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str | None\naxis number or “time” or “space”, by default 0\n0\n\n\n**kwargs\nAny\nAdditional keyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\narray with mean values\n\n\n\n\n\nSee Also\nmean : Mean values\n\n\n\nnanmin\nDataArray.nanmin(axis=0, **kwargs)\nMin value along an axis (NaN removed).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str\naxis number or “time” or “space”, by default 0\n0\n\n\n**kwargs\nAny\nAdditional keyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\narray with min values\n\n\n\n\n\nSee Also\nnanmin : Min values with NaN values removed\n\n\n\nnanquantile\nDataArray.nanquantile(q, *, axis=0, **kwargs)\nCompute the q-th quantile of the data along the specified axis, while ignoring nan values.\nWrapping np.nanquantile\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nq\nfloat | Sequence[float]\nQuantile or sequence of quantiles to compute, which must be between 0 and 1 inclusive.\nrequired\n\n\naxis\nint | str\naxis number or “time” or “space”, by default 0\n0\n\n\n**kwargs\nAny\nAdditional keyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\ndata with quantile values\n\n\n\n\n\nExamples\n&gt;&gt;&gt; da.nanquantile(q=[0.25,0.75])\n&gt;&gt;&gt; da.nanquantile(q=0.5)\n&gt;&gt;&gt; da.nanquantile(q=[0.01,0.5,0.99], axis=\"space\")\n\n\nSee Also\nquantile : Quantile with NaN values\n\n\n\nnanstd\nDataArray.nanstd(axis=0, **kwargs)\nStandard deviation value along an axis (NaN removed).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str\naxis number or “time” or “space”, by default 0\n0\n\n\n**kwargs\nAny\nAdditional keyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\narray with standard deviation values\n\n\n\n\n\nSee Also\nstd : Standard deviation\n\n\n\nptp\nDataArray.ptp(axis=0, **kwargs)\nRange (max - min) a.k.a Peak to Peak along an axis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str\naxis number or “time” or “space”, by default 0\n0\n\n\n**kwargs\nAny\nAdditional keyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\narray with peak to peak values\n\n\n\n\n\n\nquantile\nDataArray.quantile(q, *, axis=0, **kwargs)\nCompute the q-th quantile of the data along the specified axis.\nWrapping np.quantile\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nq\nfloat | Sequence[float]\nQuantile or sequence of quantiles to compute, which must be between 0 and 1 inclusive.\nrequired\n\n\naxis\nint | str\naxis number or “time” or “space”, by default 0\n0\n\n\n**kwargs\nAny\nAdditional keyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\ndata with quantile values\n\n\n\n\n\nExamples\n&gt;&gt;&gt; da.quantile(q=[0.25,0.75])\n&gt;&gt;&gt; da.quantile(q=0.5)\n&gt;&gt;&gt; da.quantile(q=[0.01,0.5,0.99], axis=\"space\")\n\n\nSee Also\nnanquantile : quantile with NaN values ignored\n\n\n\nsel\nDataArray.sel(\n    time=None,\n    x=None,\n    y=None,\n    z=None,\n    coords=None,\n    area=None,\n    layers=None,\n)\nReturn a new DataArray whose data is given by selecting index labels along the specified dimension(s).\nIn contrast to DataArray.isel, indexers for this method should use labels instead of integers.\nThe spatial parameters available depend on the geometry of the DataArray:\n\nGrid1D: x\nGrid2D: x, y, coords, area\nGrid3D: [not yet implemented! use isel instead]\nGeometryFM: (x,y), coords, area\nGeometryFMLayered: (x,y,z), coords, area, layers\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntime\n(str, pd.DatetimeIndex, DataArray)\ntime labels e.g. “2018-01” or slice(“2018-1-1”,“2019-1-1”), by default None\nNone\n\n\nx\nfloat\nx-coordinate of point to be selected, by default None\nNone\n\n\ny\nfloat\ny-coordinate of point to be selected, by default None\nNone\n\n\nz\nfloat\nz-coordinate of point to be selected, by default None\nNone\n\n\ncoords\nnp.array(float, float)\nAs an alternative to specifying x, y and z individually, the argument coords can be used instead. (x,y)- or (x,y,z)-coordinates of point to be selected, by default None\nNone\n\n\narea\n(float, float, float, float)\nBounding box of coordinates (left lower and right upper) to be selected, by default None\nNone\n\n\nlayers\nint or str or list\nlayer(s) to be selected: “top”, “bottom” or layer number from bottom 0,1,2,… or from the top -1,-2,… or as list of these; only for layered dfsu, by default None\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\nnew DataArray with selected data\n\n\n\n\n\nSee Also\nisel : Select data using integer indexing interp : Interp data in time and space\n\n\nExamples\n\nda = mikeio.read(\"../data/random.dfs1\")[0]\nda\n\n&lt;mikeio.DataArray&gt;\nname: testing water level\ndims: (time:100, x:3)\ntime: 2012-01-01 00:00:00 - 2012-01-01 00:19:48 (100 records)\ngeometry: Grid1D (n=3, dx=100)\n\n\n\nda.sel(time=slice(None, \"2012-1-1 00:02\"))\n\n&lt;mikeio.DataArray&gt;\nname: testing water level\ndims: (time:15, x:3)\ntime: 2012-01-01 00:00:00 - 2012-01-01 00:02:48 (15 records)\ngeometry: Grid1D (n=3, dx=100)\n\n\n\nda.sel(x=100)\n\n&lt;mikeio.DataArray&gt;\nname: testing water level\ndims: (time:100)\ntime: 2012-01-01 00:00:00 - 2012-01-01 00:19:48 (100 records)\ngeometry: GeometryUndefined()\nvalues: [0.3231, 0.6315, ..., 0.7506]\n\n\n\nda = mikeio.read(\"../data/oresund_sigma_z.dfsu\").Temperature\nda\n\n&lt;mikeio.DataArray&gt;\nname: Temperature\ndims: (time:3, element:17118)\ntime: 1997-09-15 21:00:00 - 1997-09-16 03:00:00 (3 records)\ngeometry: Dfsu3DSigmaZ (17118 elements, 12042 nodes)\n\n\n\nda.sel(time=\"1997-09-15\")\n\n&lt;mikeio.DataArray&gt;\nname: Temperature\ndims: (element:17118)\ntime: 1997-09-15 21:00:00 (time-invariant)\ngeometry: Dfsu3DSigmaZ (17118 elements, 12042 nodes)\nvalues: [16.31, 16.43, ..., 16.69]\n\n\n\nda.sel(x=340000, y=6160000, z=-3)\n\n&lt;mikeio.DataArray&gt;\nname: Temperature\ndims: (time:3)\ntime: 1997-09-15 21:00:00 - 1997-09-16 03:00:00 (3 records)\ngeometry: GeometryPoint3D(x=340028.1116933554, y=6159980.070243686, z=-3.0)\nvalues: [17.54, 17.31, 17.08]\n\n\n\nda.sel(layers=\"bottom\")\n\n&lt;mikeio.DataArray&gt;\nname: Temperature\ndims: (time:3, element:3700)\ntime: 1997-09-15 21:00:00 - 1997-09-16 03:00:00 (3 records)\ngeometry: Dfsu2D (3700 elements, 2090 nodes)\n\n\n\n\n\nsqueeze\nDataArray.squeeze()\nRemove axes of length 1.\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\n\n\n\n\n\n\n\nstd\nDataArray.std(axis=0, **kwargs)\nStandard deviation values along an axis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str\naxis number or “time” or “space”, by default 0\n0\n\n\n**kwargs\nAny\nAdditional keyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataArray\narray with standard deviation values\n\n\n\n\n\nSee Also\nnanstd : Standard deviation values with NaN values removed\n\n\n\nto_dataframe\nDataArray.to_dataframe(unit_in_name=False, round_time='ms')\nConvert to DataFrame.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nunit_in_name\nbool\ninclude unit in column name, default False,\nFalse\n\n\nround_time\nstr | bool\nround time to, by default “ms”, use False to avoid rounding\n'ms'\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\n\n\n\n\n\n\n\nto_dfs\nDataArray.to_dfs(filename, **kwargs)\nWrite data to a new dfs file.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nfull path to the new dfs file\nrequired\n\n\n**kwargs\nAny\nadditional arguments passed to the writing function, e.g. dtype for dfs0\n{}\n\n\n\n\n\n\nto_numpy\nDataArray.to_numpy()\nValues as a np.ndarray (equivalent to values).\n\n\nto_pandas\nDataArray.to_pandas()\nConvert to Pandas Series.\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\n\n\n\n\n\n\n\nto_xarray\nDataArray.to_xarray()\nExport to xarray.DataArray."
  },
  {
    "objectID": "api/dataset._DataArrayPlotterGrid2D.html",
    "href": "api/dataset._DataArrayPlotterGrid2D.html",
    "title": "dataset._DataArrayPlotterGrid2D",
    "section": "",
    "text": "dataset._DataArrayPlotterGrid2D(self, da)\nPlot a DataArray with a Grid2D geometry.\nIf DataArray has multiple time steps, the first step will be plotted."
  },
  {
    "objectID": "api/dataset._DataArrayPlotterGrid2D.html#examples",
    "href": "api/dataset._DataArrayPlotterGrid2D.html#examples",
    "title": "dataset._DataArrayPlotterGrid2D",
    "section": "Examples",
    "text": "Examples\n\nimport mikeio\nda = mikeio.read(\"../data/gebco_sound.dfs2\")[\"Elevation\"]\nda.plot()"
  },
  {
    "objectID": "api/dataset._DataArrayPlotterGrid2D.html#methods",
    "href": "api/dataset._DataArrayPlotterGrid2D.html#methods",
    "title": "dataset._DataArrayPlotterGrid2D",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\ncontour\nPlot data as contour lines.\n\n\ncontourf\nPlot data as filled contours.\n\n\nhist\nPlot DataArray as histogram (using ax.hist).\n\n\nline\nPlot data as lines (timeseries if time is present).\n\n\npcolormesh\nPlot data as coloured patches.\n\n\n\n\ncontour\ndataset._DataArrayPlotterGrid2D.contour(\n    ax=None,\n    figsize=None,\n    title=None,\n    **kwargs,\n)\nPlot data as contour lines.\n\nExamples\n\nda = mikeio.read(\"../data/gebco_sound.dfs2\")[\"Elevation\"]\nda.plot.contour()\n\n\n\n\n\n\n\n\n\n\n\ncontourf\ndataset._DataArrayPlotterGrid2D.contourf(\n    ax=None,\n    figsize=None,\n    title=None,\n    label=None,\n    **kwargs,\n)\nPlot data as filled contours.\n\nExamples\n\nda = mikeio.read(\"../data/gebco_sound.dfs2\")[\"Elevation\"]\nda.plot.contourf()\n\n\n\n\n\n\n\n\n\n\n\nhist\ndataset._DataArrayPlotterGrid2D.hist(\n    bins=None,\n    ax=None,\n    figsize=None,\n    title=None,\n    **kwargs,\n)\nPlot DataArray as histogram (using ax.hist).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbins\n(int or sequence or str)\nIf bins is an integer, it defines the number of equal-width bins in the range. If bins is a sequence, it defines the bin edges, including the left edge of the first bin and the right edge of the last bin. by default: rcParams[“hist.bins”] (default: 10)\nNone\n\n\nax\nAxes | None\nAdding to existing axis, instead of creating new fig\nNone\n\n\nfigsize\ntuple[float, float] | None\nspecify size of figure\nNone\n\n\ntitle\nstr | None\naxes title\nNone\n\n\n**kwargs\nAny\nadditional arguments passed to the plotting function\n{}\n\n\n\n\n\nSee Also\nmatplotlib.pyplot.hist\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n&lt;matplotlib.axes&gt;\n\n\n\n\n\n\n\nline\ndataset._DataArrayPlotterGrid2D.line(ax=None, figsize=None, **kwargs)\nPlot data as lines (timeseries if time is present).\n\n\npcolormesh\ndataset._DataArrayPlotterGrid2D.pcolormesh(\n    ax=None,\n    figsize=None,\n    title=None,\n    label=None,\n    **kwargs,\n)\nPlot data as coloured patches.\n\nExamples\n\nda = mikeio.read(\"../data/gebco_sound.dfs2\")[\"Elevation\"]\nda.plot.pcolormesh()"
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "API Reference",
    "section": "",
    "text": "open\nOpen a dfs/mesh file (and read the header).\n\n\nread\nRead all or a subset of the data from a dfs file.\n\n\nread_pfs\nRead a pfs file to a Pfs object for further analysis/manipulation.\n\n\nfrom_pandas\nCreate a Dataset from a pandas DataFrame.\n\n\nfrom_polars\nCreate a Dataset from a polars DataFrame.\n\n\n\n\n\n\n\n\n\nDataArray\nDataArray with data and metadata for a single item in a dfs file.\n\n\nDataset\nDataset containing one or more DataArrays with common geometry and time.\n\n\n\n\n\n\n\n\n\nGrid1D\n1d spatial grid.\n\n\nGrid2D\n2D grid.\n\n\nGrid3D\n3D grid.\n\n\nMesh\nThe Mesh class is initialized with a mesh file.\n\n\nspatial.GeometryFM2D\nFlexible 2d mesh geometry.\n\n\nspatial.GeometryFM3D\nFlexible 3d mesh geometry.\n\n\nspatial.GeometryFMVerticalProfile\nFlexible mesh 2d vertical profile geometry.\n\n\nspatial.GeometryFMVerticalColumn\nA 3d geometry with consisting of a single vertical column.\n\n\nspatial._FM_geometry._GeometryFMPlotter\nPlot GeometryFM.\n\n\n\n\n\n\n\n\n\ndfsu.DfsuSpectral\nDfsu for Spectral data.\n\n\nspatial.GeometryFMPointSpectrum\nFlexible mesh point spectrum.\n\n\nspatial.GeometryFMLineSpectrum\nFlexible mesh line spectrum geometry.\n\n\nspatial.GeometryFMAreaSpectrum\nFlexible mesh area spectrum geometry.\n\n\n\n\n\n\n\n\n\nItemInfo\nInfo for dynamic items (variables).\n\n\nEUMType\nEUM type.\n\n\nEUMUnit\nEUM unit.\n\n\n\n\n\n\n\n\n\nDfs0\nClass for reading/writing dfs0 files.\n\n\nDfs1\nClass for reading/writing dfs1 files.\n\n\nDfs2\nClass for reading/writing dfs2 files.\n\n\nDfs3\nClass for reading/writing dfs3 files.\n\n\nDfsu\nFactory class for dfsu files.\n\n\ndfsu.Dfsu2DH\nClass for reading/writing dfsu 2d horizontal files.\n\n\ndfsu.Dfsu2DV\nClass for reading/writing dfsu 2d vertical files.\n\n\ndfsu.Dfsu3D\nClass for reading/writing dfsu 3d files.\n\n\n\n\n\n\n\n\n\ngeneric\nGeneric functions for working with all types of dfs files.\n\n\n\n\n\n\n\n\n\nPfsDocument\nCreate a PfsDocument object for reading, writing and manipulating pfs files.\n\n\nPfsSection\nClass for reading/writing sections in a pfs file.\n\n\n\n\n\n\nPlotting functions for Dataset and DataArray objects.\n\n\n\ndataset._DatasetPlotter\nClass for plotting scatter plots from datasets.\n\n\ndataset._DataArrayPlotter\nContext aware plotter (sensible plotting according to geometry).\n\n\ndataset._DataArrayPlotterGrid1D\nPlot a DataArray with a Grid1D geometry.\n\n\ndataset._DataArrayPlotterGrid2D\nPlot a DataArray with a Grid2D geometry.\n\n\ndataset._DataArrayPlotterFM\nPlot a DataArray with a GeometryFM geometry.\n\n\ndataset._DataArrayPlotterFMVerticalProfile\nPlot a DataArray with a 2DV GeometryFMVerticalProfile geometry.\n\n\ndataset._DataArrayPlotterFMVerticalColumn\nPlot a DataArray with a GeometryFMVerticalColumn geometry."
  },
  {
    "objectID": "api/index.html#main",
    "href": "api/index.html#main",
    "title": "API Reference",
    "section": "",
    "text": "open\nOpen a dfs/mesh file (and read the header).\n\n\nread\nRead all or a subset of the data from a dfs file.\n\n\nread_pfs\nRead a pfs file to a Pfs object for further analysis/manipulation.\n\n\nfrom_pandas\nCreate a Dataset from a pandas DataFrame.\n\n\nfrom_polars\nCreate a Dataset from a polars DataFrame."
  },
  {
    "objectID": "api/index.html#dataset",
    "href": "api/index.html#dataset",
    "title": "API Reference",
    "section": "",
    "text": "DataArray\nDataArray with data and metadata for a single item in a dfs file.\n\n\nDataset\nDataset containing one or more DataArrays with common geometry and time."
  },
  {
    "objectID": "api/index.html#geometry",
    "href": "api/index.html#geometry",
    "title": "API Reference",
    "section": "",
    "text": "Grid1D\n1d spatial grid.\n\n\nGrid2D\n2D grid.\n\n\nGrid3D\n3D grid.\n\n\nMesh\nThe Mesh class is initialized with a mesh file.\n\n\nspatial.GeometryFM2D\nFlexible 2d mesh geometry.\n\n\nspatial.GeometryFM3D\nFlexible 3d mesh geometry.\n\n\nspatial.GeometryFMVerticalProfile\nFlexible mesh 2d vertical profile geometry.\n\n\nspatial.GeometryFMVerticalColumn\nA 3d geometry with consisting of a single vertical column.\n\n\nspatial._FM_geometry._GeometryFMPlotter\nPlot GeometryFM."
  },
  {
    "objectID": "api/index.html#spectral",
    "href": "api/index.html#spectral",
    "title": "API Reference",
    "section": "",
    "text": "dfsu.DfsuSpectral\nDfsu for Spectral data.\n\n\nspatial.GeometryFMPointSpectrum\nFlexible mesh point spectrum.\n\n\nspatial.GeometryFMLineSpectrum\nFlexible mesh line spectrum geometry.\n\n\nspatial.GeometryFMAreaSpectrum\nFlexible mesh area spectrum geometry."
  },
  {
    "objectID": "api/index.html#eum",
    "href": "api/index.html#eum",
    "title": "API Reference",
    "section": "",
    "text": "ItemInfo\nInfo for dynamic items (variables).\n\n\nEUMType\nEUM type.\n\n\nEUMUnit\nEUM unit."
  },
  {
    "objectID": "api/index.html#dfs",
    "href": "api/index.html#dfs",
    "title": "API Reference",
    "section": "",
    "text": "Dfs0\nClass for reading/writing dfs0 files.\n\n\nDfs1\nClass for reading/writing dfs1 files.\n\n\nDfs2\nClass for reading/writing dfs2 files.\n\n\nDfs3\nClass for reading/writing dfs3 files.\n\n\nDfsu\nFactory class for dfsu files.\n\n\ndfsu.Dfsu2DH\nClass for reading/writing dfsu 2d horizontal files.\n\n\ndfsu.Dfsu2DV\nClass for reading/writing dfsu 2d vertical files.\n\n\ndfsu.Dfsu3D\nClass for reading/writing dfsu 3d files."
  },
  {
    "objectID": "api/index.html#generic",
    "href": "api/index.html#generic",
    "title": "API Reference",
    "section": "",
    "text": "generic\nGeneric functions for working with all types of dfs files."
  },
  {
    "objectID": "api/index.html#pfs",
    "href": "api/index.html#pfs",
    "title": "API Reference",
    "section": "",
    "text": "PfsDocument\nCreate a PfsDocument object for reading, writing and manipulating pfs files.\n\n\nPfsSection\nClass for reading/writing sections in a pfs file."
  },
  {
    "objectID": "api/index.html#dataset-plotting",
    "href": "api/index.html#dataset-plotting",
    "title": "API Reference",
    "section": "",
    "text": "Plotting functions for Dataset and DataArray objects.\n\n\n\ndataset._DatasetPlotter\nClass for plotting scatter plots from datasets.\n\n\ndataset._DataArrayPlotter\nContext aware plotter (sensible plotting according to geometry).\n\n\ndataset._DataArrayPlotterGrid1D\nPlot a DataArray with a Grid1D geometry.\n\n\ndataset._DataArrayPlotterGrid2D\nPlot a DataArray with a Grid2D geometry.\n\n\ndataset._DataArrayPlotterFM\nPlot a DataArray with a GeometryFM geometry.\n\n\ndataset._DataArrayPlotterFMVerticalProfile\nPlot a DataArray with a 2DV GeometryFMVerticalProfile geometry.\n\n\ndataset._DataArrayPlotterFMVerticalColumn\nPlot a DataArray with a GeometryFMVerticalColumn geometry."
  },
  {
    "objectID": "api/dataset._DataArrayPlotterFMVerticalProfile.html",
    "href": "api/dataset._DataArrayPlotterFMVerticalProfile.html",
    "title": "dataset._DataArrayPlotterFMVerticalProfile",
    "section": "",
    "text": "dataset._DataArrayPlotterFMVerticalProfile(self, da)\nPlot a DataArray with a 2DV GeometryFMVerticalProfile geometry.\nIf DataArray has multiple time steps, the first step will be plotted."
  },
  {
    "objectID": "api/dataset._DataArrayPlotterFMVerticalProfile.html#examples",
    "href": "api/dataset._DataArrayPlotterFMVerticalProfile.html#examples",
    "title": "dataset._DataArrayPlotterFMVerticalProfile",
    "section": "Examples",
    "text": "Examples\n\nimport mikeio\nda = mikeio.read(\"../data/oresund_vertical_slice.dfsu\")[\"Temperature\"]\nda.plot()\n\n\n\n\n\n\n\n\n\nda.plot.hist()\n\n(array([  1.,   2.,  19.,  70., 202., 384., 336., 167., 127.,  15.]),\n array([15.45340729, 15.63446045, 15.81551361, 15.99656582, 16.17761993,\n        16.3586731 , 16.53972435, 16.72077751, 16.90183067, 17.08288383,\n        17.263937  ]),\n &lt;BarContainer object of 10 artists&gt;)"
  },
  {
    "objectID": "api/dataset._DataArrayPlotterFMVerticalProfile.html#methods",
    "href": "api/dataset._DataArrayPlotterFMVerticalProfile.html#methods",
    "title": "dataset._DataArrayPlotterFMVerticalProfile",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nhist\nPlot DataArray as histogram (using ax.hist).\n\n\nline\nPlot data as lines (timeseries if time is present).\n\n\n\n\nhist\ndataset._DataArrayPlotterFMVerticalProfile.hist(\n    bins=None,\n    ax=None,\n    figsize=None,\n    title=None,\n    **kwargs,\n)\nPlot DataArray as histogram (using ax.hist).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbins\n(int or sequence or str)\nIf bins is an integer, it defines the number of equal-width bins in the range. If bins is a sequence, it defines the bin edges, including the left edge of the first bin and the right edge of the last bin. by default: rcParams[“hist.bins”] (default: 10)\nNone\n\n\nax\nAxes | None\nAdding to existing axis, instead of creating new fig\nNone\n\n\nfigsize\ntuple[float, float] | None\nspecify size of figure\nNone\n\n\ntitle\nstr | None\naxes title\nNone\n\n\n**kwargs\nAny\nadditional arguments passed to the plotting function\n{}\n\n\n\n\n\nSee Also\nmatplotlib.pyplot.hist\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n&lt;matplotlib.axes&gt;\n\n\n\n\n\n\n\nline\ndataset._DataArrayPlotterFMVerticalProfile.line(ax=None, figsize=None, **kwargs)\nPlot data as lines (timeseries if time is present)."
  },
  {
    "objectID": "api/dfsu.Dfsu2DV.html",
    "href": "api/dfsu.Dfsu2DV.html",
    "title": "dfsu.Dfsu2DV",
    "section": "",
    "text": "dfsu.Dfsu2DV(self, filename)\nClass for reading/writing dfsu 2d vertical files.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nPath to dfsu file\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndeletevalue\nFile delete value.\n\n\nend_time\nFile end time.\n\n\nitems\nList of items.\n\n\nn_items\nNumber of items.\n\n\nn_layers\nMaximum number of layers.\n\n\nn_sigma_layers\nNumber of sigma layers.\n\n\nn_timesteps\nNumber of time steps.\n\n\nn_z_layers\nMaximum number of z-layers.\n\n\nstart_time\nFile start time.\n\n\ntimestep\nTime step size in seconds.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nappend\nAppend data to a dfsu file.\n\n\nread\nRead data from a dfsu file.\n\n\n\n\n\ndfsu.Dfsu2DV.append(ds, validate=True)\nAppend data to a dfsu file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nDataset to append\nrequired\n\n\nvalidate\nbool\nValidate that the dataset to append has the same geometry and items, by default True\nTrue\n\n\n\n\n\n\n\ndfsu.Dfsu2DV.read(\n    items=None,\n    time=None,\n    elements=None,\n    area=None,\n    x=None,\n    y=None,\n    z=None,\n    layers=None,\n    keepdims=False,\n    dtype=np.float32,\n    error_bad_data=True,\n    fill_bad_data_value=np.nan,\n)\nRead data from a dfsu file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\nkeepdims\nbool\nWhen reading a single time step only, should the time-dimension be kept in the returned Dataset? by default: False\nFalse\n\n\narea\ntuple[float, float, float, float] | None\nRead only data inside (horizontal) area given as a bounding box (tuple with left, lower, right, upper) or as list of coordinates for a polygon, by default None\nNone\n\n\nx\nfloat | None\nRead only data for elements containing the (x,y,z) points(s)\nNone\n\n\ny\nfloat | None\nRead only data for elements containing the (x,y,z) points(s)\nNone\n\n\nz\nfloat | None\nRead only data for elements containing the (x,y,z) points(s)\nNone\n\n\nlayers\nint | Layer | Sequence[int] | None\nRead only data for specific layers, by default None\nNone\n\n\nelements\nSequence[int] | np.ndarray | None\nRead only selected element ids, by default None\nNone\n\n\nerror_bad_data\nbool\nraise error if data is corrupt, by default True,\nTrue\n\n\nfill_bad_data_value\nfloat\nfill value for to impute corrupt data, used in conjunction with error_bad_data=False default np.nan\nnp.nan\n\n\ndtype\nAny\nData type to read, by default np.float32\nnp.float32\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nA Dataset with data dimensions [t,elements]"
  },
  {
    "objectID": "api/dfsu.Dfsu2DV.html#parameters",
    "href": "api/dfsu.Dfsu2DV.html#parameters",
    "title": "dfsu.Dfsu2DV",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nPath to dfsu file\nrequired"
  },
  {
    "objectID": "api/dfsu.Dfsu2DV.html#attributes",
    "href": "api/dfsu.Dfsu2DV.html#attributes",
    "title": "dfsu.Dfsu2DV",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndeletevalue\nFile delete value.\n\n\nend_time\nFile end time.\n\n\nitems\nList of items.\n\n\nn_items\nNumber of items.\n\n\nn_layers\nMaximum number of layers.\n\n\nn_sigma_layers\nNumber of sigma layers.\n\n\nn_timesteps\nNumber of time steps.\n\n\nn_z_layers\nMaximum number of z-layers.\n\n\nstart_time\nFile start time.\n\n\ntimestep\nTime step size in seconds."
  },
  {
    "objectID": "api/dfsu.Dfsu2DV.html#methods",
    "href": "api/dfsu.Dfsu2DV.html#methods",
    "title": "dfsu.Dfsu2DV",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nappend\nAppend data to a dfsu file.\n\n\nread\nRead data from a dfsu file.\n\n\n\n\n\ndfsu.Dfsu2DV.append(ds, validate=True)\nAppend data to a dfsu file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nds\nDataset\nDataset to append\nrequired\n\n\nvalidate\nbool\nValidate that the dataset to append has the same geometry and items, by default True\nTrue\n\n\n\n\n\n\n\ndfsu.Dfsu2DV.read(\n    items=None,\n    time=None,\n    elements=None,\n    area=None,\n    x=None,\n    y=None,\n    z=None,\n    layers=None,\n    keepdims=False,\n    dtype=np.float32,\n    error_bad_data=True,\n    fill_bad_data_value=np.nan,\n)\nRead data from a dfsu file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\nkeepdims\nbool\nWhen reading a single time step only, should the time-dimension be kept in the returned Dataset? by default: False\nFalse\n\n\narea\ntuple[float, float, float, float] | None\nRead only data inside (horizontal) area given as a bounding box (tuple with left, lower, right, upper) or as list of coordinates for a polygon, by default None\nNone\n\n\nx\nfloat | None\nRead only data for elements containing the (x,y,z) points(s)\nNone\n\n\ny\nfloat | None\nRead only data for elements containing the (x,y,z) points(s)\nNone\n\n\nz\nfloat | None\nRead only data for elements containing the (x,y,z) points(s)\nNone\n\n\nlayers\nint | Layer | Sequence[int] | None\nRead only data for specific layers, by default None\nNone\n\n\nelements\nSequence[int] | np.ndarray | None\nRead only selected element ids, by default None\nNone\n\n\nerror_bad_data\nbool\nraise error if data is corrupt, by default True,\nTrue\n\n\nfill_bad_data_value\nfloat\nfill value for to impute corrupt data, used in conjunction with error_bad_data=False default np.nan\nnp.nan\n\n\ndtype\nAny\nData type to read, by default np.float32\nnp.float32\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nA Dataset with data dimensions [t,elements]"
  },
  {
    "objectID": "api/dataset._DataArrayPlotter.html",
    "href": "api/dataset._DataArrayPlotter.html",
    "title": "dataset._DataArrayPlotter",
    "section": "",
    "text": "dataset._DataArrayPlotter(self, da)\nContext aware plotter (sensible plotting according to geometry).\n\n\n\n\n\nName\nDescription\n\n\n\n\nhist\nPlot DataArray as histogram (using ax.hist).\n\n\nline\nPlot data as lines (timeseries if time is present).\n\n\n\n\n\ndataset._DataArrayPlotter.hist(\n    bins=None,\n    ax=None,\n    figsize=None,\n    title=None,\n    **kwargs,\n)\nPlot DataArray as histogram (using ax.hist).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbins\n(int or sequence or str)\nIf bins is an integer, it defines the number of equal-width bins in the range. If bins is a sequence, it defines the bin edges, including the left edge of the first bin and the right edge of the last bin. by default: rcParams[“hist.bins”] (default: 10)\nNone\n\n\nax\nAxes | None\nAdding to existing axis, instead of creating new fig\nNone\n\n\nfigsize\ntuple[float, float] | None\nspecify size of figure\nNone\n\n\ntitle\nstr | None\naxes title\nNone\n\n\n**kwargs\nAny\nadditional arguments passed to the plotting function\n{}\n\n\n\n\n\n\nmatplotlib.pyplot.hist\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n&lt;matplotlib.axes&gt;\n\n\n\n\n\n\n\n\ndataset._DataArrayPlotter.line(ax=None, figsize=None, **kwargs)\nPlot data as lines (timeseries if time is present)."
  },
  {
    "objectID": "api/dataset._DataArrayPlotter.html#methods",
    "href": "api/dataset._DataArrayPlotter.html#methods",
    "title": "dataset._DataArrayPlotter",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nhist\nPlot DataArray as histogram (using ax.hist).\n\n\nline\nPlot data as lines (timeseries if time is present).\n\n\n\n\n\ndataset._DataArrayPlotter.hist(\n    bins=None,\n    ax=None,\n    figsize=None,\n    title=None,\n    **kwargs,\n)\nPlot DataArray as histogram (using ax.hist).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbins\n(int or sequence or str)\nIf bins is an integer, it defines the number of equal-width bins in the range. If bins is a sequence, it defines the bin edges, including the left edge of the first bin and the right edge of the last bin. by default: rcParams[“hist.bins”] (default: 10)\nNone\n\n\nax\nAxes | None\nAdding to existing axis, instead of creating new fig\nNone\n\n\nfigsize\ntuple[float, float] | None\nspecify size of figure\nNone\n\n\ntitle\nstr | None\naxes title\nNone\n\n\n**kwargs\nAny\nadditional arguments passed to the plotting function\n{}\n\n\n\n\n\n\nmatplotlib.pyplot.hist\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n&lt;matplotlib.axes&gt;\n\n\n\n\n\n\n\n\ndataset._DataArrayPlotter.line(ax=None, figsize=None, **kwargs)\nPlot data as lines (timeseries if time is present)."
  },
  {
    "objectID": "api/ItemInfo.html",
    "href": "api/ItemInfo.html",
    "title": "ItemInfo",
    "section": "",
    "text": "ItemInfo(\n    self,\n    name=None,\n    itemtype=None,\n    unit=None,\n    data_value_type='Instantaneous',\n)\nInfo for dynamic items (variables)."
  },
  {
    "objectID": "api/ItemInfo.html#parameters",
    "href": "api/ItemInfo.html#parameters",
    "title": "ItemInfo",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | EUMType | None\nUser defined name\nNone\n\n\nitemtype\nEUMType | EUMUnit | int | None\nDefault EUMType.Undefined\nNone\n\n\nunit\nEUMUnit | int | None\nDefault unit matching EUMType\nNone\n\n\ndata_value_type\nLiteral['Instantaneous', 'Accumulated', 'StepAccumulated', 'MeanStepBackward']\nOne of the following strings: ‘Instantaneous’, ‘Accumulated’, ‘StepAccumulated’, ‘MeanStepBackward’, ‘MeanStepForward’. Default: ‘Instantaneous’\n'Instantaneous'"
  },
  {
    "objectID": "api/ItemInfo.html#examples",
    "href": "api/ItemInfo.html#examples",
    "title": "ItemInfo",
    "section": "Examples",
    "text": "Examples\n\nimport mikeio\nmikeio.ItemInfo(\"Viken\", mikeio.EUMType.Water_Level)\n\nViken &lt;Water Level&gt; (meter)\n\n\n\nmikeio.ItemInfo(mikeio.EUMType.Wind_speed)\n\nWind speed &lt;Wind speed&gt; (meter per sec)"
  },
  {
    "objectID": "api/ItemInfo.html#attributes",
    "href": "api/ItemInfo.html#attributes",
    "title": "ItemInfo",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\nunit\nItem unit."
  },
  {
    "objectID": "api/ItemInfo.html#methods",
    "href": "api/ItemInfo.html#methods",
    "title": "ItemInfo",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nfrom_mikecore_dynamic_item_info\nCreate ItemInfo from a mikecore.DfsDynamicItemInfo object.\n\n\n\n\nfrom_mikecore_dynamic_item_info\nItemInfo.from_mikecore_dynamic_item_info(dfsItemInfo)\nCreate ItemInfo from a mikecore.DfsDynamicItemInfo object."
  },
  {
    "objectID": "api/read.html",
    "href": "api/read.html",
    "title": "read",
    "section": "",
    "text": "read(filename, *, items=None, time=None, keepdims=False, **kwargs)\nRead all or a subset of the data from a dfs file.\nAll dfs files can be subsetted with the items and time arguments. But the following file types also have the shown additional arguments:\n\nDfs2: area\nDfs3: layers\nDfsu-2d: (x,y), elements, area\nDfsu-layered: (xy,z), elements, area, layers\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nfull path and file name to the dfs file.\nrequired\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name, by default None (=all)\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\nkeepdims\nbool\nWhen reading a single time step only, should the time-dimension be kept in the returned Dataset? by default: False\nFalse\n\n\nx\n\nDfsu: Read only data for elements containing the (x,y) or (x,y,z) points(s), by default None\nrequired\n\n\ny\n\nDfsu: Read only data for elements containing the (x,y) or (x,y,z) points(s), by default None\nrequired\n\n\nz\n\nDfsu: Read only data for elements containing the (x,y) or (x,y,z) points(s), by default None\nrequired\n\n\narea\n\nDfs2/Dfsu: read only data within an area given by a bounding box of coordinates (left, lower, right, upper), by default None (=all)\nrequired\n\n\nlayers\n\nDfs3/Dfsu-layered: read only data from specific layers, by default None (=all layers)\nrequired\n\n\nerror_bad_data\n\nraise error if data is corrupt, by default True,\nrequired\n\n\nfill_bad_data_value\n\nfill value for to impute corrupt data, used in conjunction with error_bad_data=False default np.nan\nrequired\n\n\n**kwargs\nAny\nAdditional keyword arguments\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nA Dataset with specification according to the file type\n\n\n\n\n\n\nmikeio.open - open a Dfs file and only read the header\n\n\n\n&gt;&gt;&gt; ds = mikeio.read(\"ts.dfs0\")\n&gt;&gt;&gt; ds = mikeio.read(\"ts.dfs0\", items=0)\n&gt;&gt;&gt; ds = mikeio.read(\"ts.dfs0\", items=\"Temperature\")\n&gt;&gt;&gt; ds = mikeio.read(\"sw_points.dfs0, items=\"*Buoy 4*\")\n&gt;&gt;&gt; ds = mikeio.read(\"ts.dfs0\", items=[\"u\",\"v\"], time=\"2016\")\n&gt;&gt;&gt; ds = mikeio.read(\"tide.dfs1\", time=\"2018-5\")\n&gt;&gt;&gt; ds = mikeio.read(\"tide.dfs1\", time=slice(\"2018-5-1\",\"2018-6-1\"))\n&gt;&gt;&gt; ds = mikeio.read(\"tide.dfs1\", items=[0,3,6], time=-1)\n&gt;&gt;&gt; ds = mikeio.read(\"tide.dfs1\", time=-1, keepdims=True)\n&gt;&gt;&gt; ds = mikeio.read(\"era5.dfs2\", area=(10,50,16,58))\n&gt;&gt;&gt; ds = mikeio.read(\"HD2D.dfsu\")\n&gt;&gt;&gt; ds = mikeio.read(\"HD2D.dfsu\", x=2.2, y=54.2)\n&gt;&gt;&gt; ds = mikeio.read(\"HD2D.dfsu\", elements=183)\n&gt;&gt;&gt; ds = mikeio.read(\"HD2D.dfsu\", elements=range(0,2000))\n&gt;&gt;&gt; ds = mikeio.read(\"HD2D.dfsu\", area=(10,50,16,58))\n&gt;&gt;&gt; ds = mikeio.read(\"MT3D_sigma_z.dfsu\", x=11.4, y=56.2)\n&gt;&gt;&gt; ds = mikeio.read(\"MT3D_sigma_z.dfsu\", x=11.4, y=56.2, z=-1.1)\n&gt;&gt;&gt; ds = mikeio.read(\"MT3D_sigma_z.dfsu\", elements=lst_of_elems)\n&gt;&gt;&gt; ds = mikeio.read(\"MT3D_sigma_z.dfsu\", layers=\"bottom\")\n&gt;&gt;&gt; ds = mikeio.read(\"MT3D_sigma_z.dfsu\", layers=[-2,-1])\n&gt;&gt;&gt; ds = mikeio.read(\"HD2D.dfsu\", error_bad_data=False) # replace corrupt data with np.nan\n&gt;&gt;&gt; ds = mikeio.read(\"HD2D.dfsu\", error_bad_data=False, fill_bad_data_value=0.0) # replace corrupt data with 0.0"
  },
  {
    "objectID": "api/read.html#parameters",
    "href": "api/read.html#parameters",
    "title": "read",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nfull path and file name to the dfs file.\nrequired\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name, by default None (=all)\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\nkeepdims\nbool\nWhen reading a single time step only, should the time-dimension be kept in the returned Dataset? by default: False\nFalse\n\n\nx\n\nDfsu: Read only data for elements containing the (x,y) or (x,y,z) points(s), by default None\nrequired\n\n\ny\n\nDfsu: Read only data for elements containing the (x,y) or (x,y,z) points(s), by default None\nrequired\n\n\nz\n\nDfsu: Read only data for elements containing the (x,y) or (x,y,z) points(s), by default None\nrequired\n\n\narea\n\nDfs2/Dfsu: read only data within an area given by a bounding box of coordinates (left, lower, right, upper), by default None (=all)\nrequired\n\n\nlayers\n\nDfs3/Dfsu-layered: read only data from specific layers, by default None (=all layers)\nrequired\n\n\nerror_bad_data\n\nraise error if data is corrupt, by default True,\nrequired\n\n\nfill_bad_data_value\n\nfill value for to impute corrupt data, used in conjunction with error_bad_data=False default np.nan\nrequired\n\n\n**kwargs\nAny\nAdditional keyword arguments\n{}"
  },
  {
    "objectID": "api/read.html#returns",
    "href": "api/read.html#returns",
    "title": "read",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nDataset\nA Dataset with specification according to the file type"
  },
  {
    "objectID": "api/read.html#see-also",
    "href": "api/read.html#see-also",
    "title": "read",
    "section": "",
    "text": "mikeio.open - open a Dfs file and only read the header"
  },
  {
    "objectID": "api/read.html#examples",
    "href": "api/read.html#examples",
    "title": "read",
    "section": "",
    "text": "&gt;&gt;&gt; ds = mikeio.read(\"ts.dfs0\")\n&gt;&gt;&gt; ds = mikeio.read(\"ts.dfs0\", items=0)\n&gt;&gt;&gt; ds = mikeio.read(\"ts.dfs0\", items=\"Temperature\")\n&gt;&gt;&gt; ds = mikeio.read(\"sw_points.dfs0, items=\"*Buoy 4*\")\n&gt;&gt;&gt; ds = mikeio.read(\"ts.dfs0\", items=[\"u\",\"v\"], time=\"2016\")\n&gt;&gt;&gt; ds = mikeio.read(\"tide.dfs1\", time=\"2018-5\")\n&gt;&gt;&gt; ds = mikeio.read(\"tide.dfs1\", time=slice(\"2018-5-1\",\"2018-6-1\"))\n&gt;&gt;&gt; ds = mikeio.read(\"tide.dfs1\", items=[0,3,6], time=-1)\n&gt;&gt;&gt; ds = mikeio.read(\"tide.dfs1\", time=-1, keepdims=True)\n&gt;&gt;&gt; ds = mikeio.read(\"era5.dfs2\", area=(10,50,16,58))\n&gt;&gt;&gt; ds = mikeio.read(\"HD2D.dfsu\")\n&gt;&gt;&gt; ds = mikeio.read(\"HD2D.dfsu\", x=2.2, y=54.2)\n&gt;&gt;&gt; ds = mikeio.read(\"HD2D.dfsu\", elements=183)\n&gt;&gt;&gt; ds = mikeio.read(\"HD2D.dfsu\", elements=range(0,2000))\n&gt;&gt;&gt; ds = mikeio.read(\"HD2D.dfsu\", area=(10,50,16,58))\n&gt;&gt;&gt; ds = mikeio.read(\"MT3D_sigma_z.dfsu\", x=11.4, y=56.2)\n&gt;&gt;&gt; ds = mikeio.read(\"MT3D_sigma_z.dfsu\", x=11.4, y=56.2, z=-1.1)\n&gt;&gt;&gt; ds = mikeio.read(\"MT3D_sigma_z.dfsu\", elements=lst_of_elems)\n&gt;&gt;&gt; ds = mikeio.read(\"MT3D_sigma_z.dfsu\", layers=\"bottom\")\n&gt;&gt;&gt; ds = mikeio.read(\"MT3D_sigma_z.dfsu\", layers=[-2,-1])\n&gt;&gt;&gt; ds = mikeio.read(\"HD2D.dfsu\", error_bad_data=False) # replace corrupt data with np.nan\n&gt;&gt;&gt; ds = mikeio.read(\"HD2D.dfsu\", error_bad_data=False, fill_bad_data_value=0.0) # replace corrupt data with 0.0"
  },
  {
    "objectID": "api/Dfs0.html",
    "href": "api/Dfs0.html",
    "title": "Dfs0",
    "section": "",
    "text": "Dfs0(self, filename)\nClass for reading/writing dfs0 files.\n\n\n\n\n\nName\nDescription\n\n\n\n\nitems\nList of items.\n\n\nn_items\nNumber of items.\n\n\nn_timesteps\nNumber of time steps.\n\n\nstart_time\nFile start time.\n\n\ntime\nFile all datetimes.\n\n\ntimestep\nTime step size in seconds.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nread\nRead data from a dfs0 file.\n\n\nto_dataframe\nRead data from the dfs0 file and return a Pandas DataFrame.\n\n\n\n\n\nDfs0.read(items=None, time=None, **kwargs)\nRead data from a dfs0 file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\n**kwargs\nAny\nAdditional keyword arguments are ignored\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nA Dataset with data dimensions [t]\n\n\n\n\n\n\n\nDfs0.to_dataframe(unit_in_name=False)\nRead data from the dfs0 file and return a Pandas DataFrame.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nunit_in_name\nbool\ninclude unit in column name, default False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame"
  },
  {
    "objectID": "api/Dfs0.html#attributes",
    "href": "api/Dfs0.html#attributes",
    "title": "Dfs0",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nitems\nList of items.\n\n\nn_items\nNumber of items.\n\n\nn_timesteps\nNumber of time steps.\n\n\nstart_time\nFile start time.\n\n\ntime\nFile all datetimes.\n\n\ntimestep\nTime step size in seconds."
  },
  {
    "objectID": "api/Dfs0.html#methods",
    "href": "api/Dfs0.html#methods",
    "title": "Dfs0",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nread\nRead data from a dfs0 file.\n\n\nto_dataframe\nRead data from the dfs0 file and return a Pandas DataFrame.\n\n\n\n\n\nDfs0.read(items=None, time=None, **kwargs)\nRead data from a dfs0 file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nitems\nstr | int | Sequence[str | int] | None\nRead only selected items, by number (0-based), or by name\nNone\n\n\ntime\nint | str | slice | Sequence[int] | None\nRead only selected time steps, by default None (=all)\nNone\n\n\n**kwargs\nAny\nAdditional keyword arguments are ignored\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nA Dataset with data dimensions [t]\n\n\n\n\n\n\n\nDfs0.to_dataframe(unit_in_name=False)\nRead data from the dfs0 file and return a Pandas DataFrame.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nunit_in_name\nbool\ninclude unit in column name, default False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame"
  },
  {
    "objectID": "api/Grid2D.html",
    "href": "api/Grid2D.html",
    "title": "Grid2D",
    "section": "",
    "text": "Grid2D(\n    self,\n    *,\n    x=None,\n    x0=0.0,\n    dx=None,\n    nx=None,\n    y=None,\n    y0=0.0,\n    dy=None,\n    ny=None,\n    bbox=None,\n    projection='LONG/LAT',\n    origin=None,\n    orientation=0.0,\n    axis_names=('x', 'y'),\n    is_spectral=False,\n    is_vertical=False,\n)\n2D grid.\nOrigin in the center of cell in lower-left corner x and y axes are increasing and equidistant"
  },
  {
    "objectID": "api/Grid2D.html#parameters",
    "href": "api/Grid2D.html#parameters",
    "title": "Grid2D",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\narray_like\nx coordinates of cell centers\nNone\n\n\nx0\nfloat\nx coordinate of lower-left corner of first cell\n0.0\n\n\ndx\nfloat\nx cell size\nNone\n\n\nnx\nint\nnumber of cells in x direction\nNone\n\n\ny\narray_like\ny coordinates of cell centers\nNone\n\n\ny0\nfloat\ny coordinate of lower-left corner of first cell\n0.0\n\n\ndy\nfloat\ny cell size\nNone\n\n\nny\nint\nnumber of cells in y direction\nNone\n\n\nbbox\ntuple\n(x0, y0, x1, y1) of bounding box\nNone\n\n\nprojection\nstr\nprojection string, by default “NON-UTM”\n'LONG/LAT'\n\n\norigin\ntuple\nuser-defined origin, by default None\nNone\n\n\norientation\nfloat\nrotation angle in degrees, by default 0.0\n0.0\n\n\naxis_names\ntuple\nnames of x and y axes, by default (“x”, “y”)\n('x', 'y')\n\n\nis_spectral\nbool\nif True, the grid is spectral, by default False\nFalse\n\n\nis_vertical\nbool\nif True, the grid is vertical, by default False\nFalse"
  },
  {
    "objectID": "api/Grid2D.html#examples",
    "href": "api/Grid2D.html#examples",
    "title": "Grid2D",
    "section": "Examples",
    "text": "Examples\n\nimport mikeio\nmikeio.Grid2D(x0=12.0, nx=2, dx=0.25, y0=55.0, ny=3, dy=0.25, projection=\"LONG/LAT\")\n\n&lt;mikeio.Grid2D&gt;\nx: [12, 12.25] (nx=2, dx=0.25)\ny: [55, 55.25, 55.5] (ny=3, dy=0.25)\nprojection: LONG/LAT"
  },
  {
    "objectID": "api/Grid2D.html#attributes",
    "href": "api/Grid2D.html#attributes",
    "title": "Grid2D",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\nbbox\nBounding box (left, bottom, right, top).\n\n\ndx\nX grid spacing.\n\n\ndy\nY grid spacing.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nnx\nNumber of x grid points.\n\n\nny\nNumber of y grid points.\n\n\norientation\nGrid orientation.\n\n\norigin\nCoordinates of grid origo (in projection).\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string.\n\n\nx\nArray of x coordinates (element center).\n\n\nxy\nN-by-2 array of x- and y-coordinates.\n\n\ny\nArray of y coordinates (element center)."
  },
  {
    "objectID": "api/Grid2D.html#methods",
    "href": "api/Grid2D.html#methods",
    "title": "Grid2D",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\ncontains\nTest if a list of points are inside grid.\n\n\nfind_index\nFind nearest index (i,j) of point(s).\n\n\nget_node_coordinates\nNode coordinates for this grid.\n\n\nisel\nReturn a new geometry as a subset of Grid2D along the given axis.\n\n\nto_geometryFM\nConvert Grid2D to GeometryFM2D.\n\n\nto_mesh\nExport grid to mesh file.\n\n\n\n\ncontains\nGrid2D.contains(coords)\nTest if a list of points are inside grid.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncoords\narray(float)\nxy-coordinate of points given as n-by-2 array\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool array\nTrue for points inside, False otherwise\n\n\n\n\n\n\nfind_index\nGrid2D.find_index(x=None, y=None, coords=None, area=None)\nFind nearest index (i,j) of point(s).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nfloat\nx-coordinate of point\nNone\n\n\ny\nfloat\ny-coordinate of point\nNone\n\n\ncoords\narray(float)\nxy-coordinate of points given as n-by-2 array\nNone\n\n\narea\narray(float)\nxy-coordinates of bounding box\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n(array(int), array(int))\ni- and j-index of nearest cell\n\n\n\n\n\n\nget_node_coordinates\nGrid2D.get_node_coordinates()\nNode coordinates for this grid.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\narray(float)\n2d array with x,y-coordinates, length=(nx+1)*(ny+1)\n\n\n\n\n\n\nisel\nGrid2D.isel(idx, axis)\nReturn a new geometry as a subset of Grid2D along the given axis.\n\n\nto_geometryFM\nGrid2D.to_geometryFM(z=None, west=2, east=4, north=5, south=3)\nConvert Grid2D to GeometryFM2D.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nz\nfloat\nbathymetry values for each node, by default 0\nNone\n\n\nwest\nint\ncode value for west boundary\n2\n\n\neast\nint\ncode value for east boundary\n4\n\n\nnorth\nint\ncode value for north boundary\n5\n\n\nsouth\nint\ncode value for south boundary\n3\n\n\n\n\n\n\nto_mesh\nGrid2D.to_mesh(outfilename, z=None)\nExport grid to mesh file.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noutfilename\nstr\npath of new mesh file\nrequired\n\n\nz\nfloat or array(float)\nbathymetry values for each node, by default 0 if array: must have length=(nx+1)*(ny+1)\nNone"
  },
  {
    "objectID": "api/Grid3D.html",
    "href": "api/Grid3D.html",
    "title": "Grid3D",
    "section": "",
    "text": "Grid3D(\n    self,\n    *,\n    x=None,\n    x0=0.0,\n    dx=None,\n    nx=None,\n    y=None,\n    y0=0.0,\n    dy=None,\n    ny=None,\n    z=None,\n    z0=0.0,\n    dz=None,\n    nz=None,\n    projection='NON-UTM',\n    origin=(0.0, 0.0),\n    orientation=0.0,\n)\n3D grid.\nOrigin in the center of cell in lower-left corner x, y and z axes are increasing and equidistant\n\n\n\n\n\nName\nDescription\n\n\n\n\ndx\nX-axis grid spacing.\n\n\ndy\nY-axis grid spacing.\n\n\ndz\nZ-axis grid spacing.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nnx\nNumber of x grid points.\n\n\nny\nNumber of y grid points.\n\n\nnz\nNumber of z grid points.\n\n\norientation\nGrid orientation.\n\n\norigin\nCoordinates of grid origo (in projection).\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string.\n\n\nx\nArray of x-axis coordinates (element center).\n\n\ny\nArray of y-axis coordinates (element center).\n\n\nz\nArray of z-axis node coordinates.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nisel\nGet a subset geometry from this geometry.\n\n\n\n\n\nGrid3D.isel(idx, axis)\nGet a subset geometry from this geometry."
  },
  {
    "objectID": "api/Grid3D.html#attributes",
    "href": "api/Grid3D.html#attributes",
    "title": "Grid3D",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndx\nX-axis grid spacing.\n\n\ndy\nY-axis grid spacing.\n\n\ndz\nZ-axis grid spacing.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nnx\nNumber of x grid points.\n\n\nny\nNumber of y grid points.\n\n\nnz\nNumber of z grid points.\n\n\norientation\nGrid orientation.\n\n\norigin\nCoordinates of grid origo (in projection).\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string.\n\n\nx\nArray of x-axis coordinates (element center).\n\n\ny\nArray of y-axis coordinates (element center).\n\n\nz\nArray of z-axis node coordinates."
  },
  {
    "objectID": "api/Grid3D.html#methods",
    "href": "api/Grid3D.html#methods",
    "title": "Grid3D",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nisel\nGet a subset geometry from this geometry.\n\n\n\n\n\nGrid3D.isel(idx, axis)\nGet a subset geometry from this geometry."
  },
  {
    "objectID": "api/spatial.GeometryFMVerticalColumn.html",
    "href": "api/spatial.GeometryFMVerticalColumn.html",
    "title": "spatial.GeometryFMVerticalColumn",
    "section": "",
    "text": "spatial.GeometryFMVerticalColumn(\n    self,\n    *,\n    node_coordinates,\n    element_table,\n    codes=None,\n    projection='LONG/LAT',\n    dfsu_type=DfsuFileType.Dfsu3DSigma,\n    element_ids=None,\n    node_ids=None,\n    n_layers=1,\n    n_sigma=None,\n    validate=True,\n    reindex=False,\n)\nA 3d geometry with consisting of a single vertical column.\n\n\n\n\n\nName\nDescription\n\n\n\n\nbottom_elements\nList of 3d element ids of bottom layer.\n\n\nboundary_codes\nUnique list of boundary codes.\n\n\ncodes\nNode codes of all nodes (0=water, 1=land, 2…=open boundaries).\n\n\ne2_e3_table\nThe 2d-to-3d element connectivity table for a 3d object.\n\n\nelem2d_ids\nThe associated 2d element id for each 3d element.\n\n\nelement_coordinates\nCenter coordinates of each element.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nlayer_ids\nThe layer number (0=bottom, 1, 2, …) for each 3d element.\n\n\nmax_nodes_per_element\nThe maximum number of nodes for an element.\n\n\nn_elements\nNumber of 3d elements.\n\n\nn_layers\nMaximum number of layers.\n\n\nn_layers_per_column\nList of number of layers for each column.\n\n\nn_sigma_layers\nNumber of sigma layers.\n\n\nn_z_layers\nMaximum number of z-layers.\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string.\n\n\ntop_elements\nList of 3d element ids of surface layer.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_layer_elements\n3d element ids for one (or more) specific layer(s).\n\n\nto_2d_geometry\nextract 2d geometry from 3d geometry.\n\n\n\n\n\nspatial.GeometryFMVerticalColumn.get_layer_elements(layers)\n3d element ids for one (or more) specific layer(s).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlayers\nint or list(int)\nlayer between 0 (bottom) and n_layers-1 (top) (can also be negative counting from -1 at the top layer)\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array(int)\nelement ids\n\n\n\n\n\n\n\nspatial.GeometryFMVerticalColumn.to_2d_geometry()\nextract 2d geometry from 3d geometry.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nGeometryFM2D\n2d geometry"
  },
  {
    "objectID": "api/spatial.GeometryFMVerticalColumn.html#attributes",
    "href": "api/spatial.GeometryFMVerticalColumn.html#attributes",
    "title": "spatial.GeometryFMVerticalColumn",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nbottom_elements\nList of 3d element ids of bottom layer.\n\n\nboundary_codes\nUnique list of boundary codes.\n\n\ncodes\nNode codes of all nodes (0=water, 1=land, 2…=open boundaries).\n\n\ne2_e3_table\nThe 2d-to-3d element connectivity table for a 3d object.\n\n\nelem2d_ids\nThe associated 2d element id for each 3d element.\n\n\nelement_coordinates\nCenter coordinates of each element.\n\n\nis_geo\nAre coordinates geographical (LONG/LAT)?\n\n\nis_local_coordinates\nAre coordinates relative (NON-UTM)?\n\n\nlayer_ids\nThe layer number (0=bottom, 1, 2, …) for each 3d element.\n\n\nmax_nodes_per_element\nThe maximum number of nodes for an element.\n\n\nn_elements\nNumber of 3d elements.\n\n\nn_layers\nMaximum number of layers.\n\n\nn_layers_per_column\nList of number of layers for each column.\n\n\nn_sigma_layers\nNumber of sigma layers.\n\n\nn_z_layers\nMaximum number of z-layers.\n\n\nprojection\nThe projection.\n\n\nprojection_string\nThe projection string.\n\n\ntop_elements\nList of 3d element ids of surface layer."
  },
  {
    "objectID": "api/spatial.GeometryFMVerticalColumn.html#methods",
    "href": "api/spatial.GeometryFMVerticalColumn.html#methods",
    "title": "spatial.GeometryFMVerticalColumn",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_layer_elements\n3d element ids for one (or more) specific layer(s).\n\n\nto_2d_geometry\nextract 2d geometry from 3d geometry.\n\n\n\n\n\nspatial.GeometryFMVerticalColumn.get_layer_elements(layers)\n3d element ids for one (or more) specific layer(s).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlayers\nint or list(int)\nlayer between 0 (bottom) and n_layers-1 (top) (can also be negative counting from -1 at the top layer)\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array(int)\nelement ids\n\n\n\n\n\n\n\nspatial.GeometryFMVerticalColumn.to_2d_geometry()\nextract 2d geometry from 3d geometry.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nGeometryFM2D\n2d geometry"
  },
  {
    "objectID": "api/read_pfs.html",
    "href": "api/read_pfs.html",
    "title": "read_pfs",
    "section": "",
    "text": "read_pfs(filename, encoding='cp1252', unique_keywords=False)\nRead a pfs file to a Pfs object for further analysis/manipulation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nFile name including full path to the pfs file.\nrequired\n\n\nencoding\nstr\nHow is the pfs file encoded? By default ‘cp1252’\n'cp1252'\n\n\nunique_keywords\nbool\nShould the keywords in a section be unique? Some tools e.g. the MIKE Plot Composer allows non-unique keywords. If True: warnings will be issued if non-unique keywords are present and the first occurence will be used by default False\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nPfsDocument\nA PfsDocument object"
  },
  {
    "objectID": "api/read_pfs.html#parameters",
    "href": "api/read_pfs.html#parameters",
    "title": "read_pfs",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nFile name including full path to the pfs file.\nrequired\n\n\nencoding\nstr\nHow is the pfs file encoded? By default ‘cp1252’\n'cp1252'\n\n\nunique_keywords\nbool\nShould the keywords in a section be unique? Some tools e.g. the MIKE Plot Composer allows non-unique keywords. If True: warnings will be issued if non-unique keywords are present and the first occurence will be used by default False\nFalse"
  },
  {
    "objectID": "api/read_pfs.html#returns",
    "href": "api/read_pfs.html#returns",
    "title": "read_pfs",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nPfsDocument\nA PfsDocument object"
  },
  {
    "objectID": "api/spatial._FM_geometry._GeometryFMPlotter.html",
    "href": "api/spatial._FM_geometry._GeometryFMPlotter.html",
    "title": "spatial._FM_geometry._GeometryFMPlotter",
    "section": "",
    "text": "spatial._FM_geometry._GeometryFMPlotter(self, geometry)\nPlot GeometryFM."
  },
  {
    "objectID": "api/spatial._FM_geometry._GeometryFMPlotter.html#examples",
    "href": "api/spatial._FM_geometry._GeometryFMPlotter.html#examples",
    "title": "spatial._FM_geometry._GeometryFMPlotter",
    "section": "Examples",
    "text": "Examples\n\nimport mikeio\n\nds = mikeio.read(\"../data/FakeLake.dfsu\")\ng = ds.geometry\ng.plot()"
  },
  {
    "objectID": "api/spatial._FM_geometry._GeometryFMPlotter.html#methods",
    "href": "api/spatial._FM_geometry._GeometryFMPlotter.html#methods",
    "title": "spatial._FM_geometry._GeometryFMPlotter",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nboundary_nodes\nPlot mesh boundary nodes and their code values.\n\n\ncontour\nPlot bathymetry as contour lines.\n\n\ncontourf\nPlot bathymetry as filled contours.\n\n\nmesh\nPlot mesh only.\n\n\noutline\nPlot domain outline.\n\n\n\n\nboundary_nodes\nspatial._FM_geometry._GeometryFMPlotter.boundary_nodes(\n    boundary_names=None,\n    figsize=None,\n    ax=None,\n)\nPlot mesh boundary nodes and their code values.\n\nExamples\n\ng.plot.boundary_nodes()\n\n\n\n\n\n\n\n\n\n\n\ncontour\nspatial._FM_geometry._GeometryFMPlotter.contour(ax=None, figsize=None, **kwargs)\nPlot bathymetry as contour lines.\n\nExamples\n\ng.plot.contour()\n\n\n\n\n\n\n\n\n\n\n\ncontourf\nspatial._FM_geometry._GeometryFMPlotter.contourf(\n    ax=None,\n    figsize=None,\n    **kwargs,\n)\nPlot bathymetry as filled contours.\n\nExamples\n\ng.plot.contourf()\n\n\n\n\n\n\n\n\n\n\n\nmesh\nspatial._FM_geometry._GeometryFMPlotter.mesh(\n    title='Mesh',\n    figsize=None,\n    ax=None,\n)\nPlot mesh only.\n\nExamples\n\ng.plot.mesh()\n\n\n\n\n\n\n\n\n\n\n\noutline\nspatial._FM_geometry._GeometryFMPlotter.outline(\n    title='Outline',\n    figsize=None,\n    ax=None,\n)\nPlot domain outline.\n\nExamples\n\ng.plot.outline()"
  },
  {
    "objectID": "api/Dfsu.html",
    "href": "api/Dfsu.html",
    "title": "Dfsu",
    "section": "",
    "text": "Dfsu\nDfsu()\nFactory class for dfsu files."
  },
  {
    "objectID": "api/Dataset.html",
    "href": "api/Dataset.html",
    "title": "Dataset",
    "section": "",
    "text": "Dataset(self, data, validate=True)\nDataset containing one or more DataArrays with common geometry and time.\nMost often obtained by reading a dfs file. But can also be created a sequence or dictonary of DataArrays. The mikeio.Dataset is inspired by and similar to the xarray.Dataset.\nThe Dataset is primarily a container for one or more DataArrays all having the same time and geometry (and shape, dims, etc)."
  },
  {
    "objectID": "api/Dataset.html#parameters",
    "href": "api/Dataset.html#parameters",
    "title": "Dataset",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nMapping[str, DataArray] | Sequence[DataArray]\nDataArray, list of DataArrays or dict of DataArrays\nrequired\n\n\nvalidate\nbool\nOptional validation of consistency of data arrays.\nTrue"
  },
  {
    "objectID": "api/Dataset.html#notes",
    "href": "api/Dataset.html#notes",
    "title": "Dataset",
    "section": "Notes",
    "text": "Notes\nSelecting a specific item “itemA” (at position 0) from a Dataset ds can be done with:\n\nds[[“itemA”]] - returns a new Dataset with “itemA”\nds[“itemA”] - returns the “itemA” DataArray\nds[[0]] - returns a new Dataset with “itemA”\nds[0] - returns the “itemA” DataArray\nds.itemA - returns the “itemA” DataArray"
  },
  {
    "objectID": "api/Dataset.html#examples",
    "href": "api/Dataset.html#examples",
    "title": "Dataset",
    "section": "Examples",
    "text": "Examples\n\nimport mikeio\nmikeio.read(\"../data/europe_wind_long_lat.dfs2\")\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1, y:101, x:221)\ntime: 2012-01-01 00:00:00 (time-invariant)\ngeometry: Grid2D (ny=101, nx=221)\nitems:\n  0:  Mean Sea Level Pressure &lt;Air Pressure&gt; (hectopascal)\n  1:  Wind x-comp (10m) &lt;Wind Velocity&gt; (meter per sec)\n  2:  Wind y-comp (10m) &lt;Wind Velocity&gt; (meter per sec)"
  },
  {
    "objectID": "api/Dataset.html#attributes",
    "href": "api/Dataset.html#attributes",
    "title": "Dataset",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\ndeletevalue\nFile delete value.\n\n\ndims\nNamed array dimensions of each DataArray.\n\n\nend_time\nLast time instance (as datetime).\n\n\ngeometry\nGeometry of each DataArray.\n\n\nis_equidistant\nIs Dataset equidistant in time?\n\n\nitems\nItemInfo for each of the DataArrays as a list.\n\n\nn_items\nNumber of items/DataArrays, equivalent to len().\n\n\nn_timesteps\nNumber of time steps.\n\n\nnames\nName of each of the DataArrays as a list.\n\n\nndim\nNumber of array dimensions of each DataArray.\n\n\nshape\nShape of each DataArray.\n\n\nstart_time\nFirst time instance (as datetime).\n\n\ntime\nTime axis.\n\n\ntimestep\nTime step in seconds if equidistant (and at"
  },
  {
    "objectID": "api/Dataset.html#methods",
    "href": "api/Dataset.html#methods",
    "title": "Dataset",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\naggregate\nAggregate along an axis.\n\n\naverage\nCompute the weighted average along the specified axis.\n\n\nconcat\nConcatenate Datasets along the time axis.\n\n\ncopy\nReturns a copy of this dataset.\n\n\ncreate_data_array\nCreate a new DataArray with the same time and geometry as the dataset.\n\n\ndescribe\nGenerate descriptive statistics.\n\n\ndropna\nRemove time steps where all items are NaN.\n\n\nextract_track\nExtract data along a moving track.\n\n\nfillna\nFill NA/NaN value.\n\n\nflipud\nFlip data upside down (on first non-time axis).\n\n\nfrom_numpy\nCreate a Dataset from numpy arrays.\n\n\ninsert\nInsert DataArray in a specific position.\n\n\ninterp\nInterpolate data in time and space.\n\n\ninterp_like\nInterpolate in space (and in time) to other geometry (and time axis).\n\n\ninterp_time\nTemporal interpolation.\n\n\nisel\nReturn a new Dataset whose data is given by\n\n\nmax\nMax value along an axis.\n\n\nmean\nMean value along an axis.\n\n\nmerge\nMerge Datasets along the item dimension.\n\n\nmin\nMin value along an axis.\n\n\nnanmax\nMax value along an axis (NaN removed).\n\n\nnanmean\nMean value along an axis (NaN removed).\n\n\nnanmin\nMin value along an axis (NaN removed).\n\n\nnanquantile\nCompute the q-th quantile of the data along the specified axis, while ignoring nan values.\n\n\nnanstd\nStandard deviation along an axis (NaN removed).\n\n\nptp\nRange (max - min) a.k.a Peak to Peak along an axis\n\n\nquantile\nCompute the q-th quantile of the data along the specified axis.\n\n\nremove\nRemove DataArray from Dataset.\n\n\nrename\nRename items (DataArrays) in Dataset.\n\n\nsel\nReturn a new Dataset whose data is given by\n\n\nsqueeze\nRemove axes of length 1.\n\n\nstd\nStandard deviation along an axis.\n\n\nto_dataframe\nConvert Dataset to a Pandas DataFrame.\n\n\nto_dfs\nWrite dataset to a new dfs file.\n\n\nto_numpy\nStack data to a single ndarray with shape (n_items, n_timesteps, …).\n\n\nto_pandas\nConvert Dataset to a Pandas DataFrame.\n\n\nto_xarray\nExport to xarray.Dataset.\n\n\n\n\naggregate\nDataset.aggregate(axis=0, func=np.nanmean, **kwargs)\nAggregate along an axis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str | None\naxis number or “time”, “space” or “items”, by default 0\n0\n\n\nfunc\nCallable\ndefault np.nanmean\nnp.nanmean\n\n\n**kwargs\nAny\nadditional arguments passed to the function\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ndataset with aggregated values\n\n\n\n\n\n\naverage\nDataset.average(weights, axis=0, **kwargs)\nCompute the weighted average along the specified axis.\nWraps numpy.average\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nweights\n\nweights to average over\nrequired\n\n\naxis\n\naxis number or “time”, “space” or “items”, by default 0\n0\n\n\n**kwargs\n\nadditional arguments passed to the function\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ndataset with weighted average values\n\n\n\n\n\nSee Also\nnanmean : Mean values with NaN values removed\naggregate : Weighted average\n\n\nExamples\n&gt;&gt;&gt; dfs = Dfsu(\"HD2D.dfsu\")\n&gt;&gt;&gt; ds = dfs.read([\"Current speed\"])\n&gt;&gt;&gt; area = dfs.get_element_area()\n&gt;&gt;&gt; ds2 = ds.average(axis=\"space\", weights=area)\n\n\n\nconcat\nDataset.concat(datasets, keep='last')\nConcatenate Datasets along the time axis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndatasets\nSequence[Dataset]\nlist of Datasets to concatenate\nrequired\n\n\nkeep\nLiteral['last', 'first']\nwhich values to keep in case of overlap, by default ‘last’\n'last'\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nconcatenated dataset\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import mikeio\n&gt;&gt;&gt; ds1 = mikeio.read(\"HD2D.dfsu\", time=[0,1])\n&gt;&gt;&gt; ds2 = mikeio.read(\"HD2D.dfsu\", time=[2,3])\n&gt;&gt;&gt; ds1.n_timesteps\n2\n&gt;&gt;&gt; ds3 = Dataset.concat([ds1,ds2])\n&gt;&gt;&gt; ds3.n_timesteps\n4\n\n\n\ncopy\nDataset.copy()\nReturns a copy of this dataset.\n\n\ncreate_data_array\nDataset.create_data_array(data, item=None, name=None)\nCreate a new DataArray with the same time and geometry as the dataset.\n\n\ndescribe\nDataset.describe(**kwargs)\nGenerate descriptive statistics.\nWraps pandas.DataFrame.describe.\n\n\ndropna\nDataset.dropna()\nRemove time steps where all items are NaN.\n\n\nextract_track\nDataset.extract_track(track, method='nearest', dtype=np.float32)\nExtract data along a moving track.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrack\nstr | Path | Dataset | pd.DataFrame\nwith DatetimeIndex and (x, y) of track points as first two columns x,y coordinates must be in same coordinate system as dataset\nrequired\n\n\nmethod\nLiteral['nearest', 'inverse_distance']\nSpatial interpolation method (‘nearest’ or ‘inverse_distance’) default=‘nearest’\n'nearest'\n\n\ndtype\nAny\nData type of the returned data, default=np.float32\nnp.float32\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nA dataset with data dimension t The first two items will be x- and y- coordinates of track\n\n\n\n\n\n\nfillna\nDataset.fillna(value=0.0)\nFill NA/NaN value.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nfloat\nValue used to fill missing values. Default is 0.0.\n0.0\n\n\n\n\n\n\nflipud\nDataset.flipud()\nFlip data upside down (on first non-time axis).\n\n\nfrom_numpy\nDataset.from_numpy(\n    data,\n    time=None,\n    items=None,\n    *,\n    geometry=None,\n    zn=None,\n    dims=None,\n    validate=True,\n    dt=1.0,\n)\nCreate a Dataset from numpy arrays.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nSequence[NDArray[np.floating]]\nData to be converted to DataArrays\nrequired\n\n\ntime\npd.DatetimeIndex | None\nTime axis, by default None\nNone\n\n\nitems\nSequence[ItemInfo] | Sequence[str] | None\nItemInfo for each DataArray, by default None\nNone\n\n\ngeometry\nAny | None\nGeometry of the DataArrays, by default None\nNone\n\n\nzn\nNDArray[np.floating] | None\nZ-coordinates of the DataArrays, by default None\nNone\n\n\ndims\ntuple[str, …] | None\nNamed dimensions of the DataArrays, by default None\nNone\n\n\nvalidate\nbool\nValidate the DataArrays, by default True\nTrue\n\n\ndt\nfloat\nDummy time step in seconds, by default 1.0\n1.0\n\n\n\n\n\n\ninsert\nDataset.insert(key, value)\nInsert DataArray in a specific position.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nint\nindex in Dataset where DataArray should be inserted\nrequired\n\n\nvalue\nDataArray\nDataArray to be inserted, must comform with with existing DataArrays and must have a unique item name\nrequired\n\n\n\n\n\n\ninterp\nDataset.interp(time=None, x=None, y=None, z=None, **kwargs)\nInterpolate data in time and space.\nThis method currently has limited functionality for spatial interpolation. It will be extended in the future.\nThe spatial parameters available depend on the geometry of the Dataset:\n\nGrid1D: x\nGrid2D: x, y\nGrid3D: [not yet implemented!]\nGeometryFM: (x,y)\nGeometryFMLayered: (x,y) [surface point will be returned!]\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntime\n(float, pd.DatetimeIndex or Dataset)\ntimestep in seconds or discrete time instances given by pd.DatetimeIndex (typically from another Dataset da2.time), by default None (=don’t interp in time)\nNone\n\n\nx\nfloat\nx-coordinate of point to be interpolated to, by default None\nNone\n\n\ny\nfloat\ny-coordinate of point to be interpolated to, by default None\nNone\n\n\nz\nfloat\nz-coordinate of point to be interpolated to, by default None\nNone\n\n\nn_nearest\nint\nWhen using IDW interpolation, how many nearest points should be used, by default: 3\nrequired\n\n\n**kwargs\nAny\nAdditional keyword arguments are passed to the interpolant\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nnew Dataset with interped data\n\n\n\n\n\nSee Also\nsel : Select data using label indexing interp_like : Interp to another time/space of another DataSet interp_time : Interp in the time direction only\n\n\nExamples\n&gt;&gt;&gt; ds = mikeio.read(\"random.dfs1\")\n&gt;&gt;&gt; ds.interp(time=3600)\n&gt;&gt;&gt; ds.interp(x=110)\n&gt;&gt;&gt; ds = mikeio.read(\"HD2D.dfsu\")\n&gt;&gt;&gt; ds.interp(x=340000, y=6160000)\n\n\n\ninterp_like\nDataset.interp_like(other, **kwargs)\nInterpolate in space (and in time) to other geometry (and time axis).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nDataset | DataArray | Grid2D | GeometryFM2D | pd.DatetimeIndex\nDataset, DataArray, Grid2D or GeometryFM2D to interpolate to\nrequired\n\n\n**kwargs\nAny\nadditional kwargs are passed to interpolation method\n{}\n\n\n\n\n\nNotes\nCurrently only supports interpolation from dfsu-2d to dfs2 or other dfsu-2d Datasets\n\n\nExamples\n&gt;&gt;&gt; ds = mikeio.read(\"HD.dfsu\")\n&gt;&gt;&gt; ds2 = mikeio.read(\"wind.dfs2\")\n&gt;&gt;&gt; dsi = ds.interp_like(ds2)\n&gt;&gt;&gt; dsi.to_dfs(\"HD_gridded.dfs2\")\n&gt;&gt;&gt; dse = ds.interp_like(ds2, extrapolate=True)\n&gt;&gt;&gt; dst = ds.interp_like(ds2.time)\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nInterpolated Dataset\n\n\n\n\n\n\ninterp_time\nDataset.interp_time(\n    dt=None,\n    *,\n    freq=None,\n    method='linear',\n    extrapolate=True,\n    fill_value=np.nan,\n)\nTemporal interpolation.\nWrapper of scipy.interpolate.interp1d.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndt\nfloat | int | pd.DatetimeIndex | Dataset | DataArray | None\noutput timestep in seconds or discrete time instances given as a pd.DatetimeIndex (typically from another Dataset ds2.time)\nNone\n\n\nfreq\nstr | None\npandas frequency\nNone\n\n\nmethod\nstr\nSpecifies the kind of interpolation as a string (‘linear’, ‘nearest’, ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’, ‘previous’, ‘next’, where ‘zero’, ‘slinear’, ‘quadratic’ and ‘cubic’ refer to a spline interpolation of zeroth, first, second or third order; ‘previous’ and ‘next’ simply return the previous or next value of the point) or as an integer specifying the order of the spline interpolator to use. Default is ‘linear’.\n'linear'\n\n\nextrapolate\nbool\nDefault True. If False, a ValueError is raised any time interpolation is attempted on a value outside of the range of x (where extrapolation is necessary). If True, out of bounds values are assigned fill_value\nTrue\n\n\nfill_value\nfloat\nDefault NaN. this value will be used to fill in for points outside of the time range.\nnp.nan\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\n\n\n\n\n\n\nExamples\n\nds = mikeio.read(\"../data/HD2D.dfsu\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:9, element:884)\ntime: 1985-08-06 07:00:00 - 1985-08-07 03:00:00 (9 records)\ngeometry: Dfsu2D (884 elements, 529 nodes)\nitems:\n  0:  Surface elevation &lt;Surface Elevation&gt; (meter)\n  1:  U velocity &lt;u velocity component&gt; (meter per sec)\n  2:  V velocity &lt;v velocity component&gt; (meter per sec)\n  3:  Current speed &lt;Current Speed&gt; (meter per sec)\n\n\n\nds.interp_time(dt=1800)\n\n&lt;mikeio.Dataset&gt;\ndims: (time:41, element:884)\ntime: 1985-08-06 07:00:00 - 1985-08-07 03:00:00 (41 records)\ngeometry: Dfsu2D (884 elements, 529 nodes)\nitems:\n  0:  Surface elevation &lt;Surface Elevation&gt; (meter)\n  1:  U velocity &lt;u velocity component&gt; (meter per sec)\n  2:  V velocity &lt;v velocity component&gt; (meter per sec)\n  3:  Current speed &lt;Current Speed&gt; (meter per sec)\n\n\n\nds.interp_time(freq='2h')\n\n&lt;mikeio.Dataset&gt;\ndims: (time:11, element:884)\ntime: 1985-08-06 07:00:00 - 1985-08-07 03:00:00 (11 records)\ngeometry: Dfsu2D (884 elements, 529 nodes)\nitems:\n  0:  Surface elevation &lt;Surface Elevation&gt; (meter)\n  1:  U velocity &lt;u velocity component&gt; (meter per sec)\n  2:  V velocity &lt;v velocity component&gt; (meter per sec)\n  3:  Current speed &lt;Current Speed&gt; (meter per sec)\n\n\n\n\n\nisel\nDataset.isel(\n    idx=None,\n    *,\n    time=None,\n    x=None,\n    y=None,\n    z=None,\n    element=None,\n    node=None,\n    layer=None,\n    frequency=None,\n    direction=None,\n    axis=0,\n)\nReturn a new Dataset whose data is given by integer indexing along the specified dimension(s).\nThe spatial parameters available depend on the dims (i.e. geometry) of the Dataset:\n\nGrid1D: x\nGrid2D: x, y\nGrid3D: x, y, z\nGeometryFM: element\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\nIndexType\nIndex, or indices, along the specified dimension(s)\nNone\n\n\naxis\nint | str\naxis number or “time”, by default 0\n0\n\n\ntime\nint\ntime index,by default None\nNone\n\n\nx\nint\nx index, by default None\nNone\n\n\ny\nint\ny index, by default None\nNone\n\n\nz\nint\nz index, by default None\nNone\n\n\nelement\nint\nBounding box of coordinates (left lower and right upper) to be selected, by default None\nNone\n\n\nlayer\nIndexType\nlayer index, only used in dfsu 3d\nNone\n\n\ndirection\nIndexType\ndirection index, only used in sprectra\nNone\n\n\nfrequency\nIndexType\nfrequencey index, only used in spectra\nNone\n\n\nnode\nIndexType\nnode index, only used in spectra\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ndataset with subset\n\n\n\n\n\nExamples\n\nimport mikeio\nds = mikeio.read(\"../data/europe_wind_long_lat.dfs2\")\nds.isel(x=slice(10,20), y=slice(40,60))\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1, y:20, x:10)\ntime: 2012-01-01 00:00:00 (time-invariant)\ngeometry: Grid2D (ny=20, nx=10)\nitems:\n  0:  Mean Sea Level Pressure &lt;Air Pressure&gt; (hectopascal)\n  1:  Wind x-comp (10m) &lt;Wind Velocity&gt; (meter per sec)\n  2:  Wind y-comp (10m) &lt;Wind Velocity&gt; (meter per sec)\n\n\n\n\n\nmax\nDataset.max(axis=0, **kwargs)\nMax value along an axis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str\naxis number or “time”, “space” or “items”, by default 0\n0\n\n\n**kwargs\nAny\nadditional arguments passed to the function\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ndataset with max values\n\n\n\n\n\nSee Also\nnanmax : Max values with NaN values removed\n\n\n\nmean\nDataset.mean(axis=0, **kwargs)\nMean value along an axis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str\naxis number or “time”, “space” or “items”, by default 0\n0\n\n\n**kwargs\nAny\nadditional arguments passed to the function\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ndataset with mean values\n\n\n\n\n\nSee Also\nnanmean : Mean values with NaN values removed\naverage : Weighted average\n\n\n\nmerge\nDataset.merge(datasets)\nMerge Datasets along the item dimension.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndatasets\nSequence[Dataset]\nlist of Datasets to merge\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nmerged dataset\n\n\n\n\n\n\nmin\nDataset.min(axis=0, **kwargs)\nMin value along an axis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str\naxis number or “time”, “space” or “items”, by default 0\n0\n\n\n**kwargs\nAny\nadditional arguments passed to the function\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ndataset with min values\n\n\n\n\n\nSee Also\nnanmin : Min values with NaN values removed\n\n\n\nnanmax\nDataset.nanmax(axis=0, **kwargs)\nMax value along an axis (NaN removed).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str | None\naxis number or “time”, “space” or “items”, by default 0\n0\n\n\n**kwargs\nAny\nadditional arguments passed to the function\n{}\n\n\n\n\n\nSee Also\nmax : Mean values\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ndataset with max values\n\n\n\n\n\n\nnanmean\nDataset.nanmean(axis=0, **kwargs)\nMean value along an axis (NaN removed).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str\naxis number or “time”, “space” or “items”, by default 0\n0\n\n\n**kwargs\nAny\nadditional arguments passed to the function\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ndataset with mean values\n\n\n\n\n\n\nnanmin\nDataset.nanmin(axis=0, **kwargs)\nMin value along an axis (NaN removed).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str | None\naxis number or “time”, “space” or “items”, by default 0\n0\n\n\n**kwargs\nAny\nadditional arguments passed to the function\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ndataset with min values\n\n\n\n\n\n\nnanquantile\nDataset.nanquantile(q, *, axis=0, **kwargs)\nCompute the q-th quantile of the data along the specified axis, while ignoring nan values.\nWrapping np.nanquantile\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nq\nfloat | Sequence[float]\nQuantile or sequence of quantiles to compute, which must be between 0 and 1 inclusive.\nrequired\n\n\naxis\nint | str\naxis number or “time”, “space” or “items”, by default 0\n0\n\n\n**kwargs\nAny\nadditional arguments passed to the function\n{}\n\n\n\n\n\nExamples\n&gt;&gt;&gt; ds.nanquantile(q=[0.25,0.75])\n&gt;&gt;&gt; ds.nanquantile(q=0.5)\n&gt;&gt;&gt; ds.nanquantile(q=[0.01,0.5,0.99], axis=\"space\")\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ndataset with quantile values\n\n\n\n\n\n\nnanstd\nDataset.nanstd(axis=0, **kwargs)\nStandard deviation along an axis (NaN removed).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str\naxis number or “time”, “space” or “items”, by default 0\n0\n\n\n**kwargs\nAny\nadditional arguments passed to the function\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ndataset with standard deviation values\n\n\n\n\n\nSee Also\nstd : Standard deviation\n\n\n\nptp\nDataset.ptp(axis=0, **kwargs)\nRange (max - min) a.k.a Peak to Peak along an axis\n\nParameters.\naxis: (int, str, None), optional axis number or “time”, “space” or “items”, by default 0\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ndataset with peak to peak values\n\n\n\n\n\n\nquantile\nDataset.quantile(q, *, axis=0, **kwargs)\nCompute the q-th quantile of the data along the specified axis.\nWrapping np.quantile\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nq\nfloat | Sequence[float]\nQuantile or sequence of quantiles to compute, which must be between 0 and 1 inclusive.\nrequired\n\n\naxis\nint | str\naxis number or “time”, “space” or “items”, by default 0\n0\n\n\n**kwargs\nAny\nadditional arguments passed to the function\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ndataset with quantile values\n\n\n\n\n\nExamples\n&gt;&gt;&gt; ds.quantile(q=[0.25,0.75])\n&gt;&gt;&gt; ds.quantile(q=0.5)\n&gt;&gt;&gt; ds.quantile(q=[0.01,0.5,0.99], axis=\"space\")\n\n\nSee Also\nnanquantile : quantile with NaN values ignored\n\n\n\nremove\nDataset.remove(key)\nRemove DataArray from Dataset.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\n(int, str)\nindex or name of DataArray to be remove from Dataset\nrequired\n\n\n\n\n\nSee also\npop\n\n\n\nrename\nDataset.rename(mapper, **kwargs)\nRename items (DataArrays) in Dataset.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmapper\nMapping[str, str]\ndictionary (or similar) mapping from old to new names\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\n\n\n\n\n\n\nExamples\n\nimport mikeio\nds = mikeio.read(\"../data/tide1.dfs1\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:97, x:10)\ntime: 2019-01-01 00:00:00 - 2019-01-03 00:00:00 (97 records)\ngeometry: Grid1D (n=10, dx=0.06667)\nitems:\n  0:  Level &lt;Water Level&gt; (meter)\n\n\n\nds.rename({\"Level\":\"Surface Elevation\"})\n\n&lt;mikeio.Dataset&gt;\ndims: (time:97, x:10)\ntime: 2019-01-01 00:00:00 - 2019-01-03 00:00:00 (97 records)\ngeometry: Grid1D (n=10, dx=0.06667)\nitems:\n  0:  Surface Elevation &lt;Water Level&gt; (meter)\n\n\n\n\n\nsel\nDataset.sel(\n    time=None,\n    x=None,\n    y=None,\n    z=None,\n    coords=None,\n    area=None,\n    layers=None,\n)\nReturn a new Dataset whose data is given by selecting index labels along the specified dimension(s).\nIn contrast to Dataset.isel, indexers for this method should use labels instead of integers.\nThe spatial parameters available depend on the geometry of the Dataset:\n\nGrid1D: x\nGrid2D: x, y, coords, area\nGrid3D: [not yet implemented! use isel instead]\nGeometryFM: (x,y), coords, area\nGeometryFMLayered: (x,y,z), coords, area, layers\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntime\n(str, pd.DatetimeIndex or Dataset)\ntime labels e.g. “2018-01” or slice(“2018-1-1”,“2019-1-1”), by default None\nNone\n\n\nx\nfloat\nx-coordinate of point to be selected, by default None\nNone\n\n\ny\nfloat\ny-coordinate of point to be selected, by default None\nNone\n\n\nz\nfloat\nz-coordinate of point to be selected, by default None\nNone\n\n\ncoords\nnp.array(float, float)\nAs an alternative to specifying x, y and z individually, the argument coords can be used instead. (x,y)- or (x,y,z)-coordinates of point to be selected, by default None\nNone\n\n\narea\n(float, float, float, float)\nBounding box of coordinates (left lower and right upper) to be selected, by default None\nNone\n\n\nlayers\nint or str or list\nlayer(s) to be selected: “top”, “bottom” or layer number from bottom 0,1,2,… or from the top -1,-2,… or as list of these; only for layered dfsu, by default None\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\nnew Dataset with selected data\n\n\n\n\n\nSee Also\nisel : Select data using integer indexing\n\n\nExamples\n\nimport mikeio\nds = mikeio.read(\"../data/oresund_sigma_z.dfsu\")\nds.sel(x=340000, y=6160000, z=-3)\n\n&lt;mikeio.Dataset&gt;\ndims: (time:3)\ntime: 1997-09-15 21:00:00 - 1997-09-16 03:00:00 (3 records)\ngeometry: GeometryPoint3D(x=340028.1116933554, y=6159980.070243686, z=-3.0)\nitems:\n  0:  Temperature &lt;Temperature&gt; (degree Celsius)\n  1:  Salinity &lt;Salinity&gt; (PSU)\n\n\n\nds.sel(area=(340000, 6160000, 350000, 6170000))\n\n&lt;mikeio.Dataset&gt;\ndims: (time:3, element:224)\ntime: 1997-09-15 21:00:00 - 1997-09-16 03:00:00 (3 records)\ngeometry: Dfsu3DSigmaZ (224 elements, 275 nodes)\nitems:\n  0:  Temperature &lt;Temperature&gt; (degree Celsius)\n  1:  Salinity &lt;Salinity&gt; (PSU)\n\n\n\nds.sel(layers=\"bottom\")\n\n&lt;mikeio.Dataset&gt;\ndims: (time:3, element:3700)\ntime: 1997-09-15 21:00:00 - 1997-09-16 03:00:00 (3 records)\ngeometry: Dfsu2D (3700 elements, 2090 nodes)\nitems:\n  0:  Temperature &lt;Temperature&gt; (degree Celsius)\n  1:  Salinity &lt;Salinity&gt; (PSU)\n\n\n\n\n\nsqueeze\nDataset.squeeze()\nRemove axes of length 1.\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\n\n\n\n\n\n\n\nstd\nDataset.std(axis=0, **kwargs)\nStandard deviation along an axis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\naxis\nint | str\naxis number or “time”, “space” or “items”, by default 0\n0\n\n\n**kwargs\nAny\nadditional arguments passed to the function\n{}\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDataset\ndataset with standard deviation values\n\n\n\n\n\nSee Also\nnanstd : Standard deviation with NaN values removed\n\n\n\nto_dataframe\nDataset.to_dataframe(unit_in_name=False, round_time='ms')\nConvert Dataset to a Pandas DataFrame.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nunit_in_name\nbool\ninclude unit in column name, default False,\nFalse\n\n\nround_time\nstr | bool\nround time to, by default “ms”, use False to avoid rounding\n'ms'\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\n\n\n\n\n\n\n\nto_dfs\nDataset.to_dfs(filename, **kwargs)\nWrite dataset to a new dfs file.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | Path\nfull path to the new dfs file\nrequired\n\n\n**kwargs\nAny\nadditional arguments passed to the writing function, e.g. dtype for dfs0\n{}\n\n\n\n\n\n\nto_numpy\nDataset.to_numpy()\nStack data to a single ndarray with shape (n_items, n_timesteps, …).\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\n\n\n\n\n\n\n\nto_pandas\nDataset.to_pandas(unit_in_name=False, round_time='ms')\nConvert Dataset to a Pandas DataFrame.\n\n\nto_xarray\nDataset.to_xarray()\nExport to xarray.Dataset."
  },
  {
    "objectID": "api/EUMType.html",
    "href": "api/EUMType.html",
    "title": "EUMType",
    "section": "",
    "text": "EUMType(self, code)\nEUM type."
  },
  {
    "objectID": "api/EUMType.html#examples",
    "href": "api/EUMType.html#examples",
    "title": "EUMType",
    "section": "Examples",
    "text": "Examples\n\nimport mikeio\nmikeio.EUMType.Temperature\n\nTemperature\n\n\n\nmikeio.EUMType.Temperature.units\n\n[degree Celsius, degree Fahrenheit, degree Kelvin]"
  },
  {
    "objectID": "api/EUMType.html#attributes",
    "href": "api/EUMType.html#attributes",
    "title": "EUMType",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\ndisplay_name\nDisplay friendly name.\n\n\nunits\nList valid units for this EUM type."
  },
  {
    "objectID": "api/EUMType.html#methods",
    "href": "api/EUMType.html#methods",
    "title": "EUMType",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nsearch\nSearch EUM types by name pattern.\n\n\n\n\nsearch\nEUMType.search(pattern)\nSearch EUM types by name pattern.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npattern\nstr\nSearch pattern (case insensitive).\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[EUMType]\nList of matching EUM types."
  },
  {
    "objectID": "api/dataset._DataArrayPlotterFMVerticalColumn.html",
    "href": "api/dataset._DataArrayPlotterFMVerticalColumn.html",
    "title": "dataset._DataArrayPlotterFMVerticalColumn",
    "section": "",
    "text": "dataset._DataArrayPlotterFMVerticalColumn(self, da)\nPlot a DataArray with a GeometryFMVerticalColumn geometry."
  },
  {
    "objectID": "api/dataset._DataArrayPlotterFMVerticalColumn.html#examples",
    "href": "api/dataset._DataArrayPlotterFMVerticalColumn.html#examples",
    "title": "dataset._DataArrayPlotterFMVerticalColumn",
    "section": "Examples",
    "text": "Examples\n\nimport mikeio\nds = mikeio.read(\"../data/oresund_sigma_z.dfsu\")\ndsp = ds.sel(x=333934.1, y=6158101.5)\nda = dsp[\"Temperature\"]\nda.plot()\n\n\n\n\n\n\n\n\n\nda.plot(extrapolate=False, marker='o')\n\n\n\n\n\n\n\n\n\nda.plot.pcolormesh()"
  },
  {
    "objectID": "api/dataset._DataArrayPlotterFMVerticalColumn.html#methods",
    "href": "api/dataset._DataArrayPlotterFMVerticalColumn.html#methods",
    "title": "dataset._DataArrayPlotterFMVerticalColumn",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nhist\nPlot DataArray as histogram (using ax.hist).\n\n\nline\nPlot data as vertical lines.\n\n\npcolormesh\nPlot data as coloured patches.\n\n\n\n\nhist\ndataset._DataArrayPlotterFMVerticalColumn.hist(\n    bins=None,\n    ax=None,\n    figsize=None,\n    title=None,\n    **kwargs,\n)\nPlot DataArray as histogram (using ax.hist).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbins\n(int or sequence or str)\nIf bins is an integer, it defines the number of equal-width bins in the range. If bins is a sequence, it defines the bin edges, including the left edge of the first bin and the right edge of the last bin. by default: rcParams[“hist.bins”] (default: 10)\nNone\n\n\nax\nAxes | None\nAdding to existing axis, instead of creating new fig\nNone\n\n\nfigsize\ntuple[float, float] | None\nspecify size of figure\nNone\n\n\ntitle\nstr | None\naxes title\nNone\n\n\n**kwargs\nAny\nadditional arguments passed to the plotting function\n{}\n\n\n\n\n\nSee Also\nmatplotlib.pyplot.hist\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n&lt;matplotlib.axes&gt;\n\n\n\n\n\n\n\nline\ndataset._DataArrayPlotterFMVerticalColumn.line(\n    ax=None,\n    figsize=None,\n    extrapolate=True,\n    **kwargs,\n)\nPlot data as vertical lines.\n\n\npcolormesh\ndataset._DataArrayPlotterFMVerticalColumn.pcolormesh(\n    ax=None,\n    figsize=None,\n    title=None,\n    **kwargs,\n)\nPlot data as coloured patches."
  },
  {
    "objectID": "api/dataset._DataArrayPlotterFM.html",
    "href": "api/dataset._DataArrayPlotterFM.html",
    "title": "dataset._DataArrayPlotterFM",
    "section": "",
    "text": "dataset._DataArrayPlotterFM(self, da)\nPlot a DataArray with a GeometryFM geometry.\nIf DataArray has multiple time steps, the first step will be plotted.\nIf DataArray is 3D the surface layer will be plotted."
  },
  {
    "objectID": "api/dataset._DataArrayPlotterFM.html#examples",
    "href": "api/dataset._DataArrayPlotterFM.html#examples",
    "title": "dataset._DataArrayPlotterFM",
    "section": "Examples",
    "text": "Examples\n\nimport mikeio\nda = mikeio.read(\"../data/HD2D.dfsu\")[\"Surface elevation\"]\nda.plot()"
  },
  {
    "objectID": "api/dataset._DataArrayPlotterFM.html#methods",
    "href": "api/dataset._DataArrayPlotterFM.html#methods",
    "title": "dataset._DataArrayPlotterFM",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\ncontour\nPlot data as contour lines.\n\n\ncontourf\nPlot data as filled contours.\n\n\nhist\nPlot DataArray as histogram (using ax.hist).\n\n\nline\nPlot data as lines (timeseries if time is present).\n\n\nmesh\nPlot mesh only.\n\n\noutline\nPlot domain outline.\n\n\npatch\nPlot data as coloured patches.\n\n\n\n\ncontour\ndataset._DataArrayPlotterFM.contour(ax=None, figsize=None, **kwargs)\nPlot data as contour lines.\n\nExamples\n\nda = mikeio.read(\"../data/HD2D.dfsu\")[\"Surface elevation\"]\nda.plot.contour()\n\n\n\n\n\n\n\n\n\n\n\ncontourf\ndataset._DataArrayPlotterFM.contourf(ax=None, figsize=None, **kwargs)\nPlot data as filled contours.\n\nExamples\n\nda = mikeio.read(\"../data/HD2D.dfsu\")[\"Surface elevation\"]\nda.plot.contourf()\n\n\n\n\n\n\n\n\n\n\n\nhist\ndataset._DataArrayPlotterFM.hist(\n    bins=None,\n    ax=None,\n    figsize=None,\n    title=None,\n    **kwargs,\n)\nPlot DataArray as histogram (using ax.hist).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbins\n(int or sequence or str)\nIf bins is an integer, it defines the number of equal-width bins in the range. If bins is a sequence, it defines the bin edges, including the left edge of the first bin and the right edge of the last bin. by default: rcParams[“hist.bins”] (default: 10)\nNone\n\n\nax\nAxes | None\nAdding to existing axis, instead of creating new fig\nNone\n\n\nfigsize\ntuple[float, float] | None\nspecify size of figure\nNone\n\n\ntitle\nstr | None\naxes title\nNone\n\n\n**kwargs\nAny\nadditional arguments passed to the plotting function\n{}\n\n\n\n\n\nSee Also\nmatplotlib.pyplot.hist\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n&lt;matplotlib.axes&gt;\n\n\n\n\n\n\n\nline\ndataset._DataArrayPlotterFM.line(ax=None, figsize=None, **kwargs)\nPlot data as lines (timeseries if time is present).\n\n\nmesh\ndataset._DataArrayPlotterFM.mesh(ax=None, figsize=None, **kwargs)\nPlot mesh only.\n\nExamples\n\nda = mikeio.read(\"../data/HD2D.dfsu\")[\"Surface elevation\"]\nda.plot.mesh()\n\n\n\n\n\n\n\n\n\n\n\noutline\ndataset._DataArrayPlotterFM.outline(ax=None, figsize=None, **kwargs)\nPlot domain outline.\n\nExamples\n\nda = mikeio.read(\"../data/HD2D.dfsu\")[\"Surface elevation\"]\nda.plot.outline()\n\n\n\n\n\n\n\n\n\n\n\npatch\ndataset._DataArrayPlotterFM.patch(ax=None, figsize=None, **kwargs)\nPlot data as coloured patches.\n\nExamples\n\nda = mikeio.read(\"../data/HD2D.dfsu\")[\"Surface elevation\"]\nda.plot.patch()"
  },
  {
    "objectID": "examples/dfsu/index.html",
    "href": "examples/dfsu/index.html",
    "title": "Dfsu examples",
    "section": "",
    "text": "A collection of specific examples of working with dfsu files. For a general introduction to dfsu see the user guide and the API reference.\n\n2D spatial interpolation\nMerging subdomain dfsu files",
    "crumbs": [
      "Examples",
      "Dfsu examples"
    ]
  },
  {
    "objectID": "examples/dfsu/merge_subdomains.html",
    "href": "examples/dfsu/merge_subdomains.html",
    "title": "Merging subdomain dfsu files",
    "section": "",
    "text": "During simulation MIKE will commonly split simulation files into subdomains and output results with a p_# suffix. This script will merge dfsu files of this type into a single file.\nNote: Below implementation considers a 2D dfsu file. For 3D dfsu file, the script needs to be modified accordingly."
  },
  {
    "objectID": "examples/dfsu/merge_subdomains.html#import-libraries",
    "href": "examples/dfsu/merge_subdomains.html#import-libraries",
    "title": "Merging subdomain dfsu files",
    "section": "Import libraries",
    "text": "Import libraries\n\nimport mikeio \nimport numpy as np\nfrom mikeio.spatial import GeometryFM2D\n\n\n# (optional) check first file, items etc. \nmikeio.open(\"../../data/SimA_HD_p0.dfsu\")\n\n&lt;mikeio.Dfsu2DH&gt;\nnumber of elements: 194\nnumber of nodes: 120\nprojection: PROJCS[\"UTM-32\",GEOGCS[\"Unused\",DATUM[\"UTM Projections\",SPHEROID[\"WGS 1984\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"False_Easting\",500000],PARAMETER[\"False_Northing\",0],PARAMETER[\"Central_Meridian\",9],PARAMETER[\"Scale_Factor\",0.9996],PARAMETER[\"Latitude_Of_Origin\",0],UNIT[\"Meter\",1]]\nitems:\n  0:  Surface elevation &lt;Surface Elevation&gt; (meter)\n  1:  Current speed &lt;Current Speed&gt; (meter per sec)\n  2:  Current direction &lt;Current Direction&gt; (radian)\ntime: 2014-01-01 00:00:00 - 2014-01-01 10:00:00 (3 records)"
  },
  {
    "objectID": "examples/dfsu/merge_subdomains.html#choose-items-to-process",
    "href": "examples/dfsu/merge_subdomains.html#choose-items-to-process",
    "title": "Merging subdomain dfsu files",
    "section": "Choose items to process",
    "text": "Choose items to process\n\n# choose items to process (when in doubt look at one of the files you want to process with mikeio.open)\nitems = [\"Surface elevation\", \"Current speed\", \"Current direction\"]"
  },
  {
    "objectID": "examples/dfsu/merge_subdomains.html#read-files",
    "href": "examples/dfsu/merge_subdomains.html#read-files",
    "title": "Merging subdomain dfsu files",
    "section": "Read files",
    "text": "Read files\nOption A: automatically find all with _p# suffix\n\nimport glob\nimport os\n\nbasename = \"../../data/SimA_HD\"  # basename of the dfsu files\n\n\ndef find_dfsu_files(basename):\n    pattern = f\"{basename}_p*.dfsu\"\n    files = sorted(glob.glob(pattern))\n    if not files:\n        raise ValueError(f\"No files found matching the pattern: {pattern}\")\n    return files\n\n\ndfs_files = find_dfsu_files(basename)\nprint(f\"Found {len(dfs_files)} files:\")\nfor file in dfs_files:\n    print(f\"  - {os.path.basename(file)}\")\n\ndfs_list = [mikeio.read(file, items=items) for file in dfs_files]\n\nFound 4 files:\n  - SimA_HD_p0.dfsu\n  - SimA_HD_p1.dfsu\n  - SimA_HD_p2.dfsu\n  - SimA_HD_p3.dfsu\n\n\nOption B: manually select files\n\n# List of input dfsu files\ndfs_files = [\n    \"../../data/SimA_HD_p0.dfsu\",\n    \"../../data/SimA_HD_p1.dfsu\",\n    \"../../data/SimA_HD_p2.dfsu\",\n    \"../../data/SimA_HD_p3.dfsu\",\n]\n\n# read all dfsu files\ndfs_list = [mikeio.read(file, items=items) for file in dfs_files]"
  },
  {
    "objectID": "examples/dfsu/merge_subdomains.html#extract-data-of-all-subdomains",
    "href": "examples/dfsu/merge_subdomains.html#extract-data-of-all-subdomains",
    "title": "Merging subdomain dfsu files",
    "section": "Extract data of all subdomains",
    "text": "Extract data of all subdomains\n\n# Create a dictionary to store data for each item\ndata_dict = {item: [] for item in items}\n\n# Get time steps (assuming all files have the same time steps)\ntime_steps = dfs_list[0][items[0]].time\n\n# loop over items and time steps and concatenate data from all subdomains\nfor item in items:\n    for i in range(len(time_steps)):\n        # Extract and combine data for the current time step from all subdomains\n        combined_data = np.concatenate([dfs[item].values[i, :] for dfs in dfs_list])\n        data_dict[item].append(combined_data)\n    \n    # Convert the list to a numpy array\n    data_dict[item] = np.array(data_dict[item])\n\n# Prepare Merged Data\nmerged_data = np.array([data_dict[item] for item in items])"
  },
  {
    "objectID": "examples/dfsu/merge_subdomains.html#merge-geometry-of-all-subdomains",
    "href": "examples/dfsu/merge_subdomains.html#merge-geometry-of-all-subdomains",
    "title": "Merging subdomain dfsu files",
    "section": "Merge geometry of all subdomains",
    "text": "Merge geometry of all subdomains\n\ngeometries = [dfs.geometry for dfs in dfs_list]\n\ncombined_node_coordinates = []\ncombined_element_table = []\nnode_offset = 0\n\n# loop through geometries to combine nodes and elements of all subdomains\nfor geom in geometries:\n    current_node_coordinates = geom.node_coordinates\n    current_element_table = geom.element_table\n    \n    combined_node_coordinates.extend(current_node_coordinates)\n    adjusted_element_table = [element + node_offset for element in current_element_table]\n    combined_element_table.extend(adjusted_element_table)\n    \n    node_offset += len(current_node_coordinates)\n\ncombined_node_coordinates = np.array(combined_node_coordinates)\ncombined_element_table = np.array(combined_element_table, dtype=object)\nprojection = geometries[0]._projstr\n\n# create combined geometry\ncombined_geometry = GeometryFM2D(\n    node_coordinates=combined_node_coordinates,\n    element_table=combined_element_table,\n    projection=projection\n)\n\n\ncombined_geometry.plot()"
  },
  {
    "objectID": "examples/dfsu/merge_subdomains.html#merge-everything-into-dataset",
    "href": "examples/dfsu/merge_subdomains.html#merge-everything-into-dataset",
    "title": "Merging subdomain dfsu files",
    "section": "Merge everything into dataset",
    "text": "Merge everything into dataset\n\nds_out = mikeio.Dataset.from_numpy(\n    data=merged_data,  # n_items, timesteps, n_elements\n    items=items,\n    time=time_steps,\n    geometry=combined_geometry,\n)\n\n\nds_out[items[0]].sel(time=1).plot() # plot the first time step of the first item"
  },
  {
    "objectID": "examples/dfsu/merge_subdomains.html#write-output-to-single-file",
    "href": "examples/dfsu/merge_subdomains.html#write-output-to-single-file",
    "title": "Merging subdomain dfsu files",
    "section": "Write output to single file",
    "text": "Write output to single file\n\noutput_file = \"area_merged.dfsu\"\nds_out.to_dfs(output_file)"
  },
  {
    "objectID": "examples/dfs2/gfs.html",
    "href": "examples/dfs2/gfs.html",
    "title": "Dfs2 - Meteo data",
    "section": "",
    "text": "import xarray\nimport pandas as pd\nimport mikeio\nThe file gfs_wind.nc contains a small sample of the GFS forecast data downloaded via their OpenDAP service\nds = xarray.open_dataset('../../data/gfs_wind.nc')\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 32kB\nDimensions:   (time: 3, lat: 41, lon: 21)\nCoordinates: (3)\nData variables:\n    msletmsl  (time, lat, lon) float32 10kB ...\n    ugrd10m   (time, lat, lon) float32 10kB ...\n    vgrd10m   (time, lat, lon) float32 10kB ...\nAttributes: (4)xarray.DatasetDimensions:time: 3lat: 41lon: 21Coordinates: (3)time(time)datetime64[ns]2021-09-02T12:00:00 ... 2021-09-...grads_dim :tgrads_mapping :lineargrads_size :129grads_min :12z02sep2021grads_step :3hrlong_name :timeminimum :12z02sep2021maximum :12z18sep2021resolution :0.125array(['2021-09-02T12:00:00.000000000', '2021-09-02T15:00:00.000000000',\n       '2021-09-02T18:00:00.000000000'], dtype='datetime64[ns]')lat(lat)float6430.0 30.25 30.5 ... 39.5 39.75 40.0grads_dim :ygrads_mapping :lineargrads_size :721units :degrees_northlong_name :latitudeminimum :-90.0maximum :90.0resolution :0.25array([30.  , 30.25, 30.5 , 30.75, 31.  , 31.25, 31.5 , 31.75, 32.  , 32.25,\n       32.5 , 32.75, 33.  , 33.25, 33.5 , 33.75, 34.  , 34.25, 34.5 , 34.75,\n       35.  , 35.25, 35.5 , 35.75, 36.  , 36.25, 36.5 , 36.75, 37.  , 37.25,\n       37.5 , 37.75, 38.  , 38.25, 38.5 , 38.75, 39.  , 39.25, 39.5 , 39.75,\n       40.  ])lon(lon)float6410.0 10.25 10.5 ... 14.5 14.75 15.0grads_dim :xgrads_mapping :lineargrads_size :1440units :degrees_eastlong_name :longitudeminimum :0.0maximum :359.75resolution :0.25array([10.  , 10.25, 10.5 , 10.75, 11.  , 11.25, 11.5 , 11.75, 12.  , 12.25,\n       12.5 , 12.75, 13.  , 13.25, 13.5 , 13.75, 14.  , 14.25, 14.5 , 14.75,\n       15.  ])Data variables: (3)msletmsl(time, lat, lon)float32...long_name :** mean sea level mslp (eta model reduction) [pa] [2583 values with dtype=float32]ugrd10m(time, lat, lon)float32...long_name :** 10 m above ground u-component of wind [m/s] [2583 values with dtype=float32]vgrd10m(time, lat, lon)float32...long_name :** 10 m above ground v-component of wind [m/s] [2583 values with dtype=float32]Attributes: (4)title :GFS 0.25 deg starting from 12Z02sep2021, downloaded Sep 02 17:14 UTCConventions :COARDS\nGrADSdataType :Gridhistory :Thu Sep 02 17:27:02 GMT 2021 : imported by GrADS Data Server 2.0\nRunning a Mike 21 HD model, needs at least three variables of meteorological forcing * Mean Sea Level Pressure * U 10m * V 10m\nLet’s take a look the U 10m\nds.ugrd10m.isel(time=0).plot();"
  },
  {
    "objectID": "examples/dfs2/gfs.html#convert-to-dfs2",
    "href": "examples/dfs2/gfs.html#convert-to-dfs2",
    "title": "Dfs2 - Meteo data",
    "section": "Convert to dfs2",
    "text": "Convert to dfs2\n\nTime\n\ntime = pd.DatetimeIndex(ds.time)\ntime\n\nDatetimeIndex(['2021-09-02 12:00:00', '2021-09-02 15:00:00',\n               '2021-09-02 18:00:00'],\n              dtype='datetime64[ns]', freq=None)\n\n\n\n\nVariable types\n\nmikeio.EUMType.Air_Pressure\n\nAir Pressure\n\n\n\nmikeio.EUMType.Air_Pressure.units\n\n[hectopascal, millibar]\n\n\n\nmikeio.EUMType.Wind_Velocity\n\nWind Velocity\n\n\n\nmikeio.EUMType.Wind_Velocity.units\n\n[meter per sec, feet per sec, miles per hour, km per hour, knot]\n\n\n\nmslp = ds.msletmsl.values / 100 # conversion from Pa to hPa\nu = ds.ugrd10m.values\nv = ds.vgrd10m.values\n\n\ngeometry = mikeio.Grid2D(x=ds.lon.values, y=ds.lat.values, projection=\"LONG/LAT\")\ngeometry\n\n&lt;mikeio.Grid2D&gt;\nx: [10, 10.25, ..., 15] (nx=21, dx=0.25)\ny: [30, 30.25, ..., 40] (ny=41, dy=0.25)\nprojection: LONG/LAT\n\n\n\nfrom mikeio import ItemInfo, EUMType, EUMUnit\n\nmslp_da = mikeio.DataArray(data=mslp,time=time, geometry=geometry, item=ItemInfo(\"Mean Sea Level Pressure\", EUMType.Air_Pressure, EUMUnit.hectopascal))\nu_da = mikeio.DataArray(data=u,time=time, geometry=geometry, item=ItemInfo(\"Wind U\", EUMType.Wind_Velocity, EUMUnit.meter_per_sec))\nv_da = mikeio.DataArray(data=v,time=time, geometry=geometry, item=ItemInfo(\"Wind V\", EUMType.Wind_Velocity, EUMUnit.meter_per_sec))\n\n\nmds = mikeio.Dataset([mslp_da, u_da, v_da])\nmds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:3, y:41, x:21)\ntime: 2021-09-02 12:00:00 - 2021-09-02 18:00:00 (3 records)\ngeometry: Grid2D (ny=41, nx=21)\nitems:\n  0:  Mean Sea Level Pressure &lt;Air Pressure&gt; (hectopascal)\n  1:  Wind U &lt;Wind Velocity&gt; (meter per sec)\n  2:  Wind V &lt;Wind Velocity&gt; (meter per sec)\n\n\n\nmds.to_dfs(\"gfs.dfs2\")\n\nClean up\n\nimport os\n\nos.remove(\"gfs.dfs2\")"
  },
  {
    "objectID": "examples/Time-interpolation.html",
    "href": "examples/Time-interpolation.html",
    "title": "Time interpolation",
    "section": "",
    "text": "import numpy as np\nimport mikeio\nds = mikeio.read(\"../data/waves.dfs2\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:3, y:31, x:31)\ntime: 2004-01-01 00:00:00 - 2004-01-03 00:00:00 (3 records)\ngeometry: Grid2D (ny=31, nx=31)\nitems:\n  0:  Sign. Wave Height &lt;Significant wave height&gt; (meter)\n  1:  Peak Wave Period &lt;Wave period&gt; (second)\n  2:  Mean Wave Direction &lt;Mean Wave Direction&gt; (degree)",
    "crumbs": [
      "Examples",
      "Time interpolation"
    ]
  },
  {
    "objectID": "examples/Time-interpolation.html#interpolate-to-specific-timestep",
    "href": "examples/Time-interpolation.html#interpolate-to-specific-timestep",
    "title": "Time interpolation",
    "section": "Interpolate to specific timestep",
    "text": "Interpolate to specific timestep\nA common use case is to interpolate to a shorter timestep, in this case 1h.\n\nds_h = ds.interp_time(3600)\nds_h\n\n&lt;mikeio.Dataset&gt;\ndims: (time:49, y:31, x:31)\ntime: 2004-01-01 00:00:00 - 2004-01-03 00:00:00 (49 records)\ngeometry: Grid2D (ny=31, nx=31)\nitems:\n  0:  Sign. Wave Height &lt;Significant wave height&gt; (meter)\n  1:  Peak Wave Period &lt;Wave period&gt; (second)\n  2:  Mean Wave Direction &lt;Mean Wave Direction&gt; (degree)\n\n\nAnd to store the interpolated data in a new file.\n\nds_h.to_dfs(\"waves_3h.dfs2\")",
    "crumbs": [
      "Examples",
      "Time interpolation"
    ]
  },
  {
    "objectID": "examples/Time-interpolation.html#interpolate-to-time-axis-of-another-dataset",
    "href": "examples/Time-interpolation.html#interpolate-to-time-axis-of-another-dataset",
    "title": "Time interpolation",
    "section": "Interpolate to time axis of another dataset",
    "text": "Interpolate to time axis of another dataset\nRead some non-equidistant data typically found in observed data.\n\nts = mikeio.read(\"../data/waves.dfs0\")\nts\n\n&lt;mikeio.Dataset&gt;\ndims: (time:24)\ntime: 2004-01-01 01:00:00 - 2004-01-03 12:00:10 (24 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  Sign. Wave Height &lt;Undefined&gt; (undefined)\n  1:  Peak Wave Period &lt;Undefined&gt; (undefined)\n  2:  Mean Wave Direction &lt;Undefined&gt; (undefined)\n\n\nThe observed timeseries is longer than the modelled data. Default is to fill values with NaN.\n\ndsi = ds.interp_time(ts)\n\n\ndsi.time\n\nDatetimeIndex(['2004-01-01 01:00:00', '2004-01-01 02:00:00',\n               '2004-01-01 03:00:00', '2004-01-01 04:00:00',\n               '2004-01-01 05:00:00', '2004-01-01 06:00:00',\n               '2004-01-01 07:00:00', '2004-01-01 08:00:00',\n               '2004-01-01 23:00:00', '2004-01-02 00:00:00',\n               '2004-01-02 01:00:00', '2004-01-02 02:00:00',\n               '2004-01-02 03:00:00', '2004-01-02 04:00:00',\n               '2004-01-02 05:00:00', '2004-01-02 06:00:00',\n               '2004-01-02 07:00:00', '2004-01-02 08:00:00',\n               '2004-01-02 09:00:00', '2004-01-02 20:00:00',\n               '2004-01-02 21:00:00', '2004-01-02 23:00:00',\n               '2004-01-03 00:00:00', '2004-01-03 12:00:10'],\n              dtype='datetime64[s]', freq=None)\n\n\n\ndsi[\"Sign. Wave Height\"].shape\n\n(24, 31, 31)\n\n\n\nax = dsi[\"Sign. Wave Height\"].sel(x=250, y=1200).plot(marker='+')\nts[\"Sign. Wave Height\"].plot(ax=ax,marker='+')",
    "crumbs": [
      "Examples",
      "Time interpolation"
    ]
  },
  {
    "objectID": "examples/Time-interpolation.html#model-validation",
    "href": "examples/Time-interpolation.html#model-validation",
    "title": "Time interpolation",
    "section": "Model validation",
    "text": "Model validation\nA common metric for model validation is mean absolute error (MAE).\nIn the example below we calculate this metric using the model data interpolated to the observed times.\nFor a more elaborate model validation library which takes care of these things for you as well as calculating a number of relevant metrics, take a look at ModelSkill.\nUse np.nanmean to skip NaN.\n\nts[\"Sign. Wave Height\"]\n\n&lt;mikeio.DataArray&gt;\nname: Sign. Wave Height\ndims: (time:24)\ntime: 2004-01-01 01:00:00 - 2004-01-03 12:00:10 (24 non-equidistant records)\ngeometry: GeometryUndefined()\nvalues: [0.06521, 0.06771, ..., 0.0576]\n\n\n\ndsi[\"Sign. Wave Height\"].sel(x=250, y=1200)\n\n&lt;mikeio.DataArray&gt;\nname: Sign. Wave Height\ndims: (time:24)\ntime: 2004-01-01 01:00:00 - 2004-01-03 12:00:10 (24 non-equidistant records)\ngeometry: GeometryPoint2D(x=275.0, y=1225.0)\nvalues: [0.0387, 0.03939, ..., nan]\n\n\n\ndiff = (ts[\"Sign. Wave Height\"]  - dsi[\"Sign. Wave Height\"].sel(x=250, y=1200))\ndiff.plot()\n\n\n\n\n\n\n\n\n\nmae = np.abs(diff).nanmean().to_numpy()\nmae\n\nnp.float64(0.030895043650399082)",
    "crumbs": [
      "Examples",
      "Time interpolation"
    ]
  },
  {
    "objectID": "examples/dfs0/cmems_insitu.html",
    "href": "examples/dfs0/cmems_insitu.html",
    "title": "Dfs0 - CMEMS in-situ data",
    "section": "",
    "text": "Copernicus Marine provides access to a wide range of model and in-situ data. In this example we will look at how to access the in-situ data and convert it to a MIKE IO dataset.\n\nimport pandas as pd\nimport xarray as xr\nimport mikeio\n\n\nfino = xr.open_dataset(\"../../data/NO_TS_MO_FINO1_202209.nc\")\nfino\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 3MB\nDimensions:      (TIME: 2879, DEPTH: 7, POSITION: 2879, LATITUDE: 2879,\n                  LONGITUDE: 2879)\nCoordinates:\n  * TIME         (TIME) datetime64[ns] 23kB 2022-09-01 ... 2022-09-30T23:31:5...\n  * LATITUDE     (LATITUDE) float32 12kB 54.0 54.0 54.0 54.0 ... 54.0 54.0 54.0\n  * LONGITUDE    (LONGITUDE) float32 12kB 6.583 6.583 6.583 ... 6.583 6.583\n    DEPH         (TIME, DEPTH) float32 81kB ...\nDimensions without coordinates: DEPTH, POSITION\nData variables: (12/22)\n    DEPH_QC      (TIME, DEPTH) float32 81kB ...\n    TIME_QC      (TIME) float32 12kB ...\n    POSITION_QC  (POSITION) float32 12kB ...\n    TEMP         (TIME, DEPTH) float64 161kB ...\n    TEMP_QC      (TIME, DEPTH) float32 81kB ...\n    VHM0         (TIME, DEPTH) float64 161kB ...\n    ...           ...\n    VTPK_DM      (TIME, DEPTH) object 161kB ...\n    VTZA         (TIME, DEPTH) float64 161kB ...\n    VTZA_QC      (TIME, DEPTH) float32 81kB ...\n    VPED         (TIME, DEPTH) float64 161kB ...\n    VPED_QC      (TIME, DEPTH) float32 81kB ...\n    VPED_DM      (TIME, DEPTH) object 161kB ...\nAttributes: (12/46)\n    platform_code:                  FINO1\n    platform_name:                  \n    data_mode:                      M\n    title:                          NWS - NRT in situ Observations\n    summary:                        Oceanographic data from North West Shelf\n    naming_authority:               Copernicus Marine In Situ\n    ...                             ...\n    doi:                            \n    pi_name:                        \n    qc_manual:                      OceanSITES User's Manual v1.2\n    date_update:                    2022-10-07T05:39:40Z\n    history:                        2022-10-01T18:04:25Z : Creation; 2022-10-...\n    wmo_inst_type:                  xarray.DatasetDimensions:TIME: 2879DEPTH: 7POSITION: 2879LATITUDE: 2879LONGITUDE: 2879Coordinates: (4)TIME(TIME)datetime64[ns]2022-09-01 ... 2022-09-30T23:31:...long_name :Timestandard_name :timevalid_min :-90000.0valid_max :90000.0uncertainty : comment : axis :Tancillary_variables :TIME_QCarray(['2022-09-01T00:00:00.000000000', '2022-09-01T00:02:00.000000000',\n       '2022-09-01T00:30:00.000000000', ..., '2022-09-30T23:02:00.000000000',\n       '2022-09-30T23:30:00.000000000', '2022-09-30T23:31:59.999999744'],\n      shape=(2879,), dtype='datetime64[ns]')LATITUDE(LATITUDE)float3254.0 54.0 54.0 ... 54.0 54.0 54.0long_name :Latitude of each locationstandard_name :latitudeunits :degree_northvalid_min :-90.0valid_max :90.0uncertainty :comment :axis :Yancillary_variables :POSITION_QCarray([54., 54., 54., ..., 54., 54., 54.], shape=(2879,), dtype=float32)LONGITUDE(LONGITUDE)float326.583 6.583 6.583 ... 6.583 6.583long_name :Longitude of each locationstandard_name :longitudeunits :degree_eastvalid_min :-180.0valid_max :180.0uncertainty :comment :axis :Xancillary_variables :POSITION_QCarray([6.583333, 6.583333, 6.583333, ..., 6.583333, 6.583333, 6.583333],\n      shape=(2879,), dtype=float32)DEPH(TIME, DEPTH)float32...long_name :Depthstandard_name :depthunits :mpositive :downvalid_min :-12000.0valid_max :12000.0uncertainty :comment :axis :Zreference :sea_leveldata_mode :Rancillary_variables :DEPH_QC[20153 values with dtype=float32]Data variables: (22)DEPH_QC(TIME, DEPTH)float32...long_name :Depth quality flagconventions :Copernicus Marine In Situ reference table 2valid_min :0valid_max :9flag_values :[0 1 2 3 4 5 6 7 8 9]flag_meanings :no_qc_performed good_data probably_good_data bad_data_that_are_potentially_correctable bad_data value_changed value_below_detection nominal_value interpolated_value missing_value[20153 values with dtype=float32]TIME_QC(TIME)float32...long_name :Time quality flagconventions :Copernicus Marine In Situ reference table 2valid_min :0valid_max :9flag_values :[0 1 2 3 4 5 6 7 8 9]flag_meanings :no_qc_performed good_data probably_good_data bad_data_that_are_potentially_correctable bad_data value_changed value_below_detection nominal_value interpolated_value missing_value[2879 values with dtype=float32]POSITION_QC(POSITION)float32...long_name :Position quality flagconventions :Copernicus Marine In Situ reference table 2valid_min :0valid_max :9flag_values :[0 1 2 3 4 5 6 7 8 9]flag_meanings :no_qc_performed good_data probably_good_data bad_data_that_are_potentially_correctable bad_data value_changed value_below_detection nominal_value interpolated_value missing_value[2879 values with dtype=float32]TEMP(TIME, DEPTH)float64...standard_name :sea_water_temperatureunits :degrees_Clong_name :Sea temperaturevalid_min :-2000valid_max :32000comment : uncertainty : accuracy : precision : resolution : cell_methods : sensor_depth :0.0sensor_mount : sensor_orientation : data_mode :Rancillary_variables :TEMP_QC[20153 values with dtype=float64]TEMP_QC(TIME, DEPTH)float32...long_name :Sea temperature quality flagconventions :Copernicus Marine In Situ reference table 2valid_min :0valid_max :9flag_values :[0 1 2 3 4 5 6 7 8 9]flag_meanings :no_qc_performed good_data probably_good_data bad_data_that_are_potentially_correctable bad_data value_changed value_below_detection nominal_value interpolated_value missing_value[20153 values with dtype=float32]VHM0(TIME, DEPTH)float64...standard_name :sea_surface_wave_significant_heightunits :mlong_name :Spectral significant wave height (Hm0)valid_min :1valid_max :25000comment : uncertainty : accuracy : precision : resolution : cell_methods : type_of_analysis :spectral analysissensor_depth :0.0sensor_mount : sensor_orientation : data_mode :Mancillary_variables :VHM0_QC VHM0_DM[20153 values with dtype=float64]VHM0_QC(TIME, DEPTH)float32...long_name :Spectral significant wave height (Hm0) quality flagconventions :Copernicus Marine In Situ reference table 2valid_min :0valid_max :9flag_values :[0 1 2 3 4 5 6 7 8 9]flag_meanings :no_qc_performed good_data probably_good_data bad_data_that_are_potentially_correctable bad_data value_changed value_below_detection nominal_value interpolated_value missing_value[20153 values with dtype=float32]VHM0_DM(TIME, DEPTH)object...long_name :Spectral significant wave height (Hm0) method of data processingconventions :Copernicus Marine In Situ reference table 1flag_values :R, A, Dflag_meanings :real-time adjusted-in-real-time delayed-mode[20153 values with dtype=object]VZMX(TIME, DEPTH)float64...standard_name :sea_surface_wave_maximum_heightunits :mlong_name :Maximum zero crossing wave height (Hmax)valid_min :0valid_max :40000comment : uncertainty : accuracy : precision : resolution : cell_methods : type_of_analysis :zero crossingsensor_depth :0.0sensor_mount : sensor_orientation : data_mode :Mancillary_variables :VZMX_QC VZMX_DM[20153 values with dtype=float64]VZMX_QC(TIME, DEPTH)float32...long_name :Maximum zero crossing wave height (Hmax) quality flagconventions :Copernicus Marine In Situ reference table 2valid_min :0valid_max :9flag_values :[0 1 2 3 4 5 6 7 8 9]flag_meanings :no_qc_performed good_data probably_good_data bad_data_that_are_potentially_correctable bad_data value_changed value_below_detection nominal_value interpolated_value missing_value[20153 values with dtype=float32]VZMX_DM(TIME, DEPTH)object...long_name :Maximum zero crossing wave height (Hmax) method of data processingconventions :Copernicus Marine In Situ reference table 1flag_values :R, A, Dflag_meanings :real-time adjusted-in-real-time delayed-mode[20153 values with dtype=object]VTM02(TIME, DEPTH)float64...standard_name :sea_surface_wave_mean_period_from_variance_spectral_density_second_frequency_momentunits :slong_name :Spectral moments (0,2) wave period (Tm02)valid_min :1000valid_max :25000comment : uncertainty : accuracy : precision : resolution : cell_methods : type_of_analysis :spectral analysissensor_depth :0.0sensor_mount : sensor_orientation : data_mode :Mancillary_variables :VTM02_QC VTM02_DM[20153 values with dtype=float64]VTM02_QC(TIME, DEPTH)float32...long_name :Spectral moments (0,2) wave period (Tm02) quality flagconventions :Copernicus Marine In Situ reference table 2valid_min :0valid_max :9flag_values :[0 1 2 3 4 5 6 7 8 9]flag_meanings :no_qc_performed good_data probably_good_data bad_data_that_are_potentially_correctable bad_data value_changed value_below_detection nominal_value interpolated_value missing_value[20153 values with dtype=float32]VTM02_DM(TIME, DEPTH)object...long_name :Spectral moments (0,2) wave period (Tm02) method of data processingconventions :Copernicus Marine In Situ reference table 1flag_values :R, A, Dflag_meanings :real-time adjusted-in-real-time delayed-mode[20153 values with dtype=object]VTPK(TIME, DEPTH)float64...standard_name :sea_surface_wave_period_at_variance_spectral_density_maximumunits :slong_name :Wave period at spectral peak / peak period (Tp)valid_min :1000valid_max :30000comment : uncertainty : accuracy : precision : resolution : cell_methods : type_of_analysis :spectral analysissensor_depth :0.0sensor_mount : sensor_orientation : data_mode :Mancillary_variables :VTPK_QC VTPK_DM[20153 values with dtype=float64]VTPK_QC(TIME, DEPTH)float32...long_name :Wave period at spectral peak / peak period (Tp) quality flagconventions :Copernicus Marine In Situ reference table 2valid_min :0valid_max :9flag_values :[0 1 2 3 4 5 6 7 8 9]flag_meanings :no_qc_performed good_data probably_good_data bad_data_that_are_potentially_correctable bad_data value_changed value_below_detection nominal_value interpolated_value missing_value[20153 values with dtype=float32]VTPK_DM(TIME, DEPTH)object...long_name :Wave period at spectral peak / peak period (Tp) method of data processingconventions :Copernicus Marine In Situ reference table 1flag_values :R, A, Dflag_meanings :real-time adjusted-in-real-time delayed-mode[20153 values with dtype=object]VTZA(TIME, DEPTH)float64...standard_name :sea_surface_wave_mean_periodunits :slong_name :Average zero crossing wave period (Tz)valid_min :2000valid_max :25000comment : uncertainty : accuracy : precision : resolution : cell_methods : type_of_analysis :zero crossingsensor_depth :0.0sensor_mount : sensor_orientation : data_mode :Rancillary_variables :VTZA_QC[20153 values with dtype=float64]VTZA_QC(TIME, DEPTH)float32...long_name :Average zero crossing wave period (Tz) quality flagconventions :Copernicus Marine In Situ reference table 2valid_min :0valid_max :9flag_values :[0 1 2 3 4 5 6 7 8 9]flag_meanings :no_qc_performed good_data probably_good_data bad_data_that_are_potentially_correctable bad_data value_changed value_below_detection nominal_value interpolated_value missing_value[20153 values with dtype=float32]VPED(TIME, DEPTH)float64...standard_name :sea_surface_wave_from_direction_at_variance_spectral_density_maximumunits :degreelong_name :Wave principal direction at spectral peakvalid_min :0valid_max :360000comment : uncertainty : accuracy : precision : resolution : cell_methods : type_of_analysis :spectral analysissensor_depth :0.0sensor_mount : sensor_orientation : data_mode :Mancillary_variables :VPED_QC VPED_DM[20153 values with dtype=float64]VPED_QC(TIME, DEPTH)float32...long_name :Wave principal direction at spectral peak quality flagconventions :Copernicus Marine In Situ reference table 2valid_min :0valid_max :9flag_values :[0 1 2 3 4 5 6 7 8 9]flag_meanings :no_qc_performed good_data probably_good_data bad_data_that_are_potentially_correctable bad_data value_changed value_below_detection nominal_value interpolated_value missing_value[20153 values with dtype=float32]VPED_DM(TIME, DEPTH)object...long_name :Wave principal direction at spectral peak method of data processingconventions :Copernicus Marine In Situ reference table 1flag_values :R, A, Dflag_meanings :real-time adjusted-in-real-time delayed-mode[20153 values with dtype=object]Attributes: (46)platform_code :FINO1platform_name :data_mode :Mtitle :NWS - NRT in situ Observationssummary :Oceanographic data from North West Shelfnaming_authority :Copernicus Marine In Situid :NO_TS_MO_FINO1_202209wmo_platform_code :6201065ices_platform_code :source :mooringsource_platform_category_code :48institution_edmo_code :3356institution :R and D centre Kiel University of Applied Sciencesinstitution_references :www.fh-kiel-gmbh.de/en/site_code :comment :contact :cmems-service@bsh.dearea :North Atlantic Oceangeospatial_lat_min :54.00000geospatial_lat_max :54.00000geospatial_lon_min :6.58333geospatial_lon_max :6.58333last_date_observation :2022-09-30T23:32:00Zlast_latitude_observation :54.00000last_longitude_observation :6.58333geospatial_vertical_min :0.00000geospatial_vertical_max :25.00000time_coverage_start :2022-09-01T00:00:00Ztime_coverage_end :2022-09-30T23:32:00Zcdm_data_type :timeSeriesdata_type :OceanSITES time-series databottom_depth :format_version :1.4Conventions :CF-1.6 Copernicus-InSituTAC-FormatManual-1.42 Copernicus-InSituTAC-SRD-1.5 Copernicus-InSituTAC-ParametersList-3.2.1netcdf_version :netCDF-4 classic modelreferences :http://marine.copernicus.eu http://www.marineinsitu.eudata_assembly_center :BSHupdate_interval :PT1Hcitation :These data were collected and made freely available by the Copernicus project and the programs that contribute to itdistribution_statement :These data follow Copernicus standards; they are public and free of charge. User assumes all risk for use of data. User must display citation in any publication or product using data. User must contact PI prior to any commercial use of data.doi :pi_name :qc_manual :OceanSITES User's Manual v1.2date_update :2022-10-07T05:39:40Zhistory :2022-10-01T18:04:25Z : Creation; 2022-10-07T05:39:40Z, BSH North West Shelf PU: Monthly subset 202209 extracted from history file NO_TS_MO_FINO1.ncwmo_inst_type :\n\n\nCMEMS in-situ data is provided in a standardised format.\nFind out which variables we are interested in to extract:\n\ndata = [\n    {\n        \"name\": fino[var].name,\n        \"standard_name\": fino[var].standard_name,\n        \"units\": fino[var].units,\n    }\n    for var in fino.data_vars\n    if hasattr(fino[var], \"units\")\n]\n\npd.DataFrame(data)\n\n\n\n\n\n\n\n\nname\nstandard_name\nunits\n\n\n\n\n0\nTEMP\nsea_water_temperature\ndegrees_C\n\n\n1\nVHM0\nsea_surface_wave_significant_height\nm\n\n\n2\nVZMX\nsea_surface_wave_maximum_height\nm\n\n\n3\nVTM02\nsea_surface_wave_mean_period_from_variance_spe...\ns\n\n\n4\nVTPK\nsea_surface_wave_period_at_variance_spectral_d...\ns\n\n\n5\nVTZA\nsea_surface_wave_mean_period\ns\n\n\n6\nVPED\nsea_surface_wave_from_direction_at_variance_sp...\ndegree\n\n\n\n\n\n\n\nThe data have a DEPTH dimension, even though variables are only measured at a single level and doesn’t vary in time although the format allows for it.\nI.e. temperature (TEMP) is available at level 1 (0.5 m)\n\nfino.DEPH.plot.line(x=\"TIME\")\n\n\n\n\n\n\n\n\n\nfino['TEMP'].plot.line(\"-^\",x='TIME')\n\n\n\n\n\n\n\n\n\nfino['VHM0'].plot.line(\"-^\",x='TIME')\n\n\n\n\n\n\n\n\nWave data are only available at the surface.\n\nfino[['VHM0','VTZA','VPED']].isel(DEPTH=0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 104kB\nDimensions:  (TIME: 2879)\nCoordinates:\n  * TIME     (TIME) datetime64[ns] 23kB 2022-09-01 ... 2022-09-30T23:31:59.99...\n    DEPH     (TIME) float32 12kB ...\nData variables:\n    VHM0     (TIME) float64 23kB ...\n    VTZA     (TIME) float64 23kB ...\n    VPED     (TIME) float64 23kB ...\nAttributes: (12/46)\n    platform_code:                  FINO1\n    platform_name:                  \n    data_mode:                      M\n    title:                          NWS - NRT in situ Observations\n    summary:                        Oceanographic data from North West Shelf\n    naming_authority:               Copernicus Marine In Situ\n    ...                             ...\n    doi:                            \n    pi_name:                        \n    qc_manual:                      OceanSITES User's Manual v1.2\n    date_update:                    2022-10-07T05:39:40Z\n    history:                        2022-10-01T18:04:25Z : Creation; 2022-10-...\n    wmo_inst_type:                  xarray.DatasetDimensions:TIME: 2879Coordinates: (2)TIME(TIME)datetime64[ns]2022-09-01 ... 2022-09-30T23:31:...long_name :Timestandard_name :timevalid_min :-90000.0valid_max :90000.0uncertainty : comment : axis :Tancillary_variables :TIME_QCarray(['2022-09-01T00:00:00.000000000', '2022-09-01T00:02:00.000000000',\n       '2022-09-01T00:30:00.000000000', ..., '2022-09-30T23:02:00.000000000',\n       '2022-09-30T23:30:00.000000000', '2022-09-30T23:31:59.999999744'],\n      shape=(2879,), dtype='datetime64[ns]')DEPH(TIME)float32...long_name :Depthstandard_name :depthunits :mpositive :downvalid_min :-12000.0valid_max :12000.0uncertainty :comment :axis :Zreference :sea_leveldata_mode :Rancillary_variables :DEPH_QC[2879 values with dtype=float32]Data variables: (3)VHM0(TIME)float64...standard_name :sea_surface_wave_significant_heightunits :mlong_name :Spectral significant wave height (Hm0)valid_min :1valid_max :25000comment : uncertainty : accuracy : precision : resolution : cell_methods : type_of_analysis :spectral analysissensor_depth :0.0sensor_mount : sensor_orientation : data_mode :Mancillary_variables :VHM0_QC VHM0_DM[2879 values with dtype=float64]VTZA(TIME)float64...standard_name :sea_surface_wave_mean_periodunits :slong_name :Average zero crossing wave period (Tz)valid_min :2000valid_max :25000comment : uncertainty : accuracy : precision : resolution : cell_methods : type_of_analysis :zero crossingsensor_depth :0.0sensor_mount : sensor_orientation : data_mode :Rancillary_variables :VTZA_QC[2879 values with dtype=float64]VPED(TIME)float64...standard_name :sea_surface_wave_from_direction_at_variance_spectral_density_maximumunits :degreelong_name :Wave principal direction at spectral peakvalid_min :0valid_max :360000comment : uncertainty : accuracy : precision : resolution : cell_methods : type_of_analysis :spectral analysissensor_depth :0.0sensor_mount : sensor_orientation : data_mode :Mancillary_variables :VPED_QC VPED_DM[2879 values with dtype=float64]Attributes: (46)platform_code :FINO1platform_name :data_mode :Mtitle :NWS - NRT in situ Observationssummary :Oceanographic data from North West Shelfnaming_authority :Copernicus Marine In Situid :NO_TS_MO_FINO1_202209wmo_platform_code :6201065ices_platform_code :source :mooringsource_platform_category_code :48institution_edmo_code :3356institution :R and D centre Kiel University of Applied Sciencesinstitution_references :www.fh-kiel-gmbh.de/en/site_code :comment :contact :cmems-service@bsh.dearea :North Atlantic Oceangeospatial_lat_min :54.00000geospatial_lat_max :54.00000geospatial_lon_min :6.58333geospatial_lon_max :6.58333last_date_observation :2022-09-30T23:32:00Zlast_latitude_observation :54.00000last_longitude_observation :6.58333geospatial_vertical_min :0.00000geospatial_vertical_max :25.00000time_coverage_start :2022-09-01T00:00:00Ztime_coverage_end :2022-09-30T23:32:00Zcdm_data_type :timeSeriesdata_type :OceanSITES time-series databottom_depth :format_version :1.4Conventions :CF-1.6 Copernicus-InSituTAC-FormatManual-1.42 Copernicus-InSituTAC-SRD-1.5 Copernicus-InSituTAC-ParametersList-3.2.1netcdf_version :netCDF-4 classic modelreferences :http://marine.copernicus.eu http://www.marineinsitu.eudata_assembly_center :BSHupdate_interval :PT1Hcitation :These data were collected and made freely available by the Copernicus project and the programs that contribute to itdistribution_statement :These data follow Copernicus standards; they are public and free of charge. User assumes all risk for use of data. User must display citation in any publication or product using data. User must contact PI prior to any commercial use of data.doi :pi_name :qc_manual :OceanSITES User's Manual v1.2date_update :2022-10-07T05:39:40Zhistory :2022-10-01T18:04:25Z : Creation; 2022-10-07T05:39:40Z, BSH North West Shelf PU: Monthly subset 202209 extracted from history file NO_TS_MO_FINO1.ncwmo_inst_type :\n\n\n\ndf = fino[['VHM0','VTZA','VPED']].isel(DEPTH=0).to_dataframe()\n\nThe data are stored on the concurrent timesteps.\n\ndf[['VHM0','VTZA','VPED']].head()\n\n\n\n\n\n\n\n\nVHM0\nVTZA\nVPED\n\n\nTIME\n\n\n\n\n\n\n\n2022-09-01 00:00:00\nNaN\n4.12\nNaN\n\n\n2022-09-01 00:02:00\n1.11\nNaN\n2.800000\n\n\n2022-09-01 00:30:00\nNaN\n4.18\nNaN\n\n\n2022-09-01 00:32:00\n1.09\nNaN\n353.000017\n\n\n2022-09-01 01:00:00\nNaN\n4.00\nNaN\n\n\n\n\n\n\n\n\ndf[['VHM0','VTZA']].plot(style='+')\n\n\n\n\n\n\n\n\nConvert the wave height data to a mikeio dataset.\n\nds = mikeio.from_pandas(\n    df[[\"VHM0\"]].dropna(), items=mikeio.ItemInfo(mikeio.EUMType.Significant_wave_height)\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1439)\ntime: 2022-09-01 00:02:00 - 2022-09-30 23:31:59.999999744 (1439 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  VHM0 &lt;Significant wave height&gt; (meter)\n\n\nStore the results in Dfs0 format.\n\nds.to_dfs(\"FINO1_VHM0.dfs0\")\n\nRead the file again to check…\n\nds = mikeio.read(\"FINO1_VHM0.dfs0\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1439)\ntime: 2022-09-01 00:02:00 - 2022-09-30 23:32:00 (1439 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  VHM0 &lt;Significant wave height&gt; (meter)"
  },
  {
    "objectID": "examples/Generic.html",
    "href": "examples/Generic.html",
    "title": "Generic dfs processing",
    "section": "",
    "text": "Tools and methods that applies to any type of dfs files.\nimport matplotlib.pyplot as plt\nimport mikeio\nimport mikeio.generic",
    "crumbs": [
      "Examples",
      "Generic dfs processing"
    ]
  },
  {
    "objectID": "examples/Generic.html#concatenation",
    "href": "examples/Generic.html#concatenation",
    "title": "Generic dfs processing",
    "section": "Concatenation",
    "text": "Concatenation\nTake a look at these two files with overlapping timesteps.\n\nt1 = mikeio.read(\"../data/tide1.dfs1\")\nt1\n\n&lt;mikeio.Dataset&gt;\ndims: (time:97, x:10)\ntime: 2019-01-01 00:00:00 - 2019-01-03 00:00:00 (97 records)\ngeometry: Grid1D (n=10, dx=0.06667)\nitems:\n  0:  Level &lt;Water Level&gt; (meter)\n\n\n\nt2 = mikeio.read(\"../data/tide2.dfs1\")\nt2\n\n&lt;mikeio.Dataset&gt;\ndims: (time:97, x:10)\ntime: 2019-01-02 00:00:00 - 2019-01-04 00:00:00 (97 records)\ngeometry: Grid1D (n=10, dx=0.06667)\nitems:\n  0:  Level &lt;Water Level&gt; (meter)\n\n\nPlot one of the points along the line.\n\nplt.plot(t1.time,t1[0].isel(x=1).values, label=\"File 1\")\nplt.plot(t2.time,t2[0].isel(x=1).values,'k+', label=\"File 2\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nmikeio.generic.concat(infilenames=[\"../data/tide1.dfs1\",\n                                   \"../data/tide2.dfs1\"],\n                     outfilename=\"concat.dfs1\")\n\n  0%|          | 0/2 [00:00&lt;?, ?it/s]100%|██████████| 2/2 [00:00&lt;00:00, 736.10it/s]\n\n\n\nc = mikeio.read(\"concat.dfs1\")\nc[0].isel(x=1).plot()\nc\n\n&lt;mikeio.Dataset&gt;\ndims: (time:145, x:10)\ntime: 2019-01-01 00:00:00 - 2019-01-04 00:00:00 (145 records)\ngeometry: Grid1D (n=10, dx=0.06667)\nitems:\n  0:  Level &lt;Water Level&gt; (meter)",
    "crumbs": [
      "Examples",
      "Generic dfs processing"
    ]
  },
  {
    "objectID": "examples/Generic.html#difference-between-two-files",
    "href": "examples/Generic.html#difference-between-two-files",
    "title": "Generic dfs processing",
    "section": "Difference between two files",
    "text": "Difference between two files\nTake difference between two dfs files with same structure - e.g. to see the difference in result between two calibration runs\n\nfn1 = \"../data/oresundHD_run1.dfsu\"\nfn2 = \"../data/oresundHD_run2.dfsu\"\nfn_diff = \"oresundHD_difference.dfsu\"\nmikeio.generic.diff(fn1, fn2, fn_diff)\n\n  0%|          | 0/5 [00:00&lt;?, ?it/s]100%|██████████| 5/5 [00:00&lt;00:00, 3288.62it/s]\n\n\n\n_, ax = plt.subplots(1,3, sharey=True, figsize=(12,5))\nda = mikeio.read(fn1, time=-1)[0]\nda.plot(vmin=0.06, vmax=0.27, ax=ax[0], title='run 1')\nda = mikeio.read(fn2, time=-1)[0]\nda.plot(vmin=0.06, vmax=0.27, ax=ax[1], title='run 2')\nda = mikeio.read(fn_diff, time=-1)[0]\nda.plot(vmin=-0.1, vmax=0.1, cmap='coolwarm', ax=ax[2], title='difference');",
    "crumbs": [
      "Examples",
      "Generic dfs processing"
    ]
  },
  {
    "objectID": "examples/Generic.html#extract-time-steps-or-items",
    "href": "examples/Generic.html#extract-time-steps-or-items",
    "title": "Generic dfs processing",
    "section": "Extract time steps or items",
    "text": "Extract time steps or items\nThe extract() method can extract a part of a file:\n\ntime slice by specifying start and/or end\nspecific items\n\n\ninfile = \"../data/tide1.dfs1\"\nmikeio.generic.extract(infile, \"extracted.dfs1\", start='2019-01-02')\n\n\ne = mikeio.read(\"extracted.dfs1\")\ne\n\n&lt;mikeio.Dataset&gt;\ndims: (time:49, x:10)\ntime: 2019-01-02 00:00:00 - 2019-01-03 00:00:00 (49 records)\ngeometry: Grid1D (n=10, dx=0.06667)\nitems:\n  0:  Level &lt;Water Level&gt; (meter)\n\n\n\ninfile = \"../data/oresund_vertical_slice.dfsu\"\nmikeio.generic.extract(infile, \"extracted.dfsu\", items='Salinity', end=-2)\n\n\ne = mikeio.read(\"extracted.dfsu\")\ne\n\n&lt;mikeio.Dataset&gt;\ndims: (time:2, element:441)\ntime: 1997-09-15 21:00:00 - 1997-09-16 00:00:00 (2 records)\ngeometry: DfsuVerticalProfileSigmaZ (441 elements, 550 nodes)\nitems:\n  0:  Salinity &lt;Salinity&gt; (PSU)",
    "crumbs": [
      "Examples",
      "Generic dfs processing"
    ]
  },
  {
    "objectID": "examples/Generic.html#scaling",
    "href": "examples/Generic.html#scaling",
    "title": "Generic dfs processing",
    "section": "Scaling",
    "text": "Scaling\nAdding a constant e.g to adjust datum\n\nds = mikeio.read(\"../data/gebco_sound.dfs2\")\nds.Elevation[0].plot();\n\n\n\n\n\n\n\n\n\nds['Elevation'][0,104,131].to_numpy()\n\nnp.float32(-1.0)\n\n\nThis is the processing step.\n\nmikeio.generic.scale(\"../data/gebco_sound.dfs2\", \n                     \"gebco_sound_local_datum.dfs2\",\n                     offset=-2.1\n                     )\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]100%|██████████| 1/1 [00:00&lt;00:00, 1373.38it/s]\n\n\n\nds2 = mikeio.read(\"gebco_sound_local_datum.dfs2\")\nds2['Elevation'][0].plot()\n\n\n\n\n\n\n\n\n\nds2['Elevation'][0,104,131].to_numpy()\n\nnp.float32(-3.1)\n\n\n\nSpatially varying correction\n\nimport numpy as np\nfactor = np.ones_like(ds['Elevation'][0].to_numpy())\nfactor.shape\n\n(264, 216)\n\n\nAdd some spatially varying factors, exaggerated values for educational purpose.\n\nfactor[:,0:100] = 5.3\nfactor[0:40,] = 0.1\nfactor[150:,150:] = 10.7\nplt.imshow(factor)\nplt.colorbar();\n\n\n\n\n\n\n\n\nThe 2d array must first be flipped upside down and then converted to a 1d vector using numpy.ndarray.flatten to match how data is stored in dfs files.\n\nfactor_ud = np.flipud(factor)\nfactor_vec  = factor_ud.flatten()\nmikeio.generic.scale(\"../data/gebco_sound.dfs2\", \n                     \"gebco_sound_spatial.dfs2\",\n                     factor=factor_vec\n                     )\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]100%|██████████| 1/1 [00:00&lt;00:00, 1393.92it/s]\n\n\n\nds3 = mikeio.read(\"gebco_sound_spatial.dfs2\")\nds3.Elevation[0].plot();",
    "crumbs": [
      "Examples",
      "Generic dfs processing"
    ]
  },
  {
    "objectID": "examples/Generic.html#derived-items",
    "href": "examples/Generic.html#derived-items",
    "title": "Generic dfs processing",
    "section": "Derived items",
    "text": "Derived items\nCreating derived items from existing items, e.g. current speed from u and v velocities.\n\nfrom mikeio.generic import DerivedItem\n\nfn = \"../data/oresundHD_run1.dfsu\"\nfn_derived = \"oresundHD_speed.dfsu\"\nmikeio.generic.transform(\n    fn,\n    fn_derived,\n    [\n        DerivedItem(\n            name=\"Current speed\",\n            type=mikeio.EUMType.Current_Speed,\n            unit=mikeio.EUMUnit.knot,\n            func=lambda x: 1.94 * np.sqrt(x[\"U velocity\"] ** 2 + x[\"V velocity\"] ** 2),\n        )\n    ],\n)",
    "crumbs": [
      "Examples",
      "Generic dfs processing"
    ]
  },
  {
    "objectID": "examples/Generic.html#time-average",
    "href": "examples/Generic.html#time-average",
    "title": "Generic dfs processing",
    "section": "Time average",
    "text": "Time average\n\nfn = \"../data/NorthSea_HD_and_windspeed.dfsu\"\nfn_avg = \"Avg_NorthSea_HD_and_windspeed.dfsu\"\nmikeio.generic.avg_time(fn, fn_avg)\n\n  0%|          | 0/66 [00:00&lt;?, ?it/s]100%|██████████| 66/66 [00:00&lt;00:00, 22853.47it/s]\n\n\n\nds = mikeio.read(fn)\nds.mean(axis=0).describe()   # alternative way of getting the time average\n\n\n\n\n\n\n\n\nSurface elevation\nWind speed\n\n\n\n\ncount\n958.000000\n958.000000\n\n\nmean\n0.449857\n12.772706\n\n\nstd\n0.178127\n2.367667\n\n\nmin\n0.114355\n6.498364\n\n\n25%\n0.373691\n11.199439\n\n\n50%\n0.431747\n12.984060\n\n\n75%\n0.479224\n14.658077\n\n\nmax\n1.202888\n16.677952\n\n\n\n\n\n\n\n\nds_avg = mikeio.read(fn_avg)\nds_avg.describe()\n\n\n\n\n\n\n\n\nSurface elevation\nWind speed\n\n\n\n\ncount\n958.000000\n958.000000\n\n\nmean\n0.449857\n12.772706\n\n\nstd\n0.178127\n2.367667\n\n\nmin\n0.114355\n6.498364\n\n\n25%\n0.373691\n11.199439\n\n\n50%\n0.431747\n12.984060\n\n\n75%\n0.479224\n14.658077\n\n\nmax\n1.202888\n16.677952",
    "crumbs": [
      "Examples",
      "Generic dfs processing"
    ]
  },
  {
    "objectID": "examples/Generic.html#quantile",
    "href": "examples/Generic.html#quantile",
    "title": "Generic dfs processing",
    "section": "Quantile",
    "text": "Quantile\nExample that calculates the 25%, 50% and 75% percentile for all items in a dfsu file.\n\nfn = \"../data/NorthSea_HD_and_windspeed.dfsu\"\nfn_q = \"Q_NorthSea_HD_and_windspeed.dfsu\"\nmikeio.generic.quantile(fn, fn_q, q=[0.25,0.5,0.75])\n\n\nds = mikeio.read(fn_q)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1, element:958)\ntime: 2017-10-27 00:00:00 (time-invariant)\ngeometry: Dfsu2D (958 elements, 570 nodes)\nitems:\n  0:  Quantile 0.25, Surface elevation &lt;Surface Elevation&gt; (meter)\n  1:  Quantile 0.5, Surface elevation &lt;Surface Elevation&gt; (meter)\n  2:  Quantile 0.75, Surface elevation &lt;Surface Elevation&gt; (meter)\n  3:  Quantile 0.25, Wind speed &lt;Wind speed&gt; (meter per sec)\n  4:  Quantile 0.5, Wind speed &lt;Wind speed&gt; (meter per sec)\n  5:  Quantile 0.75, Wind speed &lt;Wind speed&gt; (meter per sec)\n\n\n\nda_q75 = ds[\"Quantile 0.75, Wind speed\"]\nda_q75.plot(title=\"75th percentile, wind speed\", label=\"m/s\")",
    "crumbs": [
      "Examples",
      "Generic dfs processing"
    ]
  },
  {
    "objectID": "examples/Generic.html#clean-up",
    "href": "examples/Generic.html#clean-up",
    "title": "Generic dfs processing",
    "section": "Clean up",
    "text": "Clean up\n\nimport os\n\nos.remove(\"concat.dfs1\")\nos.remove(\"oresundHD_difference.dfsu\")\nos.remove(\"extracted.dfs1\")\nos.remove(\"extracted.dfsu\")\nos.remove(\"gebco_sound_local_datum.dfs2\")\nos.remove(\"gebco_sound_spatial.dfs2\")\nos.remove(\"Avg_NorthSea_HD_and_windspeed.dfsu\")\nos.remove(fn_q)\nos.remove(\"oresundHD_speed.dfsu\")",
    "crumbs": [
      "Examples",
      "Generic dfs processing"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "MIKE IO",
    "section": "",
    "text": "Copyright (c) 2019, DHI A/S All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of DHI nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "user-guide/dfs0.html",
    "href": "user-guide/dfs0.html",
    "title": "Dfs0",
    "section": "",
    "text": "A dfs0 file is also called a time series file.\nWorking with data from dfs0 files are conveniently done in one of two ways:",
    "crumbs": [
      "User Guide",
      "Dfs0"
    ]
  },
  {
    "objectID": "user-guide/dfs0.html#read-dfs0-to-dataset",
    "href": "user-guide/dfs0.html#read-dfs0-to-dataset",
    "title": "Dfs0",
    "section": "Read Dfs0 to Dataset",
    "text": "Read Dfs0 to Dataset\n\nimport mikeio\n\nds = mikeio.read(\"../data/da_diagnostic.dfs0\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:744)\ntime: 2017-10-27 00:00:00 - 2017-10-29 18:00:00 (744 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  State 1Sign. Wave Height &lt;Significant wave height&gt; (meter)\n  1:  State 2Sign. Wave Height &lt;Significant wave height&gt; (meter)\n  2:  Mean StateSign. Wave Height &lt;Significant wave height&gt; (meter)\n  3:  MeasurementSign. Wave Height &lt;Significant wave height&gt; (meter)",
    "crumbs": [
      "User Guide",
      "Dfs0"
    ]
  },
  {
    "objectID": "user-guide/dfs0.html#from-dfs0-to-pandas-dataframe",
    "href": "user-guide/dfs0.html#from-dfs0-to-pandas-dataframe",
    "title": "Dfs0",
    "section": "From Dfs0 to pandas DataFrame",
    "text": "From Dfs0 to pandas DataFrame\n\ndf = ds.to_dataframe()\ndf.head()\n\n\n\n\n\n\n\n\nState 1Sign. Wave Height\nState 2Sign. Wave Height\nMean StateSign. Wave Height\nMeasurementSign. Wave Height\n\n\n\n\n2017-10-27 00:00:00\n1.749465\n1.749465\n1.749465\n1.72\n\n\n2017-10-27 00:10:00\n1.811340\n1.796895\n1.807738\nNaN\n\n\n2017-10-27 00:20:00\n1.863424\n1.842759\n1.853422\nNaN\n\n\n2017-10-27 00:30:00\n1.922261\n1.889839\n1.897670\nNaN\n\n\n2017-10-27 00:40:00\n1.972455\n1.934886\n1.935281\nNaN",
    "crumbs": [
      "User Guide",
      "Dfs0"
    ]
  },
  {
    "objectID": "user-guide/dfs0.html#from-pandas-dataframe-to-dfs0",
    "href": "user-guide/dfs0.html#from-pandas-dataframe-to-dfs0",
    "title": "Dfs0",
    "section": "From pandas DataFrame to Dfs0",
    "text": "From pandas DataFrame to Dfs0\n\nimport pandas as pd\n\ndf = pd.read_csv(\n    \"../data/co2-mm-mlo.csv\", parse_dates=True, index_col=\"Date\", na_values=-99.99\n)\nds = mikeio.from_pandas(df)\nds.to_dfs(\"mauna_loa_co2.dfs0\")",
    "crumbs": [
      "User Guide",
      "Dfs0"
    ]
  },
  {
    "objectID": "user-guide/dfs0.html#dfs0-example-notebooks",
    "href": "user-guide/dfs0.html#dfs0-example-notebooks",
    "title": "Dfs0",
    "section": "Dfs0 example notebooks",
    "text": "Dfs0 example notebooks\n\nDfs0 | Relative time\nDfs0 | getting-started-with-mikeio - Course literature",
    "crumbs": [
      "User Guide",
      "Dfs0"
    ]
  },
  {
    "objectID": "user-guide/dfsu.html",
    "href": "user-guide/dfsu.html",
    "title": "Dfsu",
    "section": "",
    "text": "Dfsu and mesh files are both flexible mesh file formats used by MIKE 21/3 engines. The .mesh file is an ASCII file for storing the flexible mesh geometry. The .dfsu file is a binary dfs file with data on this mesh. The mesh geometry is available in a .dfsu file as static items.\nFor a detailed description of the .mesh and .dfsu file specification see the flexible file format documentation.",
    "crumbs": [
      "User Guide",
      "Dfsu"
    ]
  },
  {
    "objectID": "user-guide/dfsu.html#the-flexible-mesh",
    "href": "user-guide/dfsu.html#the-flexible-mesh",
    "title": "Dfsu",
    "section": "The flexible mesh",
    "text": "The flexible mesh\nThe mesh geometry in a .mesh or a .dfsu file consists of a list of nodes and a list of elements.\nEach node has:\n\nNode id\nx,y,z coordinates\nCode (0 for internal water points, 1 for land, &gt;1 for open boundary)\n\nEach element has:\n\nElement id\nElement table; specifies for each element the nodes that defines the element. (the number of nodes defines the type: triangular, quadrilateral, prism etc.)\n\n\n\n\n\n\n\nNote\n\n\n\nIn MIKE Zero, node ids, element ids and layer ids are 1-based. In MIKE IO, all ids are 0-based following standard Python indexing. That means, as an example, that when finding the element closest to a point its id will be 1 lower in MIKE IO compared to examining the file in MIKE Zero.",
    "crumbs": [
      "User Guide",
      "Dfsu"
    ]
  },
  {
    "objectID": "user-guide/dfsu.html#mike-io-flexible-mesh-geometry",
    "href": "user-guide/dfsu.html#mike-io-flexible-mesh-geometry",
    "title": "Dfsu",
    "section": "MIKE IO Flexible Mesh Geometry",
    "text": "MIKE IO Flexible Mesh Geometry\nMIKE IO has Flexible Mesh Geometry classes, e.g. GeometryFM2D, containing the list of node coordinates and the element table which defines the mesh, as well as a number of derived properties (e.g. element coordinates) and methods making it convenient to work with the mesh.\nIf a .dfsu file is read with mikeio.read, the returned Dataset ds will contain a Flexible Mesh Geometry geometry. If a .dfsu or a .mesh file is opened with mikeio.open, the returned object will also contain a Flexible Mesh Geometry geometry.\n\nimport mikeio\n\nds = mikeio.read(\"../data/oresundHD_run1.dfsu\")\nds.geometry\n\nFlexible Mesh Geometry: Dfsu2D\nnumber of nodes: 2046\nnumber of elements: 3612\nprojection: UTM-33\n\n\n\ndfs = mikeio.open(\"../data/oresundHD_run1.dfsu\")\ndfs.geometry\n\nFlexible Mesh Geometry: Dfsu2D\nnumber of nodes: 2046\nnumber of elements: 3612\nprojection: UTM-33",
    "crumbs": [
      "User Guide",
      "Dfsu"
    ]
  },
  {
    "objectID": "user-guide/dfsu.html#common-dfsu-and-mesh-properties",
    "href": "user-guide/dfsu.html#common-dfsu-and-mesh-properties",
    "title": "Dfsu",
    "section": "Common Dfsu and Mesh properties",
    "text": "Common Dfsu and Mesh properties\nMIKE IO has Dfsu classes for .dfsu files and a Mesh class for .mesh files which both have a mikeio.spatial.GeometryFM2D/mikeio.spatial.GeometryFM3D accessible through the ´geometry´ accessor.",
    "crumbs": [
      "User Guide",
      "Dfsu"
    ]
  },
  {
    "objectID": "user-guide/dfsu.html#dfsu-types",
    "href": "user-guide/dfsu.html#dfsu-types",
    "title": "Dfsu",
    "section": "Dfsu types",
    "text": "Dfsu types\nThe following dfsu file types are supported by MIKE IO.\n\n2D horizontal.\n3D layered.\n2D vertical profile - a vertical slice through a 3D layered file.\n1D vertical column - a vertical dfs1 file and is produced by taking out one column of a 3D layered file.\n3D/4D SW, two horizontal dimensions and 1-2 spectral dimensions. Output from MIKE 21 SW.\n\nWhen a dfsu file is opened with mikeio.open() the returned dfs object will be a specialized class Dfsu2DH, Dfsu3D, Dfsu2DV, or DfsuSpectral according to the type of dfsu file.\nThe layered files (3d, 2d/1d vertical) can have both sigma- and z-layers or only sigma-layers.\nIn most cases values are stored in cell centers and vertical (z) information in nodes, but the following values types exists:\n\nStandard value type, storing values on elements and/or nodes. This is the default type.\nFace value type, storing values on element faces. This is used e.g. for HD decoupling files, to store the discharge between elements.\nSpectral value type, for each node or element, storing vales for a number of frequencies and/or directions. This is the file type for spectral output from the MIKE 21 SW.",
    "crumbs": [
      "User Guide",
      "Dfsu"
    ]
  },
  {
    "objectID": "user-guide/dfsu.html#visualization",
    "href": "user-guide/dfsu.html#visualization",
    "title": "Dfsu",
    "section": "Visualization",
    "text": "Visualization\n\nGeometry\nThe geometry (bathymetry) can be visualized in various ways.\n\ngeom  = dfs.geometry\ngeom.plot();\n\n\n\n\n\n\n\n\n\ngeom.plot.mesh();\n\n\n\n\n\n\n\n\n\ngeom.plot(vmin=-20);\n\n\n\n\n\n\n\n\n\ngeom.plot.contour(levels=[-20,-10], cmap='tab10');\n\n\n\n\n\n\n\n\n\ngeom.plot.contourf(levels=6, vmin=-30);\n\n\n\n\n\n\n\n\n\n\nData\nThe data can be visualized in a similar way.\n\nda = ds[\"Surface elevation\"]\nda.plot(cmap='plasma');\n\n\n\n\n\n\n\n\n\nax = da.plot.contour(show_mesh=True, cmap=\"tab20\")\nax.set_ylim(6135000, 6160000);",
    "crumbs": [
      "User Guide",
      "Dfsu"
    ]
  },
  {
    "objectID": "user-guide/eum.html",
    "href": "user-guide/eum.html",
    "title": "EUM",
    "section": "",
    "text": "The dfs items in MIKE IO are represented by the ItemInfo class. An ItemInfo consists of:\n\nname - a user-defined string\ntype - an EUMType\nunit - an EUMUnit\n\nThe ItemInfo class has some sensible defaults, thus you can specify only a name or a type. If you don’t specify a unit, the default unit for that type will be used.\n\nfrom mikeio import ItemInfo, EUMType, EUMUnit\n\nitem = ItemInfo(\"Viken\", EUMType.Water_Level)\nitem\n\nViken &lt;Water Level&gt; (meter)\n\n\n\nItemInfo(EUMType.Wind_speed)\n\nWind speed &lt;Wind speed&gt; (meter per sec)\n\n\n\nItemInfo(\"Viken\", EUMType.Water_Level, EUMUnit.feet)\n\nViken &lt;Water Level&gt; (feet)\n\n\nMatching units for specific type:\n\nEUMType.Wind_speed.units\n\n[meter per sec, feet per sec, knot, km per hour, miles per hour]\n\n\nDefault unit:\n\nEUMType.Precipitation_Rate.units[0]\n\nmm per day\n\n\n\nunit = EUMType.Precipitation_Rate.units[0]\nunit\n\nmm per day\n\n\n\ntype(unit)\n\n&lt;enum 'EUMUnit'&gt;\n\n\nan EUMUnit is encoded as integers, which you can utilize in some MIKE applications.\n\nint(unit)\n2004\n\n2004\n\n\n\n\nIt is also possible to do a string based search, e.g. to find all EUM types containing the substring ‘period’:\n\nEUMType.search(\"period\")\n\n[Wave period, Return period, Update Period, Threshold period]",
    "crumbs": [
      "User Guide",
      "EUM"
    ]
  },
  {
    "objectID": "user-guide/eum.html#eum-type-search",
    "href": "user-guide/eum.html#eum-type-search",
    "title": "EUM",
    "section": "",
    "text": "It is also possible to do a string based search, e.g. to find all EUM types containing the substring ‘period’:\n\nEUMType.search(\"period\")\n\n[Wave period, Return period, Update Period, Threshold period]",
    "crumbs": [
      "User Guide",
      "EUM"
    ]
  },
  {
    "objectID": "user-guide/dataarray.html",
    "href": "user-guide/dataarray.html",
    "title": "DataArray",
    "section": "",
    "text": "The DataArray is the common MIKE IO data structure for item data from dfs files. The mikeio.read methods returns a Dataset as a container of DataArrays (Dfs items)\nEach DataArray have the following properties:\nUse DataArray’s string representation to get an overview of the DataArray\nimport mikeio\n\nds = mikeio.read(\"../data/HD2D.dfsu\")\nda = ds[\"Surface elevation\"]\nda\n\n&lt;mikeio.DataArray&gt;\nname: Surface elevation\ndims: (time:9, element:884)\ntime: 1985-08-06 07:00:00 - 1985-08-07 03:00:00 (9 records)\ngeometry: Dfsu2D (884 elements, 529 nodes)",
    "crumbs": [
      "User Guide",
      "DataArray"
    ]
  },
  {
    "objectID": "user-guide/dataarray.html#temporal-selection",
    "href": "user-guide/dataarray.html#temporal-selection",
    "title": "DataArray",
    "section": " Temporal selection",
    "text": "Temporal selection\nA time slice of a DataArray can be selected in several different ways.\n\nda.sel(time=\"1985-08-06 12:00\")\n\n&lt;mikeio.DataArray&gt;\nname: Surface elevation\ndims: (element:884)\ntime: 1985-08-06 12:00:00 (time-invariant)\ngeometry: Dfsu2D (884 elements, 529 nodes)\nvalues: [0.1012, 0.1012, ..., 0.105]\n\n\n\nda.sel(time=slice(\"1985-08-06 12:00\", \"1985-08-06 17:00\"))\n\n&lt;mikeio.DataArray&gt;\nname: Surface elevation\ndims: (time:3, element:884)\ntime: 1985-08-06 12:00:00 - 1985-08-06 17:00:00 (3 records)\ngeometry: Dfsu2D (884 elements, 529 nodes)\n\n\n\nda.isel(time=2)\n\n&lt;mikeio.DataArray&gt;\nname: Surface elevation\ndims: (element:884)\ntime: 1985-08-06 12:00:00 (time-invariant)\ngeometry: Dfsu2D (884 elements, 529 nodes)\nvalues: [0.1012, 0.1012, ..., 0.105]\n\n\n\nda.isel(time=range(2, ds.n_timesteps, 2))\n\n&lt;mikeio.DataArray&gt;\nname: Surface elevation\ndims: (time:4, element:884)\ntime: 1985-08-06 12:00:00 - 1985-08-07 03:00:00 (4 records)\ngeometry: Dfsu2D (884 elements, 529 nodes)",
    "crumbs": [
      "User Guide",
      "DataArray"
    ]
  },
  {
    "objectID": "user-guide/dataarray.html#spatial-selection",
    "href": "user-guide/dataarray.html#spatial-selection",
    "title": "DataArray",
    "section": " Spatial selection",
    "text": "Spatial selection\nThe sel method finds the nearest element.\n\nda.sel(x=607002, y=6906734)\n\n&lt;mikeio.DataArray&gt;\nname: Surface elevation\ndims: (time:9)\ntime: 1985-08-06 07:00:00 - 1985-08-07 03:00:00 (9 records)\ngeometry: GeometryPoint2D(x=607002.7094112666, y=6906734.833048992)\nvalues: [0.4591, 0.8078, ..., -0.6311]",
    "crumbs": [
      "User Guide",
      "DataArray"
    ]
  },
  {
    "objectID": "user-guide/dataarray.html#modifying-values",
    "href": "user-guide/dataarray.html#modifying-values",
    "title": "DataArray",
    "section": "Modifying values",
    "text": "Modifying values\nYou can modify the values of a DataArray by changing its values:\n\nda.values[0, 3] = 5.0\n\nIf you wish to change values of a subset of the DataArray you should be aware of the difference between a view and a copy of the data. Similar to NumPy, MIKE IO selection method will return a view of the data when using single index and slices, but a copy of the data using fancy indexing (a list of indicies or boolean indexing). Note that prior to release 1.3, MIKE IO would always return a copy.\nIt is recommended to change the values using values property directly on the original DataArray (like above), but it is also possible to change the values of the original DataArray by working on a subset DataArray if it is selected with single index or slice as explained above.\n\nda_sub = da.isel(time=0)\nda_sub.values[:] = 5.0    # will change da\n\nFancy indexing will return a copy and therefore not change the original:\n\nda_sub = da.isel(time=[0,1,2])\nda_sub.values[:] = 5.0    # will NOT change da",
    "crumbs": [
      "User Guide",
      "DataArray"
    ]
  },
  {
    "objectID": "user-guide/dataarray.html#plotting",
    "href": "user-guide/dataarray.html#plotting",
    "title": "DataArray",
    "section": " Plotting",
    "text": "Plotting\nThe plotting of a DataArray is context-aware meaning that plotting behaviour depends on the geometry of the DataArray being plotted.\n\nda.plot()\n\n\n\n\n\n\n\n\n\nda.plot.contourf()\n\n\n\n\n\n\n\n\n\nda.plot.mesh()\n\n\n\n\n\n\n\n\nSee details in the API specification below and in the bottom of the relevant pages e.g. DataArray Plotter Grid2D API.",
    "crumbs": [
      "User Guide",
      "DataArray"
    ]
  },
  {
    "objectID": "user-guide/dataarray.html#properties",
    "href": "user-guide/dataarray.html#properties",
    "title": "DataArray",
    "section": "Properties",
    "text": "Properties\nThe DataArray has several properties:\n\ntime - Time index\ngeometry - geometry of the data (e.g. spatial.GeometryFM2D)\nshape - Shape of the data\ndeletevalue - File delete value (NaN value)\n\n\nda.geometry\n\nFlexible Mesh Geometry: Dfsu2D\nnumber of nodes: 529\nnumber of elements: 884\nprojection: UTM-29",
    "crumbs": [
      "User Guide",
      "DataArray"
    ]
  },
  {
    "objectID": "user-guide/dataarray.html#methods",
    "href": "user-guide/dataarray.html#methods",
    "title": "DataArray",
    "section": "Methods",
    "text": "Methods\nDataArray has several useful methods for working with data, including different ways of selecting data:\n\nsel() - Select subset along an axis\nisel() - Select subset along an axis with an integer\n\n\nAggregations along an axis\n\nmean() - Mean value along an axis\nnanmean() - Mean value along an axis (NaN removed)\nmax() - Max value along an axis\nnanmax() - Max value along an axis (NaN removed)\nmin() - Min value along an axis\nnanmin() - Min value along an axis (NaN removed)\naggregate() - Aggregate along an axis\nquantile() - Quantiles along an axis",
    "crumbs": [
      "User Guide",
      "DataArray"
    ]
  },
  {
    "objectID": "user-guide/dataarray.html#mathematical-operations",
    "href": "user-guide/dataarray.html#mathematical-operations",
    "title": "DataArray",
    "section": " Mathematical operations",
    "text": "Mathematical operations\n\nds + value\nds - value\nds * value\n\nand + and - between two DataArrays (if number of items and shapes conform):\n\nds1 + ds2\nds1 - ds2",
    "crumbs": [
      "User Guide",
      "DataArray"
    ]
  },
  {
    "objectID": "user-guide/dataarray.html#multiply-or-add-scalar",
    "href": "user-guide/dataarray.html#multiply-or-add-scalar",
    "title": "DataArray",
    "section": "Multiply or add scalar",
    "text": "Multiply or add scalar\n\nda1 = mikeio.read(\"../data/oresundHD_run1.dfsu\", items=\"Surface elevation\")[0]\nda2 = mikeio.read(\"../data/oresundHD_run2.dfsu\", items=\"Surface elevation\")[0]\n\nda1.values.mean()\n\nnp.float32(0.18681717)\n\n\n\nda1_A = da1 + 1\nda1_B = da1 - 1\nda1_A.values.mean(), da1_B.values.mean()\n\n(np.float32(1.1868172), np.float32(-0.81318283))\n\n\n\nda1_C = da1 * 2\nda1_D = da1 / 2\nda1_C.values.mean(), da1_D.values.mean()\n\n(np.float32(0.37363434), np.float32(0.093408585))",
    "crumbs": [
      "User Guide",
      "DataArray"
    ]
  },
  {
    "objectID": "user-guide/dataarray.html#difference-between-two-dataarrays",
    "href": "user-guide/dataarray.html#difference-between-two-dataarrays",
    "title": "DataArray",
    "section": "Difference between two DataArrays",
    "text": "Difference between two DataArrays\nAssume that we have two calibration runs and we wan’t to find the difference…\n\nda_diff = da1-da2\nda_diff.plot(title=\"Difference\");\n\n\n\n\n\n\n\n\n\nda_ratio = da1 / da2\nda_ratio.plot(title=\"\", label=\"Ratio\", vmin=0.8, vmax=1.2, levels=9, cmap=\"coolwarm\")",
    "crumbs": [
      "User Guide",
      "DataArray"
    ]
  },
  {
    "objectID": "user-guide/dataarray.html#unit-handling",
    "href": "user-guide/dataarray.html#unit-handling",
    "title": "DataArray",
    "section": "Unit handling",
    "text": "Unit handling\nMultiplication and divison of two physical quantities would normally change the unit of the result, but in the case of DataArrays, the type and unit of the result will be the ones of the first operand.\nOther methods that also return a DataArray:\n\ninterp_like - Spatio (temporal) interpolation (see example Dfsu interpolation\ninterp_time() - Temporal interpolation (see example Time interpolation)\ndropna() - Remove time steps where all items are NaN\nfillna() - Fill missing values with a constant value\nsqueeze() - Remove axes of length 1\n\n\nConversion:\n\nto_xarray() - Convert DataArray to a xarray DataArray (great for Dfs2)\nto_dfs() - Write DataArray to a Dfs file",
    "crumbs": [
      "User Guide",
      "DataArray"
    ]
  },
  {
    "objectID": "user-guide/getting-started.html",
    "href": "user-guide/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Windows or Linux operating system\nPython x64 3.10 - 3.13\n(Windows) VC++ redistributables (already installed if you have MIKE)",
    "crumbs": [
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "user-guide/getting-started.html#requirements",
    "href": "user-guide/getting-started.html#requirements",
    "title": "Getting started",
    "section": "",
    "text": "Windows or Linux operating system\nPython x64 3.10 - 3.13\n(Windows) VC++ redistributables (already installed if you have MIKE)",
    "crumbs": [
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "user-guide/getting-started.html#installation",
    "href": "user-guide/getting-started.html#installation",
    "title": "Getting started",
    "section": "Installation",
    "text": "Installation\n\n\n\n\n\n\nTipUsing uv\n\n\n\nuv is an extremely fast Python package and project manager that is 10-100x faster than pip, and also makes it easy to install Python and manage projects. With uv, creating a virtual environment is as easy as uv venv.\n\n\nTo install MIKE IO, run this command in a terminal:\n\npipuv\n\n\npip install mikeio\n\n\nuv pip install mikeio",
    "crumbs": [
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "user-guide/getting-started.html#dataset",
    "href": "user-guide/getting-started.html#dataset",
    "title": "Getting started",
    "section": " Dataset",
    "text": "Dataset\nThe Dataset is the common MIKE IO data structure for data read from dfs files. The mikeio.read method returns a Dataset with a DataArray for each item.\nEach DataArray have the following properties:\n\nitem - an mikeio.ItemInfo with name, type and unit\ntime - a pandas.DatetimeIndex with the time instances of the data\ngeometry - a Geometry object with the spatial description of the data\nvalues - a numpy.ndarray",
    "crumbs": [
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "user-guide/getting-started.html#types-and-units",
    "href": "user-guide/getting-started.html#types-and-units",
    "title": "Getting started",
    "section": " Types and units",
    "text": "Types and units\nThe dfs items in MIKE IO are represented by the ItemInfo class. An ItemInfo consists of:\n\nname - a user-defined string\ntype - an EUMType\nunit - an EUMUnit\n\n\nimport mikeio\n\nmikeio.ItemInfo(\"Viken\", mikeio.EUMType.Water_Level)\n\nViken &lt;Water Level&gt; (meter)\n\n\n\nmikeio.ItemInfo(mikeio.EUMType.Wind_speed)\n\nWind speed &lt;Wind speed&gt; (meter per sec)",
    "crumbs": [
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "user-guide/getting-started.html#dfs0",
    "href": "user-guide/getting-started.html#dfs0",
    "title": "Getting started",
    "section": " Dfs0",
    "text": "Dfs0\nA dfs0 file is also called a time series file.\nRead Dfs0 to Dataset:\n\nds = mikeio.read(\"../data/da_diagnostic.dfs0\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:744)\ntime: 2017-10-27 00:00:00 - 2017-10-29 18:00:00 (744 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  State 1Sign. Wave Height &lt;Significant wave height&gt; (meter)\n  1:  State 2Sign. Wave Height &lt;Significant wave height&gt; (meter)\n  2:  Mean StateSign. Wave Height &lt;Significant wave height&gt; (meter)\n  3:  MeasurementSign. Wave Height &lt;Significant wave height&gt; (meter)\n\n\nRead more on the Dfs0 page.\nConvert the timeseries dataset to a pandas DataFrame:\n\ndf = ds.to_dataframe()\ndf.head()\n\n\n\n\n\n\n\n\nState 1Sign. Wave Height\nState 2Sign. Wave Height\nMean StateSign. Wave Height\nMeasurementSign. Wave Height\n\n\n\n\n2017-10-27 00:00:00\n1.749465\n1.749465\n1.749465\n1.72\n\n\n2017-10-27 00:10:00\n1.811340\n1.796895\n1.807738\nNaN\n\n\n2017-10-27 00:20:00\n1.863424\n1.842759\n1.853422\nNaN\n\n\n2017-10-27 00:30:00\n1.922261\n1.889839\n1.897670\nNaN\n\n\n2017-10-27 00:40:00\n1.972455\n1.934886\n1.935281\nNaN",
    "crumbs": [
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "user-guide/getting-started.html#dfs2",
    "href": "user-guide/getting-started.html#dfs2",
    "title": "Getting started",
    "section": " Dfs2",
    "text": "Dfs2\nA dfs2 file is also called a grid series file. Values in a dfs2 file are ‘element based’, i.e. values are defined in the centre of each grid cell.\n\nds = mikeio.read(\"../data/gebco_sound.dfs2\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1, y:264, x:216)\ntime: 2020-05-15 11:04:52 (time-invariant)\ngeometry: Grid2D (ny=264, nx=216)\nitems:\n  0:  Elevation &lt;Total Water Depth&gt; (meter)\n\n\nRead more on the Dfs2 page.",
    "crumbs": [
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "user-guide/getting-started.html#generic-dfs",
    "href": "user-guide/getting-started.html#generic-dfs",
    "title": "Getting started",
    "section": " Generic dfs",
    "text": "Generic dfs\nMIKE IO has generic functionality that works for all dfs files:\n\nconcat() - Concatenates files along the time axis\nextract() - Extract timesteps and/or items to a new dfs file\ndiff() - Calculate difference between two dfs files with identical geometry\nsum() - Calculate the sum of two dfs files\nscale() - Apply scaling to any dfs file\navg_time() - Create a temporally averaged dfs file\nquantile() - Create a dfs file with temporal quantiles\n\nAll generic methods creates a new dfs file.\nfrom mikeio import generic\ngeneric.concat([\"fileA.dfs2\", \"fileB.dfs2\"], \"new_file.dfs2\")",
    "crumbs": [
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "user-guide/getting-started.html#additional-resources",
    "href": "user-guide/getting-started.html#additional-resources",
    "title": "Getting started",
    "section": " Additional resources",
    "text": "Additional resources\n\nOnline book: Getting started with Dfs files in Python using MIKE IO\nOnline book: Python for marine modelers using MIKE IO and ModellSkill\nDFS file system specification",
    "crumbs": [
      "User Guide",
      "Getting started"
    ]
  },
  {
    "objectID": "user-guide/pfs.html",
    "href": "user-guide/pfs.html",
    "title": "PFS",
    "section": "",
    "text": "A PFS file is a text file with a tree structure that contains parameters and settings for MIKE tools and engines. MIKE IO can read, modify and create PFS files.",
    "crumbs": [
      "User Guide",
      "PFS"
    ]
  },
  {
    "objectID": "user-guide/pfs.html#the-pfs-file",
    "href": "user-guide/pfs.html#the-pfs-file",
    "title": "PFS",
    "section": "The PFS file",
    "text": "The PFS file\nThe content of the PFS file is similar to a nested dictionary. The root element is often called the target. Some PFS files have multiple root elements. The below sections are called PFS Sections which can be nested and contain key-value pairs called keywords and parameters.\n1[TARGET1]\n   keywordA = parameterA\n   [SECTION1]\n      keywordB = parameterB\n      keywordC = parameterC\n      [SECTION2]\n         keywordD = parameterD\n      EndSect  // SECTION2\n   EndSect  // SECTION1\nEndSect  // TARGET1\n\n2[TARGET2]\n   keywordE = parameterE\n   [SECTION3]\n      keywordF = parameterF\n   EndSect  // SECTION3\nEndSect  // TARGET2\n\n1\n\nFirst target, access this part with pfs.targets[0]\n\n2\n\nSecond target, access this part with pfs.targets[1]\n\n\n\n\n\n\n\n\nNote\n\n\n\nComments // is used to add comments to the PFS file (e.g. // SECTION2), the comments are ignored by MIKE IO.",
    "crumbs": [
      "User Guide",
      "PFS"
    ]
  },
  {
    "objectID": "user-guide/pfs.html#read",
    "href": "user-guide/pfs.html#read",
    "title": "PFS",
    "section": "Read",
    "text": "Read\nWhen a PFS file is read with MIKE IO, a PfsDocument object is created. It will contain one or more PfsSection objects - one for each target. The PfsSections will typically contain other PfsSections together with a number of key-value pairs.\nA PFS file is read using mikeio.read_pfs:\n\nimport mikeio\n\npfs = mikeio.read_pfs(\"../data/pfs/concat.mzt\")\npfs\n\n[txconc]\n   CLSID = 'TxConc.dll'\n   TypeName = 'txconc'\n   CREATEDTIME = '2020-03-11T15:24:45'\n   MODIFIEDTIME = '2020-03-11T15:24:45'\n   NOTES = ''\n   [Setup]\n      Name = 'Setup Name'\n      NumberFiles = 2\n      NumberDimensions = 1\n      NumberItems = 1\n      InsertDelIfGabs = 0\n      UseTimeRange = 1\n      TimeRange = 0, 145, 1\n      FirstFileTimeDef = 0\n      OverwriteWithLatest = 1\n      [File_1]\n         InputFile = |.\\tide1.dfs1|\n         Items = 1\n      EndSect  // File_1\n      [File_2]\n         InputFile = |.\\tide2.dfs1|\n         Items = 1\n      EndSect  // File_2\n      [File_Out]\n         OutputFile = |.\\txconc.dfs1|\n         OutputFileTitle = ''\n      EndSect  // File_Out\n   EndSect  // Setup\nEndSect  // txconc\n\n\n\nPfsDocument\nThe mikeio.PfsDocument is the MIKE IO equivalent to a PFS file. Its targets can be accessed by their name (as properties), like this:\n\npfs.txconc\n\nCLSID = 'TxConc.dll'\nTypeName = 'txconc'\nCREATEDTIME = '2020-03-11T15:24:45'\nMODIFIEDTIME = '2020-03-11T15:24:45'\nNOTES = ''\n[Setup]\n   Name = 'Setup Name'\n   NumberFiles = 2\n   NumberDimensions = 1\n   NumberItems = 1\n   InsertDelIfGabs = 0\n   UseTimeRange = 1\n   TimeRange = 0, 145, 1\n   FirstFileTimeDef = 0\n   OverwriteWithLatest = 1\n   [File_1]\n      InputFile = |.\\tide1.dfs1|\n      Items = 1\n   EndSect  // File_1\n   [File_2]\n      InputFile = |.\\tide2.dfs1|\n      Items = 1\n   EndSect  // File_2\n   [File_Out]\n      OutputFile = |.\\txconc.dfs1|\n      OutputFileTitle = ''\n   EndSect  // File_Out\nEndSect  // Setup\n\n\nOr by the pfs.targets object (which is a list of PfsSections). Each of the targets is a PfsSection object consisting of key-value pairs (keyword-parameter) and other PfsSections.\nThe mikeio.PfsDocument object is similar to a dictionary. You can loop over its contents with items(), keys() and values() like a dictionary.\n\n\nPfsSection\nThe mikeio.PfsSection object is also similar to a dictionary. You can loop over its contents with items(), keys() and values() like a dictionary.\n\npfs.txconc.keys()\n\ndict_keys(['CLSID', 'TypeName', 'CREATEDTIME', 'MODIFIEDTIME', 'NOTES', 'Setup'])\n\n\nYou can access a specific parameter with the get() method:\n\npfs.txconc.get(\"CLSID\")\n\n'TxConc.dll'\n\n\nOr as a property with dot-notation—which is prefered in most cases as it is more readable:\n\npfs.txconc.CLSID\n\n'TxConc.dll'\n\n\nA PfsSection can be converted to a dictionary with the to_dict() method:\n\npfs.txconc.Setup.File_1.to_dict()\n\n{'InputFile': '|.\\\\tide1.dfs1|', 'Items': 1}\n\n\nYou can access nested subsections by passing a path-like string, using “/” as a separator character:\n\npfs[\"txconc/Setup/File_1\"]\n\nInputFile = |.\\tide1.dfs1|\nItems = 1\n\n\nIf a PfsSection contains enumerated subsections, they can be converted to a pandas DataFrame with the to_dataframe() method:\n\npfs.txconc.Setup.to_dataframe(prefix=\"File_\")\n\n\n\n\n\n\n\n\nInputFile\nItems\n\n\n\n\n1\n|.\\tide1.dfs1|\n1\n\n\n2\n|.\\tide2.dfs1|\n1\n\n\n\n\n\n\n\n\n\nUnique or non-unique keywords\nDepending on the engine intended for reading the PFS file it may or may not make sense to have multiple identical keywords in the same PfsSection. MIKE 21/3 and the marine tools does not support non-unique keywords—if non-unique keywords are present, only the first will be read and the presence is most likely a mistake made by hand-editing the file. In other tools, e.g. MIKE Plot Composer, non-unique keywords are used a lot. How MIKE IO shall deal with non-unique keywords can be specified using the unique_keywords argument in the mikeio.read_pfs function:\npfs = mikeio.read_pfs(\"myplot.plt\", unique_keywords=False)\nIf a PfsSection contains non-unique PfsSections or keywords and unique_keywords=False, the repeated key will only appear once and the corresponding value will be a list.",
    "crumbs": [
      "User Guide",
      "PFS"
    ]
  },
  {
    "objectID": "user-guide/pfs.html#update",
    "href": "user-guide/pfs.html#update",
    "title": "PFS",
    "section": "Update",
    "text": "Update\nThe PfsSection object can be modified. Existing values can be changed, new key-value pairs can be added, subsections can added or removed.\n\nModify existing keyword\nIt is very simple to modify an existing keyword:\npfs.txconc.Setup.Name = \"new name\"\n\n\nAdd new key-value pair\nA new key-value pair can be added, like in a dictionary, in this way:\npfs.txconc.Setup[\"NewKeyword\"] = 12.0\n\n\nAdd new section as a copy of another section\nOften a PfsSection is added using an existing PfsSection as a template.\ns = pfs.txconc.Setup.File_1.copy()\ns.InputFile = '|.\\tide3.dfs1|'\npfs.txconc.Setup[\"File_3\"] = s\n\n\nAdd new section from a dictionary\nA PfsSection can be created from a dictionary and then added to another PfsSection like any other key-value pair:\n\nd = {'InputFile': '|.\\\\tide4.dfs1|', 'Items': 1}\ns = mikeio.PfsSection(d)\npfs.txconc.Setup[\"File_4\"] = s",
    "crumbs": [
      "User Guide",
      "PFS"
    ]
  },
  {
    "objectID": "user-guide/pfs.html#write-to-file",
    "href": "user-guide/pfs.html#write-to-file",
    "title": "PFS",
    "section": "Write to file",
    "text": "Write to file\nA Pfs document can be written to a file using the write method.\npfs.write(\"new.pfs\")",
    "crumbs": [
      "User Guide",
      "PFS"
    ]
  },
  {
    "objectID": "user-guide/pfs.html#create-new-pfs-files",
    "href": "user-guide/pfs.html#create-new-pfs-files",
    "title": "PFS",
    "section": "Create new Pfs files",
    "text": "Create new Pfs files\nA new PFS file can be created from dictionary in the following way:\n\nd = dict(\n    key1=1,\n    lst=[0.3, 0.7],\n    file_name=r\"|path\\file.dfs0|\",\n    start_time=[2019, 7, 1, 0, 0, 0],\n)\npfs = mikeio.PfsDocument({\"MYTOOL\": d})\npfs\n\n[MYTOOL]\n   key1 = 1\n   lst = 0.3, 0.7\n   file_name = |path\\file.dfs0|\n   start_time = 2019, 7, 1, 0, 0, 0\nEndSect  // MYTOOL\n\n\nMultiple targets can be achieved by providing a list of mikeio.PfsSection, in this way you can create a PFS file with multiple targets for the same tool.\n\ndata = [\n    mikeio.PfsSection({\"ATOOL\": {\"file_name\": r\"|path\\file1.dfs0|\"}}),\n    mikeio.PfsSection({\"ATOOL\": {\"file_name\": r\"|path\\file2.dfs0|\"}}),\n]\n\npfs = mikeio.PfsDocument(data)\npfs\n\n[ATOOL]\n   file_name = |path\\file1.dfs0|\nEndSect  // ATOOL\n[ATOOL]\n   file_name = |path\\file2.dfs0|\nEndSect  // ATOOL",
    "crumbs": [
      "User Guide",
      "PFS"
    ]
  }
]