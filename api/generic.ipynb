{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# generic\n",
        "\n",
        "`generic`\n",
        "\n",
        "Generic functions for working with all types of dfs files.\n",
        "\n",
        "## Functions\n",
        "\n",
        "| Name | Description |\n",
        "|------------------------------------|------------------------------------|\n",
        "| [add](#mikeio.generic.add) | Add two dfs files (a+b). |\n",
        "| [avg_time](#mikeio.generic.avg_time) | Create a temporally averaged dfs file. |\n",
        "| [change_datatype](#mikeio.generic.change_datatype) | Change datatype of a DFS file. |\n",
        "| [concat](#mikeio.generic.concat) | Concatenates files along the time axis. |\n",
        "| [diff](#mikeio.generic.diff) | Calculate difference between two dfs files (a-b). |\n",
        "| [extract](#mikeio.generic.extract) | Extract timesteps and/or items to a new dfs file. |\n",
        "| [fill_corrupt](#mikeio.generic.fill_corrupt) | Replace corrupt (unreadable) data with fill_value, default delete value. |\n",
        "| [quantile](#mikeio.generic.quantile) | Create temporal quantiles of all items in dfs file. |\n",
        "| [scale](#mikeio.generic.scale) | Apply scaling to any dfs file. |\n",
        "| [transform](#mikeio.generic.transform) | Transform a dfs file by applying functions to items. |\n",
        "\n",
        "### add\n",
        "\n",
        "``` python\n",
        "generic.add(infilename_a, infilename_b, outfilename)\n",
        "```\n",
        "\n",
        "Add two dfs files (a+b).\n",
        "\n",
        "### avg_time\n",
        "\n",
        "``` python\n",
        "generic.avg_time(infilename, outfilename, skipna=True)\n",
        "```\n",
        "\n",
        "Create a temporally averaged dfs file.\n",
        "\n",
        "#### Parameters\n",
        "\n",
        "| Name | Type | Description | Default |\n",
        "|---------|--------------|-----------------------------------------|---------|\n",
        "| infilename | str \\| pathlib.Path | input filename | *required* |\n",
        "| outfilename | str \\| pathlib.Path | output filename | *required* |\n",
        "| skipna | bool | exclude NaN/delete values when computing the result, default True | `True` |\n",
        "\n",
        "### change_datatype\n",
        "\n",
        "``` python\n",
        "generic.change_datatype(infilename, outfilename, datatype)\n",
        "```\n",
        "\n",
        "Change datatype of a DFS file.\n",
        "\n",
        "The data type tag is used to classify the file within a specific\n",
        "modeling context, such as MIKE 21. There is no global standard for these\n",
        "tags—they are interpreted locally within a model setup.\n",
        "\n",
        "Application developers can use these tags to classify files such as\n",
        "bathymetries, input data, or result files according to their own\n",
        "conventions.\n",
        "\n",
        "Default data type values assigned by MikeIO when creating new files\n",
        "are: - dfs0: datatype=1 - dfs1-3: datatype=0 - dfsu: datatype=2001\n",
        "\n",
        "#### Parameters\n",
        "\n",
        "| Name | Type | Description | Default |\n",
        "|------------|------------------|---------------------------------|-----------|\n",
        "| infilename | str \\| pathlib.Path | input filename | *required* |\n",
        "| outfilename | str \\| pathlib.Path | output filename | *required* |\n",
        "| datatype | int | DataType to be used for the output file | *required* |\n",
        "\n",
        "#### Examples\n",
        "\n",
        "``` python\n",
        ">>> change_datatype(\"in.dfsu\", \"out.dfsu\", datatype=107)\n",
        "```\n",
        "\n",
        "### concat\n",
        "\n",
        "``` python\n",
        "generic.concat(infilenames, outfilename, keep='last')\n",
        "```\n",
        "\n",
        "Concatenates files along the time axis.\n",
        "\n",
        "Overlap handling is defined by the `keep` argument, by default the last\n",
        "one will be used.\n",
        "\n",
        "#### Parameters\n",
        "\n",
        "| Name | Type | Description | Default |\n",
        "|-------|----------------|--------------------------------------------|-------|\n",
        "| infilenames | Sequence\\[str \\| pathlib.Path\\] | filenames to concatenate | *required* |\n",
        "| outfilename | str \\| pathlib.Path | filename of output | *required* |\n",
        "| keep | str | either ‘first’ (keep older), ‘last’ (keep newer) or ‘average’ can be selected. By default ‘last’ | `'last'` |\n",
        "\n",
        "#### Notes\n",
        "\n",
        "The list of input files have to be sorted, i.e. in chronological order\n",
        "\n",
        "### diff\n",
        "\n",
        "``` python\n",
        "generic.diff(infilename_a, infilename_b, outfilename)\n",
        "```\n",
        "\n",
        "Calculate difference between two dfs files (a-b).\n",
        "\n",
        "#### Parameters\n",
        "\n",
        "| Name | Type | Description | Default |\n",
        "|-------------|------------------|-------------------------------|-----------|\n",
        "| infilename_a | str \\| pathlib.Path | full path to the first input file | *required* |\n",
        "| infilename_b | str \\| pathlib.Path | full path to the second input file | *required* |\n",
        "| outfilename | str \\| pathlib.Path | full path to the output file | *required* |\n",
        "\n",
        "#### Examples"
      ],
      "id": "26ca1972-312d-48ed-8506-7db636d8be19"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]100%|██████████| 5/5 [00:00<00:00, 2560.94it/s]"
          ]
        }
      ],
      "source": [
        "from mikeio import generic\n",
        "generic.diff(\"../data/oresundHD_run1.dfsu\", \"../data/oresundHD_run2.dfsu\", \"diff.dfsu\");"
      ],
      "id": "0d4e6e3d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### extract\n",
        "\n",
        "``` python\n",
        "generic.extract(infilename, outfilename, start=0, end=-1, step=1, items=None)\n",
        "```\n",
        "\n",
        "Extract timesteps and/or items to a new dfs file.\n",
        "\n",
        "#### Parameters\n",
        "\n",
        "| Name | Type | Description | Default |\n",
        "|-------|----------------|--------------------------------------------|-------|\n",
        "| infilename | str \\| pathlib.Path | path to input dfs file | *required* |\n",
        "| outfilename | str \\| pathlib.Path | path to output dfs file | *required* |\n",
        "| start | (int, float, str or datetime) | start of extraction as either step, relative seconds or datetime/str, by default 0 (start of file) | `0` |\n",
        "| end | (int, float, str or datetime) | end of extraction as either step, relative seconds or datetime/str, by default -1 (end of file) | `-1` |\n",
        "| step | int | jump this many step, by default 1 (every step between start and end) | `1` |\n",
        "| items | (int, list(int), str, list(str)) | items to be extracted to new file | `None` |\n",
        "\n",
        "#### Examples\n",
        "\n",
        "``` python\n",
        ">>> extract('f_in.dfs0', 'f_out.dfs0', start='2018-1-1')\n",
        ">>> extract('f_in.dfs2', 'f_out.dfs2', end=-3)\n",
        ">>> extract('f_in.dfsu', 'f_out.dfsu', start=1800.0, end=3600.0)\n",
        ">>> extract('f_hourly.dfsu', 'f_daily.dfsu', step=24)\n",
        ">>> extract('f_in.dfsu', 'f_out.dfsu', items=[2, 0])\n",
        ">>> extract('f_in.dfsu', 'f_out.dfsu', items=\"Salinity\")\n",
        ">>> extract('f_in.dfsu', 'f_out.dfsu', end='2018-2-1 00:00', items=\"Salinity\")\n",
        "```\n",
        "\n",
        "### fill_corrupt\n",
        "\n",
        "``` python\n",
        "generic.fill_corrupt(infilename, outfilename, fill_value=np.nan, items=None)\n",
        "```\n",
        "\n",
        "Replace corrupt (unreadable) data with fill_value, default delete value.\n",
        "\n",
        "#### Parameters\n",
        "\n",
        "| Name | Type | Description | Default |\n",
        "|--------|------------------|----------------------------------------|--------|\n",
        "| infilename | str \\| pathlib.Path | full path to the input file | *required* |\n",
        "| outfilename | str \\| pathlib.Path | full path to the output file | *required* |\n",
        "| fill_value | float | value to use where data is corrupt, default delete value | `np.nan` |\n",
        "| items | Sequence\\[str \\| int\\] \\| None | Process only selected items, by number (0-based) or name, by default: all | `None` |\n",
        "\n",
        "### quantile\n",
        "\n",
        "``` python\n",
        "generic.quantile(\n",
        "    infilename,\n",
        "    outfilename,\n",
        "    q,\n",
        "    *,\n",
        "    items=None,\n",
        "    skipna=True,\n",
        "    buffer_size=1000000000.0,\n",
        ")\n",
        "```\n",
        "\n",
        "Create temporal quantiles of all items in dfs file.\n",
        "\n",
        "#### Parameters\n",
        "\n",
        "| Name | Type | Description | Default |\n",
        "|-----|-------------|-------------------------------------------------|------|\n",
        "| infilename | str \\| pathlib.Path | input filename | *required* |\n",
        "| outfilename | str \\| pathlib.Path | output filename | *required* |\n",
        "| q | float \\| Sequence\\[float\\] | Quantile or sequence of quantiles to compute, which must be between 0 and 1 inclusive. | *required* |\n",
        "| items | Sequence\\[int \\| str\\] \\| int \\| str \\| None | Process only selected items, by number (0-based) or name, by default: all | `None` |\n",
        "| skipna | bool | exclude NaN/delete values when computing the result, default True | `True` |\n",
        "| buffer_size | float | for huge files the quantiles need to be calculated for chunks of elements. buffer_size gives the maximum amount of memory available for the computation in bytes, by default 1e9 (=1GB) | `1000000000.0` |\n",
        "\n",
        "#### Examples\n",
        "\n",
        "``` python\n",
        ">>> quantile(\"in.dfsu\", \"IQR.dfsu\", q=[0.25,0.75])\n",
        "```\n",
        "\n",
        "``` python\n",
        ">>> quantile(\"huge.dfsu\", \"Q01.dfsu\", q=0.1, buffer_size=5.0e9)\n",
        "```\n",
        "\n",
        "``` python\n",
        ">>> quantile(\"with_nans.dfsu\", \"Q05.dfsu\", q=0.5, skipna=False)\n",
        "```\n",
        "\n",
        "### scale\n",
        "\n",
        "``` python\n",
        "generic.scale(infilename, outfilename, offset=0.0, factor=1.0, items=None)\n",
        "```\n",
        "\n",
        "Apply scaling to any dfs file.\n",
        "\n",
        "#### Parameters\n",
        "\n",
        "| Name | Type | Description | Default |\n",
        "|--------|------------------|----------------------------------------|--------|\n",
        "| infilename | str \\| pathlib.Path | full path to the input file | *required* |\n",
        "| outfilename | str \\| pathlib.Path | full path to the output file | *required* |\n",
        "| offset | float | value to add to all items, default 0.0 | `0.0` |\n",
        "| factor | float | value to multiply to all items, default 1.0 | `1.0` |\n",
        "| items | Sequence\\[int \\| str\\] \\| None | Process only selected items, by number (0-based) or name, by default: all | `None` |\n",
        "\n",
        "### transform\n",
        "\n",
        "``` python\n",
        "generic.transform(infilename, outfilename, vars, keep_existing_items=True)\n",
        "```\n",
        "\n",
        "Transform a dfs file by applying functions to items.\n",
        "\n",
        "#### Parameters\n",
        "\n",
        "| Name | Type | Description | Default |\n",
        "|--------|---------|---------------------------------------------------|-----|\n",
        "| infilename | str \\| pathlib.Path | full path to the input file | *required* |\n",
        "| outfilename | str \\| pathlib.Path | full path to the output file | *required* |\n",
        "| vars | Sequence\\[DerivedItem\\] | List of derived items to compute. | *required* |\n",
        "| keep_existing_items | bool | If True, existing items in the input file will be kept in the output file. If False, only the derived items will be written to the output file. Default is True. | `True` |\n",
        "\n",
        "#### Examples"
      ],
      "id": "87848d12-83e3-4644-8815-d64b88b8bc66"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import mikeio\n",
        "from mikeio.generic import DerivedItem, transform\n",
        "item = DerivedItem(\n",
        "        name=\"Current Speed\",\n",
        "        type=mikeio.EUMType.Current_Speed,\n",
        "        func=lambda x: np.sqrt(x[\"U velocity\"] ** 2 + x[\"V velocity\"] ** 2),\n",
        "    )\n",
        "transform(\n",
        "    infilename=\"../data/oresundHD_run1.dfsu\",\n",
        "    outfilename=\"out.dfsu\",\n",
        "    vars=[item],\n",
        "    keep_existing_items=False,\n",
        ")"
      ],
      "id": "dfb2fbdc"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/home/runner/work/mikeio/mikeio/.venv/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  }
}