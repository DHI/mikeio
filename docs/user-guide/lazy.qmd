---
title: Lazy
---

# {{< fa forward >}} Lazy Evaluation API

The `mikeio.lazy` module provides a specialized API for processing DFS files that are **too large to load into memory**.

:::{.callout-note}
## When to use lazy

**Most users should use [`mikeio.read()`](`mikeio.read`) and work with Dataset/DataArray** - it's simpler and more feature-rich.

Only use the lazy API when files don't fit in memory or when you specifically need rolling statistics on large files.
:::

Operations are queued and executed in a single pass, avoiding intermediate files and minimizing memory usage.

## Quick example

Process a large file by chaining operations:

```{python}
from mikeio.lazy import scan_dfs

(scan_dfs("../data/HD2D.dfsu")
    .select(["Surface elevation"])
    .rolling(window=3, stat="mean")
    .to_dfs("smoothed.dfsu")
)
```

The operations are lazy - nothing happens until `.to_dfs()` is called. Then all operations execute in a single pass through the file.

## {{< fa link >}} Chaining operations

Operations can be chained in any logical order. Works with all DFS file types (dfs0, dfs1, dfs2, dfs3, dfsu):

```{python}
(scan_dfs("../data/eq.dfs2")
    .select([0])                         # Select first item
    .filter(time=slice(0, 10))           # First 10 timesteps
    .rolling(window=3, stat="mean")      # Smooth with rolling mean
    .to_dfs("processed.dfs2")            # Execute and write
)
```

## {{< fa filter >}} Selecting items

Select items by name or index, just like with Dataset:

```{python}
# By name (dfsu file)
(scan_dfs("../data/NorthSea_HD_and_windspeed.dfsu")
    .select(["Wind speed"])
    .to_dfs("wind_only.dfsu")
)
```

```{python}
# By index (dfs2 file)
(scan_dfs("../data/consistency/oresundHD.dfs2")
    .select([0, 1])  # First two items
    .to_dfs("first_two.dfs2")
)
```

## {{< fa calendar >}} Filtering time

Use slice notation to filter timesteps:

```{python}
# By index (dfs2 file)
(scan_dfs("../data/waves.dfs2")
    .filter(time=slice(0, 2))  # First 2 timesteps
    .to_dfs("early.dfs2")
)
```

## {{< fa chart-line >}} Rolling statistics

Apply rolling window statistics without loading data into memory:

```{python}
# Built-in statistics (dfsu file)
(scan_dfs("../data/NorthSea_HD_and_windspeed.dfsu")
    .select(["Surface elevation"])
    .rolling(window=3, stat="mean")
    .to_dfs("smoothed.dfsu")
)
```

Available statistics: `"mean"`, `"min"`, `"max"`, `"median"`, `"sum"`, `"std"`.

### Custom statistics

Pass a custom function for specialized statistics:

```{python}
import numpy as np

# 90th percentile (dfs2 file)
(scan_dfs("../data/waves.dfs2")
    .select(["Sign. Wave Height"])
    .rolling(window=3, stat=lambda x: np.nanpercentile(x, 90))
    .to_dfs("p90_waves.dfs2")
)
```

The function receives a 1D array of `window` values and should return a scalar.

## {{< fa calculator >}} Transforming data

Apply custom transformations to items:

```{python}
# Convert wind components (dfs2 file)
(scan_dfs("../data/europe_wind_long_lat.dfs2")
    .select(["Wind x-comp (10m)"])
    .with_items(**{"Wind x-comp (10m)": lambda x: x * 1.94384})  # m/s to knots
    .to_dfs("wind_knots.dfs2")
)
```

## {{< fa chart-bar >}} Temporal aggregation

Compute statistics over the entire time dimension to produce a single-timestep output:

```{python}
# Single statistic (dfsu file)
(scan_dfs("../data/HD2D.dfsu")
    .select(["Surface elevation"])
    .aggregate("max")
    .to_dfs("max_elevation.dfsu")
)
```

### Multiple statistics in one pass

Efficiently compute multiple statistics simultaneously:

```{python}
# Multiple statistics (dfs2 file)
(scan_dfs("../data/waves.dfs2")
    .select(["Sign. Wave Height"])
    .aggregate(["min", "max", "mean", "std"])
    .to_dfs("wave_stats.dfs2")
)
```

This creates a file with 4 items: "Min.: Sign. Wave Height", "Max.: Sign. Wave Height", "Mean: Sign. Wave Height", and "Std.: Sign. Wave Height".

### Custom labels and formatting

Customize how aggregate items are named:

```{python}
# Custom labels (dfs0 file)
(scan_dfs("../data/random.dfs0")
    .select([0])
    .aggregate(
        ["min", "max", "mean"],
        labels={"mean": "Avg", "max": "Maximum", "min": "Minimum"}
    )
    .to_dfs("custom_labels.dfs0")
)
```

Use suffix style instead of prefix:

```{python}
# Suffix format (dfsu file)
(scan_dfs("../data/HD2D.dfsu")
    .select(["Surface elevation"])
    .aggregate(["min", "max"], format="{item} {stat}")
    .to_dfs("suffix_stats.dfsu")
)
```

Combine custom labels and format:

```{python}
# Both custom labels and format (dfs2 file)
(scan_dfs("../data/waves.dfs2")
    .select(["Sign. Wave Height"])
    .aggregate(
        ["min", "max", "mean"],
        labels={"mean": "Avg", "max": "Max", "min": "Min"},
        format="{item} ({stat})"
    )
    .to_dfs("custom_both.dfs2")
)
```

## Complete pipeline

Combine multiple operations:

```{python}
import numpy as np

(scan_dfs("../data/HD2D.dfsu")
    .select(["Surface elevation"])
    .filter(time=slice(0, 7))
    .rolling(window=3, stat="mean", min_periods=1)
    .with_items(**{"Surface elevation": lambda x: x * 100})  # to cm
    .to_dfs("pipeline.dfsu")
)
```

## When to use lazy API vs generic module

The lazy API complements the existing [`generic`](generic.qmd) module.

**Use lazy API when:**

* You need to chain multiple operations
* You want rolling statistics
* You want to avoid intermediate files

**Use generic module when:**

* You need `concat()` to join multiple files
* You need `diff()` to compare two files
* You only need a single operation

**Example:** If you need to concatenate files *and* apply a rolling mean, use `generic.concat()` first, then `lazy` for the rolling operation.

## API Reference

* [`scan_dfs()`](`mikeio.lazy.scan_dfs`) - Create a lazy processor
* [`.select()`](`mikeio.lazy.LazyDfs.select`) - Select items
* [`.filter()`](`mikeio.lazy.LazyDfs.filter`) - Filter timesteps
* [`.rolling()`](`mikeio.lazy.LazyDfs.rolling`) - Rolling statistics
* [`.aggregate()`](`mikeio.lazy.LazyDfs.aggregate`) - Temporal aggregation
* [`.with_items()`](`mikeio.lazy.LazyDfs.with_items`) - Transform items
* [`.to_dfs()`](`mikeio.lazy.LazyDfs.to_dfs`) - Execute and write
