---
title: Lazy
---

# {{< fa forward >}} Lazy Evaluation API

The `mikeio.lazy` module provides a specialized API for processing DFS files that are **too large to load into memory**.

:::{.callout-note}
## When to use lazy

**Most users should use [`mikeio.read()`](`mikeio.read`) and work with Dataset/DataArray** - it's simpler and more feature-rich.

Only use the lazy API when files don't fit in memory or when you specifically need rolling statistics on large files.
:::

Operations are queued and executed in a single pass, avoiding intermediate files and minimizing memory usage.

## Quick example

Process a large file by chaining operations:

```{python}
from mikeio.lazy import scan_dfs

(scan_dfs("../data/HD2D.dfsu")
    .select(["Surface elevation"])
    .rolling(window=3, stat="mean")
    .to_dfs("smoothed.dfsu")
)
```

The operations are lazy - nothing happens until `.to_dfs()` is called. Then all operations execute in a single pass through the file.

## {{< fa link >}} Chaining operations

Operations can be chained in any logical order. Works with all DFS file types (dfs0, dfs1, dfs2, dfs3, dfsu):

```{python}
(scan_dfs("../data/eq.dfs2")
    .select([0])                         # Select first item
    .filter(time=slice(0, 10))           # First 10 timesteps
    .rolling(window=3, stat="mean")      # Smooth with rolling mean
    .to_dfs("processed.dfs2")            # Execute and write
)
```

## {{< fa filter >}} Selecting items

Select items by name or index, just like with Dataset:

```{python}
# By name (dfsu file)
(scan_dfs("../data/NorthSea_HD_and_windspeed.dfsu")
    .select(["Wind speed"])
    .to_dfs("wind_only.dfsu")
)
```

```{python}
# By index (dfs2 file)
(scan_dfs("../data/consistency/oresundHD.dfs2")
    .select([0, 1])  # First two items
    .to_dfs("first_two.dfs2")
)
```

## {{< fa calendar >}} Filtering time

Use slice notation to filter timesteps:

```{python}
# By index (dfs2 file)
(scan_dfs("../data/waves.dfs2")
    .filter(time=slice(0, 2))  # First 2 timesteps
    .to_dfs("early.dfs2")
)
```

## {{< fa chart-line >}} Rolling statistics

Apply rolling window statistics without loading data into memory:

```{python}
# Built-in statistics (dfsu file)
(scan_dfs("../data/NorthSea_HD_and_windspeed.dfsu")
    .select(["Surface elevation"])
    .rolling(window=3, stat="mean")
    .to_dfs("smoothed.dfsu")
)
```

Available statistics: `"mean"`, `"min"`, `"max"`, `"median"`, `"sum"`, `"std"`.

### Custom statistics

Pass a custom function for specialized statistics:

```{python}
import numpy as np

# 90th percentile (dfs2 file)
(scan_dfs("../data/waves.dfs2")
    .select(["Sign. Wave Height"])
    .rolling(window=3, stat=lambda x: np.nanpercentile(x, 90))
    .to_dfs("p90_waves.dfs2")
)
```

The function receives a 1D array of `window` values and should return a scalar.

## {{< fa calculator >}} Transforming data

Apply custom transformations to items:

```{python}
# Convert wind components (dfs2 file)
(scan_dfs("../data/europe_wind_long_lat.dfs2")
    .select(["Wind x-comp (10m)"])
    .with_items(**{"Wind x-comp (10m)": lambda x: x * 1.94384})  # m/s to knots
    .to_dfs("wind_knots.dfs2")
)
```

## Complete pipeline

Combine multiple operations:

```{python}
import numpy as np

(scan_dfs("../data/HD2D.dfsu")
    .select(["Surface elevation"])
    .filter(time=slice(0, 7))
    .rolling(window=3, stat="mean", min_periods=1)
    .with_items(**{"Surface elevation": lambda x: x * 100})  # to cm
    .to_dfs("pipeline.dfsu")
)
```

## When to use lazy API vs generic module

The lazy API complements the existing [`generic`](generic.qmd) module.

**Use lazy API when:**

* You need to chain multiple operations
* You want rolling statistics
* You want to avoid intermediate files

**Use generic module when:**

* You need `concat()` to join multiple files
* You need `diff()` to compare two files
* You only need a single operation

**Example:** If you need to concatenate files *and* apply a rolling mean, use `generic.concat()` first, then `lazy` for the rolling operation.

## API Reference

* [`scan_dfs()`](`mikeio.lazy.scan_dfs`) - Create a lazy processor
* [`.select()`](`mikeio.lazy.LazyDfs.select`) - Select items
* [`.filter()`](`mikeio.lazy.LazyDfs.filter`) - Filter timesteps
* [`.rolling()`](`mikeio.lazy.LazyDfs.rolling`) - Rolling statistics
* [`.with_items()`](`mikeio.lazy.LazyDfs.with_items`) - Transform items
* [`.to_dfs()`](`mikeio.lazy.LazyDfs.to_dfs`) - Execute and write
